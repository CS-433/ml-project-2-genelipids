{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Neural Network for single lipids"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-16 19:08:14.839099: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Users/violarenne/opt/anaconda3/envs/ml-project-2-genelipids/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "/Users/violarenne/opt/anaconda3/envs/ml-project-2-genelipids/lib/python3.10/site-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.13.0 and strictly below 2.16.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.10.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Basics\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "# Keras\n",
    "from keras import metrics\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization, Dropout, Activation\n",
    "from keras.initializers import he_normal\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.regularizers import l2\n",
    "# Sklearn\n",
    "from sklearn.model_selection import KFold"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Reproducibility"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T19:34:08.341412800Z",
     "start_time": "2023-12-12T19:34:08.163373600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Random seed for reproducibility\n",
    "seed = 42\n",
    "\n",
    "random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "tf.compat.v1.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data loading"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T19:34:10.588414Z",
     "start_time": "2023-12-12T19:34:10.580663600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dataset path \n",
    "dataset_dir = 'data/processed_data'\n",
    "# Training path\n",
    "training_input_path = os.path.join(dataset_dir, 'train_features.parquet')\n",
    "training_output_path = os.path.join(dataset_dir, 'train_targets.parquet')\n",
    "# Test path\n",
    "testing_input_path = os.path.join(dataset_dir, 'test_features.parquet')\n",
    "testing_output_path = os.path.join(dataset_dir, 'test_targets.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T19:34:12.188127200Z",
     "start_time": "2023-12-12T19:34:11.369415Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load dataset into Pandas dataframes\n",
    "training_input = pd.read_parquet(training_input_path)\n",
    "training_output = pd.read_parquet(training_output_path)\n",
    "\n",
    "testing_input = pd.read_parquet(testing_input_path)\n",
    "testing_output = pd.read_parquet(testing_output_path)\n",
    "\n",
    "training_output = training_output * 1000\n",
    "\n",
    "# Number of output nodes (lipids) for the model\n",
    "OUTPUT_NODES = training_output.shape[1]\n",
    "# Number of input nodes (genes) for the model\n",
    "input_dim = training_input.shape[1]\n",
    "# Batch size\n",
    "batch_size = 32\n",
    "# Remove Nans from lipids\n",
    "remove_nans_lipids = False"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Definition of the model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T19:34:18.847395600Z",
     "start_time": "2023-12-12T19:34:18.054833400Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-16 19:08:25.534231: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 512)               256512    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 512)               262656    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 16)                528       \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 16)                272       \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 8)                 72        \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 4)                 36        \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 4)                 20        \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 1)                 5         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 782,317\n",
      "Trainable params: 782,317\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.engine.sequential.Sequential at 0x7f944644ec80>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lr_schedule(epoch):\n",
    "    \"\"\"\n",
    "    Set the learning rate considering the epoch's number\n",
    "    :param epoch: epoch's number\n",
    "    :return: learning rate\n",
    "    \"\"\"\n",
    "    initial_learning_rate = 0.1  # Set initial learning rate\n",
    "    decay_factor = 0.9  # Set decay factor\n",
    "    lr = initial_learning_rate * decay_factor ** epoch  # Compute learnign rate\n",
    "    return lr\n",
    "\n",
    "# Create a Sequential model\n",
    "def build_model(summary=False):\n",
    "    \"\"\"\n",
    "    Build the neural network\n",
    "    :param summary: if True, print the summary of the model, if False, do not print\n",
    "    :return: the model\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add the input layer with input_dim input nodes\n",
    "    model.add(Dense(512, input_dim=input_dim, activation='gelu', kernel_initializer=he_normal(seed=seed)))\n",
    "    model.add(Dense(512, activation='gelu',  kernel_initializer=he_normal(seed=seed)))\n",
    "\n",
    "    model.add(Dense(256, activation='gelu',  kernel_initializer=he_normal(seed=seed)))\n",
    "    model.add(Dense(256, activation='gelu',  kernel_initializer=he_normal(seed=seed)))\n",
    "\n",
    "    model.add(Dense(128, activation='gelu',  kernel_initializer=he_normal(seed=seed)))\n",
    "    model.add(Dense(128, activation='gelu',  kernel_initializer=he_normal(seed=seed)))\n",
    "\n",
    "    model.add(Dense(64, activation='gelu',  kernel_initializer=he_normal(seed=seed)))\n",
    "    model.add(Dense(64, activation='gelu',  kernel_initializer=he_normal(seed=seed)))\n",
    "\n",
    "    model.add(Dense(32, activation='gelu',  kernel_initializer=he_normal(seed=seed)))\n",
    "    model.add(Dense(32, activation='gelu',  kernel_initializer=he_normal(seed=seed)))\n",
    "\n",
    "    model.add(Dense(16, activation='gelu',  kernel_initializer=he_normal(seed=seed)))\n",
    "    model.add(Dense(16, activation='gelu',  kernel_initializer=he_normal(seed=seed)))\n",
    "\n",
    "    model.add(Dense(8, activation='gelu',  kernel_initializer=he_normal(seed=seed)))\n",
    "    model.add(Dense(8, activation='gelu',  kernel_initializer=he_normal(seed=seed)))\n",
    "\n",
    "    model.add(Dense(4, activation='gelu',  kernel_initializer=he_normal(seed=seed)))\n",
    "    model.add(Dense(4, activation='gelu',  kernel_initializer=he_normal(seed=seed)))\n",
    "\n",
    "    # Add the output layer with 1 node (for single regression)\n",
    "    model.add(Dense(1, activation='relu'))\n",
    "\n",
    "    if summary:\n",
    "        # Display the model summary\n",
    "        model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "# Print the model summary\n",
    "build_model(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Use early stopping on the validation\n",
    "early_stopping = EarlyStopping(monitor='val_loss',  # Metric chose is validation loss (MSE)\n",
    "                               patience=10,         # Number of epochs with no improvement after which training stops\n",
    "                               restore_best_weights=True)  # Restores model weights from the epoch with the best value of the monitored metric"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def data_generator(X, y, batch_size):\n",
    "    \"\"\"\n",
    "    Data augmentation\n",
    "    :param X: input DataFrame (genes)\n",
    "    :param y: output DataFrame (single lipid)\n",
    "    :param batch_size: size of the mini batch\n",
    "    :return batch_X: dataset containing random modification on the input DataFrame with size corresponding to batch size\n",
    "    :return batch_y: dataset containing output with size corresponding to batch size\n",
    "    \"\"\"\n",
    "    # Set noise std and scale factor for random modifications\n",
    "    noise_std = 0.1\n",
    "    scale_factor_range = 0.3\n",
    "    while True:\n",
    "        indices = np.random.choice(X.shape[0], batch_size, replace=False)\n",
    "        batch_df = X.iloc[indices]\n",
    "\n",
    "        # Create a copy of the batch for augmentation\n",
    "        augmented_batch_df = batch_df.copy()\n",
    "\n",
    "        # Random scaling\n",
    "        scale_factor = np.random.uniform(1 - scale_factor_range, 1 + scale_factor_range)\n",
    "        augmented_batch_df *= scale_factor\n",
    "\n",
    "        # Add Gaussian noise to all features\n",
    "        augmented_batch_df += np.random.normal(loc=0, scale=noise_std, size=augmented_batch_df.shape)\n",
    "\n",
    "        batch_X = augmented_batch_df.values\n",
    "        batch_y = y.iloc[indices].values\n",
    "\n",
    "        yield batch_X, batch_y"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def remove_nans(training_input, training_output, lipid):\n",
    "    \"\"\"\n",
    "    Remove Nans from the output column, and modify the input DataFrame in accordance\n",
    "    :param training_input: input DataFrame (genes)\n",
    "    :param training_output: output DataFrame (lipids)\n",
    "    :param lipid: number of the lipid to consider\n",
    "    :return training: input DataFrame (genes) with rows without Nans\n",
    "    :return output: output DataFrame (genes) with rows without Nans\n",
    "    \"\"\"\n",
    "    value_to_replace = 0.10003404299092956\n",
    "    # Copy the output DataFrame\n",
    "    output = training_output.iloc[:, lipid].copy()\n",
    "    # Replace values with Nans\n",
    "    output = output.replace(value_to_replace, np.nan)\n",
    "    # Remove Nans rows from both DataFrames\n",
    "    non_nan_indices = np.where(~np.isnan(output))\n",
    "    output = output.iloc[non_nan_indices]\n",
    "    training = training_input.iloc[non_nan_indices]\n",
    "\n",
    "    return training, output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T19:44:35.203613400Z",
     "start_time": "2023-12-12T19:43:17.930797700Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################################################\n",
      "Start training for lipid LPC O- 18:3\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Epoch 1/50\n",
      "1564/1564 [==============================] - 31s 17ms/step - loss: 0.0392 - mean_squared_error: 0.0392 - mean_absolute_error: 0.1514 - r_square: 0.0345 - mean_absolute_percentage_error: 32.3374 - val_loss: 0.0354 - val_mean_squared_error: 0.0354 - val_mean_absolute_error: 0.1442 - val_r_square: 0.1584 - val_mean_absolute_percentage_error: 30.2675\n",
      "Epoch 2/50\n",
      "1564/1564 [==============================] - 26s 17ms/step - loss: 0.0348 - mean_squared_error: 0.0348 - mean_absolute_error: 0.1444 - r_square: 0.1455 - mean_absolute_percentage_error: 30.9024 - val_loss: 0.0351 - val_mean_squared_error: 0.0351 - val_mean_absolute_error: 0.1440 - val_r_square: 0.1649 - val_mean_absolute_percentage_error: 29.9265\n",
      "Epoch 3/50\n",
      "1564/1564 [==============================] - 25s 16ms/step - loss: 0.0345 - mean_squared_error: 0.0345 - mean_absolute_error: 0.1438 - r_square: 0.1598 - mean_absolute_percentage_error: 30.3109 - val_loss: 0.0343 - val_mean_squared_error: 0.0343 - val_mean_absolute_error: 0.1422 - val_r_square: 0.1844 - val_mean_absolute_percentage_error: 29.4328\n",
      "Epoch 4/50\n",
      "1564/1564 [==============================] - 26s 16ms/step - loss: 0.0340 - mean_squared_error: 0.0340 - mean_absolute_error: 0.1432 - r_square: 0.1698 - mean_absolute_percentage_error: 30.2000 - val_loss: 0.0339 - val_mean_squared_error: 0.0339 - val_mean_absolute_error: 0.1436 - val_r_square: 0.1950 - val_mean_absolute_percentage_error: 31.0191\n",
      "Epoch 5/50\n",
      "1564/1564 [==============================] - 24s 16ms/step - loss: 0.0341 - mean_squared_error: 0.0341 - mean_absolute_error: 0.1428 - r_square: 0.1736 - mean_absolute_percentage_error: 29.8857 - val_loss: 0.0330 - val_mean_squared_error: 0.0330 - val_mean_absolute_error: 0.1414 - val_r_square: 0.2146 - val_mean_absolute_percentage_error: 29.9623\n",
      "Epoch 6/50\n",
      "1564/1564 [==============================] - 25s 16ms/step - loss: 0.0330 - mean_squared_error: 0.0330 - mean_absolute_error: 0.1414 - r_square: 0.1910 - mean_absolute_percentage_error: 29.6393 - val_loss: 0.0336 - val_mean_squared_error: 0.0336 - val_mean_absolute_error: 0.1436 - val_r_square: 0.2017 - val_mean_absolute_percentage_error: 30.8234\n",
      "Epoch 7/50\n",
      "1564/1564 [==============================] - 28s 18ms/step - loss: 0.0341 - mean_squared_error: 0.0341 - mean_absolute_error: 0.1427 - r_square: 0.1684 - mean_absolute_percentage_error: 30.0579 - val_loss: 0.0355 - val_mean_squared_error: 0.0355 - val_mean_absolute_error: 0.1474 - val_r_square: 0.1560 - val_mean_absolute_percentage_error: 32.3636\n",
      "Epoch 8/50\n",
      "1564/1564 [==============================] - 25s 16ms/step - loss: 0.0333 - mean_squared_error: 0.0333 - mean_absolute_error: 0.1422 - r_square: 0.1815 - mean_absolute_percentage_error: 30.0260 - val_loss: 0.0328 - val_mean_squared_error: 0.0328 - val_mean_absolute_error: 0.1414 - val_r_square: 0.2200 - val_mean_absolute_percentage_error: 30.0982\n",
      "Epoch 9/50\n",
      "1564/1564 [==============================] - 23s 14ms/step - loss: 0.0329 - mean_squared_error: 0.0329 - mean_absolute_error: 0.1411 - r_square: 0.2006 - mean_absolute_percentage_error: 29.4596 - val_loss: 0.0354 - val_mean_squared_error: 0.0354 - val_mean_absolute_error: 0.1444 - val_r_square: 0.1581 - val_mean_absolute_percentage_error: 30.3274\n",
      "Epoch 10/50\n",
      "1564/1564 [==============================] - 25s 16ms/step - loss: 0.0323 - mean_squared_error: 0.0323 - mean_absolute_error: 0.1401 - r_square: 0.2074 - mean_absolute_percentage_error: 29.1101 - val_loss: 0.0324 - val_mean_squared_error: 0.0324 - val_mean_absolute_error: 0.1403 - val_r_square: 0.2293 - val_mean_absolute_percentage_error: 28.9428\n",
      "Epoch 11/50\n",
      "1564/1564 [==============================] - 23s 14ms/step - loss: 0.0317 - mean_squared_error: 0.0317 - mean_absolute_error: 0.1387 - r_square: 0.2212 - mean_absolute_percentage_error: 28.5939 - val_loss: 0.0320 - val_mean_squared_error: 0.0320 - val_mean_absolute_error: 0.1404 - val_r_square: 0.2397 - val_mean_absolute_percentage_error: 29.0213\n",
      "Epoch 12/50\n",
      "1564/1564 [==============================] - 24s 15ms/step - loss: 0.0322 - mean_squared_error: 0.0322 - mean_absolute_error: 0.1398 - r_square: 0.2078 - mean_absolute_percentage_error: 28.6283 - val_loss: 0.0349 - val_mean_squared_error: 0.0349 - val_mean_absolute_error: 0.1436 - val_r_square: 0.1707 - val_mean_absolute_percentage_error: 29.7609\n",
      "Epoch 13/50\n",
      "1564/1564 [==============================] - 26s 17ms/step - loss: 0.0322 - mean_squared_error: 0.0322 - mean_absolute_error: 0.1398 - r_square: 0.2097 - mean_absolute_percentage_error: 29.0836 - val_loss: 0.0325 - val_mean_squared_error: 0.0325 - val_mean_absolute_error: 0.1403 - val_r_square: 0.2274 - val_mean_absolute_percentage_error: 29.8687\n",
      "Epoch 14/50\n",
      "1564/1564 [==============================] - 25s 16ms/step - loss: 0.0327 - mean_squared_error: 0.0327 - mean_absolute_error: 0.1407 - r_square: 0.2150 - mean_absolute_percentage_error: 29.0521 - val_loss: 0.0335 - val_mean_squared_error: 0.0335 - val_mean_absolute_error: 0.1415 - val_r_square: 0.2041 - val_mean_absolute_percentage_error: 27.3414\n",
      "Epoch 15/50\n",
      "1564/1564 [==============================] - 26s 17ms/step - loss: 0.0322 - mean_squared_error: 0.0322 - mean_absolute_error: 0.1392 - r_square: 0.2158 - mean_absolute_percentage_error: 28.6863 - val_loss: 0.0325 - val_mean_squared_error: 0.0325 - val_mean_absolute_error: 0.1396 - val_r_square: 0.2279 - val_mean_absolute_percentage_error: 28.4004\n",
      "Epoch 16/50\n",
      "1564/1564 [==============================] - 25s 16ms/step - loss: 0.0319 - mean_squared_error: 0.0319 - mean_absolute_error: 0.1394 - r_square: 0.2181 - mean_absolute_percentage_error: 28.4867 - val_loss: 0.0324 - val_mean_squared_error: 0.0324 - val_mean_absolute_error: 0.1398 - val_r_square: 0.2296 - val_mean_absolute_percentage_error: 29.0284\n",
      "Epoch 17/50\n",
      "1564/1564 [==============================] - 24s 15ms/step - loss: 0.0319 - mean_squared_error: 0.0319 - mean_absolute_error: 0.1387 - r_square: 0.2283 - mean_absolute_percentage_error: 28.1052 - val_loss: 0.0340 - val_mean_squared_error: 0.0340 - val_mean_absolute_error: 0.1424 - val_r_square: 0.1925 - val_mean_absolute_percentage_error: 30.7112\n",
      "Epoch 18/50\n",
      "1564/1564 [==============================] - 22s 14ms/step - loss: 0.0322 - mean_squared_error: 0.0322 - mean_absolute_error: 0.1398 - r_square: 0.2175 - mean_absolute_percentage_error: 28.5845 - val_loss: 0.0321 - val_mean_squared_error: 0.0321 - val_mean_absolute_error: 0.1395 - val_r_square: 0.2361 - val_mean_absolute_percentage_error: 28.0169\n",
      "Epoch 19/50\n",
      "1564/1564 [==============================] - 28s 18ms/step - loss: 0.0316 - mean_squared_error: 0.0316 - mean_absolute_error: 0.1388 - r_square: 0.2265 - mean_absolute_percentage_error: 28.3802 - val_loss: 0.0322 - val_mean_squared_error: 0.0322 - val_mean_absolute_error: 0.1394 - val_r_square: 0.2345 - val_mean_absolute_percentage_error: 28.1784\n",
      "Epoch 20/50\n",
      "1564/1564 [==============================] - 26s 16ms/step - loss: 0.0323 - mean_squared_error: 0.0323 - mean_absolute_error: 0.1398 - r_square: 0.2224 - mean_absolute_percentage_error: 28.5859 - val_loss: 0.0324 - val_mean_squared_error: 0.0324 - val_mean_absolute_error: 0.1388 - val_r_square: 0.2290 - val_mean_absolute_percentage_error: 28.4886\n",
      "Epoch 21/50\n",
      "1564/1564 [==============================] - 27s 17ms/step - loss: 0.0325 - mean_squared_error: 0.0325 - mean_absolute_error: 0.1402 - r_square: 0.2071 - mean_absolute_percentage_error: 28.9320 - val_loss: 0.0329 - val_mean_squared_error: 0.0329 - val_mean_absolute_error: 0.1420 - val_r_square: 0.2170 - val_mean_absolute_percentage_error: 30.2508\n",
      "[0.03197409585118294, 0.03197409585118294, 0.14039327204227448, 0.2397068738937378, 29.021282196044922]\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Epoch 1/50\n",
      "1564/1564 [==============================] - 32s 18ms/step - loss: 0.4180 - mean_squared_error: 0.4180 - mean_absolute_error: 0.6138 - r_square: -9.1664 - mean_absolute_percentage_error: 99.9607 - val_loss: 0.4181 - val_mean_squared_error: 0.4181 - val_mean_absolute_error: 0.6139 - val_r_square: -9.1487 - val_mean_absolute_percentage_error: 100.0000\n",
      "Epoch 2/50\n",
      "1564/1564 [==============================] - 24s 16ms/step - loss: 0.4186 - mean_squared_error: 0.4186 - mean_absolute_error: 0.6142 - r_square: -9.1053 - mean_absolute_percentage_error: 100.0000 - val_loss: 0.4181 - val_mean_squared_error: 0.4181 - val_mean_absolute_error: 0.6139 - val_r_square: -9.1487 - val_mean_absolute_percentage_error: 100.0000\n",
      "Epoch 3/50\n",
      "1564/1564 [==============================] - 25s 16ms/step - loss: 0.4173 - mean_squared_error: 0.4173 - mean_absolute_error: 0.6132 - r_square: -9.1286 - mean_absolute_percentage_error: 100.0000 - val_loss: 0.4181 - val_mean_squared_error: 0.4181 - val_mean_absolute_error: 0.6139 - val_r_square: -9.1487 - val_mean_absolute_percentage_error: 100.0000\n",
      "Epoch 4/50\n",
      "1564/1564 [==============================] - 24s 15ms/step - loss: 0.4184 - mean_squared_error: 0.4184 - mean_absolute_error: 0.6140 - r_square: -9.1016 - mean_absolute_percentage_error: 100.0000 - val_loss: 0.4181 - val_mean_squared_error: 0.4181 - val_mean_absolute_error: 0.6139 - val_r_square: -9.1487 - val_mean_absolute_percentage_error: 100.0000\n",
      "Epoch 5/50\n",
      " 598/1564 [==========>...................] - ETA: 14s - loss: 0.4153 - mean_squared_error: 0.4153 - mean_absolute_error: 0.6124 - r_square: -9.3009 - mean_absolute_percentage_error: 100.0000"
     ]
    }
   ],
   "source": [
    "# DataFrame for results\n",
    "lipid_names = list(map(lambda s: s.strip(), training_output.columns.values))\n",
    "lipids_metrics_avg = pd.DataFrame(columns=['Loss', 'R2'], index=lipid_names)\n",
    "\n",
    "# Train a model for each lipid\n",
    "for j in range(OUTPUT_NODES):\n",
    "    print('#'*72)\n",
    "    print(f'Start training for lipid {lipid_names[j]}')\n",
    "\n",
    "    # Define the K-fold Cross Validator\n",
    "    num_folds = 5\n",
    "    k_fold = KFold(n_splits=num_folds, shuffle=True, random_state=seed)\n",
    "\n",
    "    # Metrics for each fold\n",
    "    loss_per_fold = np.zeros((num_folds,))\n",
    "    r2_per_fold = np.zeros((num_folds,))\n",
    "\n",
    "    # Modify the input and output\n",
    "    if remove_nans_lipids:\n",
    "        X, y = remove_nans(training_input, training_output, j)\n",
    "    else:\n",
    "        X = training_input\n",
    "        y = training_output.iloc[:, j]\n",
    "\n",
    "    # K-fold Cross Validation model evaluation\n",
    "    fold_no = 1\n",
    "    models = []\n",
    "    for input_indices, output_indices in k_fold.split(X, y):\n",
    "        # Build the model\n",
    "        model = build_model()\n",
    "\n",
    "        # Compile the model\n",
    "        model.compile(optimizer='adam', loss='mean_squared_error',\n",
    "                      metrics=[metrics.mean_squared_error, metrics.mean_absolute_error, tfa.metrics.RSquare(), metrics.mean_absolute_percentage_error])\n",
    "\n",
    "        print('------------------------------------------------------------------------')\n",
    "        print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "        train_generator = data_generator(X.iloc[input_indices], y.iloc[input_indices], batch_size)\n",
    "        # Train the model\n",
    "        history = model.fit(\n",
    "            train_generator,\n",
    "            epochs=50,  # Adjust the number of epochs as needed\n",
    "            steps_per_epoch=len(input_indices) // batch_size,\n",
    "            validation_data=(X.iloc[output_indices], y.iloc[output_indices]),\n",
    "            callbacks=[early_stopping]\n",
    "        ).history\n",
    "\n",
    "        # Generate generalization metrics\n",
    "        scores = model.evaluate(X.iloc[output_indices], y.iloc[output_indices], verbose=0)\n",
    "        print(scores)\n",
    "        loss_per_fold[fold_no-1] = scores[0]\n",
    "        r2_per_fold[fold_no-1] = scores[3]\n",
    "\n",
    "        models.append(model)\n",
    "\n",
    "        # Increase fold number\n",
    "        fold_no = fold_no + 1\n",
    "\n",
    "    # Computing the values\n",
    "    mean_loss = loss_per_fold.mean()\n",
    "    mean_r2 = r2_per_fold.mean()\n",
    "    lipids_metrics_avg.loc[lipid_names[0]] = [mean_loss, mean_r2]\n",
    "    print('#'*72)\n",
    "    print(f'Finish training for lipid {lipid_names[0]}')\n",
    "    print(f'Mean loss: {mean_loss}')\n",
    "    print(f'Mean r2: {mean_r2}')\n",
    "\n",
    "# Saving the values\n",
    "lipids_metrics_avg.to_csv('lipids_metrics_avg_neural_network.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Output processing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T19:40:13.547234Z",
     "start_time": "2023-12-12T19:40:13.512769300Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Print k best and worst lipids for average between folds of metrics loss and r2\n",
    "k = 5\n",
    "\n",
    "def format_names(df: pd.DataFrame, just=15):\n",
    "    return ', '.join([name.rjust(just) for name in df.index.values])\n",
    "\n",
    "def format_values(df: pd.DataFrame, col: str, just=15):\n",
    "    return ', '.join([f'{val:.5e}'.rjust(just) for val in df[col].values])\n",
    "\n",
    "best_losses = lipids_metrics_avg.nsmallest(k, 'Loss')\n",
    "worst_losses = lipids_metrics_avg.nlargest(k, 'Loss')\n",
    "print(\"Loss:\")\n",
    "print(\"  Best:\")\n",
    "print(f\"  {format_names(best_losses)}\")\n",
    "print(f\"  {format_values(best_losses, 'Loss')}\")\n",
    "print(\"  Worst:\")\n",
    "print(f\"  {format_names(worst_losses)}\")\n",
    "print(f\"  {format_values(worst_losses, 'Loss')}\")\n",
    "\n",
    "best_r2s = lipids_metrics_avg.nlargest(k, 'R2')\n",
    "worst_r2s = lipids_metrics_avg.nsmallest(k, 'R2')\n",
    "print(\"R2:\")\n",
    "print(\"  Best:\")\n",
    "print(f\"  {format_names(best_r2s)}\")\n",
    "print(f\"  {format_values(best_r2s, 'R2')}\")\n",
    "print(\"  Worst:\")\n",
    "print(f\"  {format_names(worst_r2s)}\")\n",
    "print(f\"  {format_values(worst_r2s, 'R2')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4115045,
     "sourceId": 7132195,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30587,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
