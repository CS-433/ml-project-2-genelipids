{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import cKDTree\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "from pycaret.regression import *\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lipid_path = 'data/section12/lipids_section_12.h5'\n",
    "gene_path = 'data/section12/genes_section_12.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_weight(distance, std_dev):\n",
    "    return norm.pdf(distance, 0, std_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_decay(distance, threshold, decay_rate):\n",
    "    adjusted_distance = distance - threshold\n",
    "    return np.exp(-decay_rate * adjusted_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def filter_dataframe_from_file(df, filepath):\n",
    "    # Read the text file and store the entries in a list\n",
    "    with open(filepath, 'r') as file:\n",
    "        entries = file.read().splitlines()\n",
    "\n",
    "    # Filter the DataFrame based on the index matching the entries\n",
    "    filtered_df = df.loc[df.index.isin(entries)]\n",
    "\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "lipids_section_12 = pd.read_hdf(lipid_path)\n",
    "genes_section_12 = pd.read_hdf(gene_path)\n",
    "\n",
    "# Need to remove the trailing naming scheme added before\n",
    "new_column_names = [re.sub(r'_(\\d+)$', '', col) for col in lipids_section_12.columns]\n",
    "lipids_section_12.columns = new_column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create cKDTree for fast query of neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a KDTree object for the genes\n",
    "genes_coords = genes_section_12[['y_ccf', 'z_ccf']].values\n",
    "genes_kdtree = cKDTree(genes_coords)\n",
    "\n",
    "# Extract coordinates for lipids\n",
    "lipids_coords = lipids_section_12[['y_ccf', 'z_ccf']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nearest Neighbor selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the indices of the closest gene for each lipid point\n",
    "_, indices = genes_kdtree.query(lipids_coords, k=1)\n",
    "\n",
    "# Initialize an empty array for aggregated gene data\n",
    "aggregated_gene_data = np.zeros((len(lipids_coords), genes_section_12.iloc[:, 46:-50].shape[1]))\n",
    "\n",
    "# Aggregate gene data based on the closest neighbor\n",
    "for i, gene_index in enumerate(indices):\n",
    "    aggregated_gene_data[i] = genes_section_12.iloc[gene_index, 46:-50]\n",
    "\n",
    "# Convert the aggregated data into a DataFrame\n",
    "aggregated_gene_data_df = pd.DataFrame(aggregated_gene_data, columns=genes_section_12.columns[46:-50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average of nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query to get the 5000 closest genes for each lipid point\n",
    "_, indices = genes_kdtree.query(lipids_coords, k=10000)\n",
    "\n",
    "# Pre-allocate memory for aggregated gene data\n",
    "n_genes = genes_section_12.iloc[:, 46:-50].shape[1]\n",
    "aggregated_gene_data = np.zeros((len(lipids_coords), n_genes))\n",
    "\n",
    "# Define a function to aggregate data\n",
    "def aggregate_data(i):\n",
    "    gene_indices = indices[i]\n",
    "    data = genes_section_12.iloc[gene_indices, 46:-50].values\n",
    "    return data.mean(axis=0)\n",
    "\n",
    "# Parallel processing\n",
    "with Pool(cpu_count()) as pool:\n",
    "    results = pool.map(aggregate_data, range(len(lipids_coords)))\n",
    "\n",
    "# Assign the results back to the array\n",
    "for i, res in enumerate(results):\n",
    "    aggregated_gene_data[i] = res\n",
    "\n",
    "# Convert the aggregated data into a DataFrame\n",
    "aggregated_gene_data_df = pd.DataFrame(aggregated_gene_data, columns=genes_section_12.columns[46:-50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exponential Decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the distances and indices of the 6 closest genes for each lipid point\n",
    "distances, indices = genes_kdtree.query(lipids_coords, k=6)\n",
    "\n",
    "# Calculate the average distance\n",
    "average_closest_distance = np.mean(distances)\n",
    "\n",
    "# Query to get the 100 closest genes and their distances\n",
    "distances, indices = genes_kdtree.query(lipids_coords, k=1000)\n",
    "\n",
    "# Initialize an empty array for aggregated gene data\n",
    "aggregated_gene_data = np.zeros((len(lipids_coords), genes_section_12.iloc[:, 46:-50].shape[1]))\n",
    "\n",
    "# Perform weighted aggregation\n",
    "for i, (gene_indices, dists) in enumerate(zip(indices, distances)):\n",
    "    # Weights based on distance, with a penalty for distances greater than the average closest distance\n",
    "    weights = np.where(dists <= average_closest_distance, 1, exponential_decay(dists, average_closest_distance, 3))  # Apply penalty for dist > average_closest_distance\n",
    "    weighted_data = genes_section_12.iloc[gene_indices, 46:-50] * weights[:, np.newaxis]\n",
    "    aggregated_gene_data[i] = weighted_data.sum(axis=0) / weights.sum() if weights.sum() > 0 else np.zeros(genes_section_12.iloc[:, 46:-50].shape[1])\n",
    "\n",
    "# Convert the aggregated data into a DataFrame\n",
    "aggregated_gene_data_df = pd.DataFrame(aggregated_gene_data, columns=genes_section_12.columns[46:-50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# K neighbors gaussian mean of genes for a given lipids datapoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the distances and indices of the closest 2500 genes for each lipid point\n",
    "distances, _ = genes_kdtree.query(lipids_coords, k=2500)\n",
    "\n",
    "# Calculate the std of the distances\n",
    "std_closest_distance = np.std(distances, axis=1)\n",
    "\n",
    "# Query to get the 5000 closest genes and their distances\n",
    "distances, indices = genes_kdtree.query(lipids_coords, k=5000)\n",
    "\n",
    "# Pre-allocate memory for aggregated gene data\n",
    "n_genes = genes_section_12.iloc[:, 46:-50].shape[1]\n",
    "aggregated_gene_data = np.zeros((len(lipids_coords), n_genes))\n",
    "\n",
    "# Define a function to aggregate data\n",
    "def aggregate_data(i):\n",
    "    dists = distances[i]\n",
    "    gene_indices = indices[i]\n",
    "    weights = gaussian_weight(dists, std_closest_distance[i])\n",
    "    weighted_data = genes_section_12.iloc[gene_indices, 46:-50].values * weights[:, np.newaxis]\n",
    "    return weighted_data.sum(axis=0) / weights.sum() if weights.sum() > 0 else np.zeros(n_genes)\n",
    "\n",
    "# Parallel processing\n",
    "with Pool(cpu_count()) as pool:\n",
    "    results = pool.map(aggregate_data, range(len(lipids_coords)))\n",
    "\n",
    "# Assign the results back to the array\n",
    "for i, res in enumerate(results):\n",
    "    aggregated_gene_data[i] = res\n",
    "\n",
    "# Convert the aggregated data into a DataFrame\n",
    "aggregated_gene_data_df = pd.DataFrame(aggregated_gene_data, columns=genes_section_12.columns[46:-50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take all the points into consideration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_lipid_point(lipid_coord, genes_coords, gene_data, std_closest_distance):\n",
    "    \"\"\"Process a single lipid point and return the aggregated data.\"\"\"\n",
    "    # Calculate distances to all gene points\n",
    "    dists = np.linalg.norm(genes_coords - lipid_coord, axis=1)\n",
    "\n",
    "    # Weights based on distance, with a penalty for distances greater than the average closest distance\n",
    "    weights = gaussian_weight(dists, std_closest_distance)\n",
    "\n",
    "    # Perform weighted aggregation\n",
    "    weighted_data = gene_data * weights[:, np.newaxis]\n",
    "    return weighted_data.sum(axis=0) / weights.sum() if weights.sum() > 0 else np.zeros(gene_data.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gene_data_columns = genes_section_12.columns[46:-50]\n",
    "gene_data_shape = genes_section_12[gene_data_columns].shape[1]\n",
    "\n",
    "# Initialize an empty array for aggregated gene data\n",
    "aggregated_gene_data = np.zeros((len(lipids_coords), gene_data_shape))\n",
    "\n",
    "distances, _ = genes_kdtree.query(lipids_coords, k=6)\n",
    "std_closest_distance = np.std(distances)\n",
    "\n",
    "# Extract gene data only once\n",
    "gene_data = genes_section_12[gene_data_columns].values\n",
    "\n",
    "# Parallel processing\n",
    "with Pool(cpu_count()) as pool:\n",
    "    results = pool.starmap(process_lipid_point, [(lipid_coord, genes_coords, gene_data, std_closest_distance) for lipid_coord in lipids_coords])\n",
    "\n",
    "# Combine results into aggregated_gene_data\n",
    "for i, result in enumerate(results):\n",
    "    aggregated_gene_data[i] = result\n",
    "\n",
    "# Convert the aggregated data into a DataFrame\n",
    "aggregated_gene_data_df = pd.DataFrame(aggregated_gene_data, columns=gene_data_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resulting genes and lipids DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ENSMUST00000028118</th>\n",
       "      <th>ENSMUST00000028280</th>\n",
       "      <th>ENSMUST00000030676</th>\n",
       "      <th>ENSMUST00000047328</th>\n",
       "      <th>ENSMUST00000057021</th>\n",
       "      <th>ENSMUST00000090697</th>\n",
       "      <th>ENSMUST00000091554</th>\n",
       "      <th>ENSMUST00000162772</th>\n",
       "      <th>ENSMUST00000021284</th>\n",
       "      <th>ENSMUST00000022195</th>\n",
       "      <th>...</th>\n",
       "      <th>ENSMUST00000109964</th>\n",
       "      <th>ENSMUST00000114553</th>\n",
       "      <th>ENSMUST00000152412</th>\n",
       "      <th>ENSMUST00000159365</th>\n",
       "      <th>ENSMUST00000175965</th>\n",
       "      <th>ENSMUST00000196378</th>\n",
       "      <th>ENSMUST00000228095</th>\n",
       "      <th>ENSMUST00000000219</th>\n",
       "      <th>ENSMUST00000035577</th>\n",
       "      <th>ENSMUST00000060943</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.124094</td>\n",
       "      <td>0.291178</td>\n",
       "      <td>0.128066</td>\n",
       "      <td>0.623594</td>\n",
       "      <td>0.242590</td>\n",
       "      <td>0.074589</td>\n",
       "      <td>0.084546</td>\n",
       "      <td>0.082760</td>\n",
       "      <td>0.323102</td>\n",
       "      <td>0.056296</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028234</td>\n",
       "      <td>0.008777</td>\n",
       "      <td>0.023342</td>\n",
       "      <td>0.219471</td>\n",
       "      <td>0.014906</td>\n",
       "      <td>0.242949</td>\n",
       "      <td>0.012779</td>\n",
       "      <td>0.019391</td>\n",
       "      <td>0.035380</td>\n",
       "      <td>0.037051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.123617</td>\n",
       "      <td>0.291195</td>\n",
       "      <td>0.132069</td>\n",
       "      <td>0.619720</td>\n",
       "      <td>0.238866</td>\n",
       "      <td>0.078791</td>\n",
       "      <td>0.087023</td>\n",
       "      <td>0.086119</td>\n",
       "      <td>0.317896</td>\n",
       "      <td>0.055298</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028362</td>\n",
       "      <td>0.008728</td>\n",
       "      <td>0.025231</td>\n",
       "      <td>0.221363</td>\n",
       "      <td>0.015748</td>\n",
       "      <td>0.246850</td>\n",
       "      <td>0.012993</td>\n",
       "      <td>0.019144</td>\n",
       "      <td>0.036661</td>\n",
       "      <td>0.038754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.122475</td>\n",
       "      <td>0.290745</td>\n",
       "      <td>0.136301</td>\n",
       "      <td>0.614887</td>\n",
       "      <td>0.234399</td>\n",
       "      <td>0.082712</td>\n",
       "      <td>0.089679</td>\n",
       "      <td>0.089638</td>\n",
       "      <td>0.312165</td>\n",
       "      <td>0.054260</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028319</td>\n",
       "      <td>0.008792</td>\n",
       "      <td>0.027318</td>\n",
       "      <td>0.223340</td>\n",
       "      <td>0.016716</td>\n",
       "      <td>0.249978</td>\n",
       "      <td>0.013238</td>\n",
       "      <td>0.019240</td>\n",
       "      <td>0.037741</td>\n",
       "      <td>0.040391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.120780</td>\n",
       "      <td>0.290199</td>\n",
       "      <td>0.140761</td>\n",
       "      <td>0.609180</td>\n",
       "      <td>0.229255</td>\n",
       "      <td>0.086308</td>\n",
       "      <td>0.092639</td>\n",
       "      <td>0.093405</td>\n",
       "      <td>0.305956</td>\n",
       "      <td>0.053218</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028152</td>\n",
       "      <td>0.008972</td>\n",
       "      <td>0.029628</td>\n",
       "      <td>0.225443</td>\n",
       "      <td>0.017806</td>\n",
       "      <td>0.252181</td>\n",
       "      <td>0.013485</td>\n",
       "      <td>0.019686</td>\n",
       "      <td>0.038572</td>\n",
       "      <td>0.042067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.118690</td>\n",
       "      <td>0.289892</td>\n",
       "      <td>0.145317</td>\n",
       "      <td>0.602782</td>\n",
       "      <td>0.223699</td>\n",
       "      <td>0.089497</td>\n",
       "      <td>0.095860</td>\n",
       "      <td>0.097383</td>\n",
       "      <td>0.299470</td>\n",
       "      <td>0.052184</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027889</td>\n",
       "      <td>0.009264</td>\n",
       "      <td>0.032130</td>\n",
       "      <td>0.227648</td>\n",
       "      <td>0.018977</td>\n",
       "      <td>0.253397</td>\n",
       "      <td>0.013693</td>\n",
       "      <td>0.020454</td>\n",
       "      <td>0.039100</td>\n",
       "      <td>0.043767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94742</th>\n",
       "      <td>0.170155</td>\n",
       "      <td>0.380321</td>\n",
       "      <td>0.356490</td>\n",
       "      <td>0.537902</td>\n",
       "      <td>0.281510</td>\n",
       "      <td>0.095585</td>\n",
       "      <td>0.233756</td>\n",
       "      <td>0.189178</td>\n",
       "      <td>0.395402</td>\n",
       "      <td>0.047084</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015559</td>\n",
       "      <td>0.061364</td>\n",
       "      <td>0.013411</td>\n",
       "      <td>0.123024</td>\n",
       "      <td>0.035987</td>\n",
       "      <td>0.144793</td>\n",
       "      <td>0.012435</td>\n",
       "      <td>0.033792</td>\n",
       "      <td>0.030281</td>\n",
       "      <td>0.008782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94743</th>\n",
       "      <td>0.169232</td>\n",
       "      <td>0.385538</td>\n",
       "      <td>0.349213</td>\n",
       "      <td>0.542164</td>\n",
       "      <td>0.289974</td>\n",
       "      <td>0.091108</td>\n",
       "      <td>0.232701</td>\n",
       "      <td>0.176244</td>\n",
       "      <td>0.399768</td>\n",
       "      <td>0.044931</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014296</td>\n",
       "      <td>0.059693</td>\n",
       "      <td>0.012953</td>\n",
       "      <td>0.125018</td>\n",
       "      <td>0.036701</td>\n",
       "      <td>0.143519</td>\n",
       "      <td>0.011728</td>\n",
       "      <td>0.031828</td>\n",
       "      <td>0.029679</td>\n",
       "      <td>0.007909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94744</th>\n",
       "      <td>0.168109</td>\n",
       "      <td>0.391031</td>\n",
       "      <td>0.341711</td>\n",
       "      <td>0.546342</td>\n",
       "      <td>0.298262</td>\n",
       "      <td>0.086795</td>\n",
       "      <td>0.231340</td>\n",
       "      <td>0.165354</td>\n",
       "      <td>0.403440</td>\n",
       "      <td>0.042890</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013114</td>\n",
       "      <td>0.058164</td>\n",
       "      <td>0.012628</td>\n",
       "      <td>0.127244</td>\n",
       "      <td>0.037463</td>\n",
       "      <td>0.141764</td>\n",
       "      <td>0.011042</td>\n",
       "      <td>0.029889</td>\n",
       "      <td>0.029413</td>\n",
       "      <td>0.007109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94745</th>\n",
       "      <td>0.166513</td>\n",
       "      <td>0.395388</td>\n",
       "      <td>0.338151</td>\n",
       "      <td>0.544088</td>\n",
       "      <td>0.299778</td>\n",
       "      <td>0.083737</td>\n",
       "      <td>0.233229</td>\n",
       "      <td>0.159442</td>\n",
       "      <td>0.401881</td>\n",
       "      <td>0.041783</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012161</td>\n",
       "      <td>0.057034</td>\n",
       "      <td>0.012614</td>\n",
       "      <td>0.129440</td>\n",
       "      <td>0.038170</td>\n",
       "      <td>0.138102</td>\n",
       "      <td>0.010601</td>\n",
       "      <td>0.028355</td>\n",
       "      <td>0.029906</td>\n",
       "      <td>0.006518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94746</th>\n",
       "      <td>0.165028</td>\n",
       "      <td>0.400629</td>\n",
       "      <td>0.331023</td>\n",
       "      <td>0.546551</td>\n",
       "      <td>0.306439</td>\n",
       "      <td>0.080016</td>\n",
       "      <td>0.231559</td>\n",
       "      <td>0.152272</td>\n",
       "      <td>0.403618</td>\n",
       "      <td>0.040139</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011186</td>\n",
       "      <td>0.055776</td>\n",
       "      <td>0.012629</td>\n",
       "      <td>0.131944</td>\n",
       "      <td>0.038872</td>\n",
       "      <td>0.135608</td>\n",
       "      <td>0.010006</td>\n",
       "      <td>0.026557</td>\n",
       "      <td>0.030300</td>\n",
       "      <td>0.005878</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>94747 rows × 500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ENSMUST00000028118  ENSMUST00000028280  ENSMUST00000030676  \\\n",
       "0                0.124094            0.291178            0.128066   \n",
       "1                0.123617            0.291195            0.132069   \n",
       "2                0.122475            0.290745            0.136301   \n",
       "3                0.120780            0.290199            0.140761   \n",
       "4                0.118690            0.289892            0.145317   \n",
       "...                   ...                 ...                 ...   \n",
       "94742            0.170155            0.380321            0.356490   \n",
       "94743            0.169232            0.385538            0.349213   \n",
       "94744            0.168109            0.391031            0.341711   \n",
       "94745            0.166513            0.395388            0.338151   \n",
       "94746            0.165028            0.400629            0.331023   \n",
       "\n",
       "       ENSMUST00000047328  ENSMUST00000057021  ENSMUST00000090697  \\\n",
       "0                0.623594            0.242590            0.074589   \n",
       "1                0.619720            0.238866            0.078791   \n",
       "2                0.614887            0.234399            0.082712   \n",
       "3                0.609180            0.229255            0.086308   \n",
       "4                0.602782            0.223699            0.089497   \n",
       "...                   ...                 ...                 ...   \n",
       "94742            0.537902            0.281510            0.095585   \n",
       "94743            0.542164            0.289974            0.091108   \n",
       "94744            0.546342            0.298262            0.086795   \n",
       "94745            0.544088            0.299778            0.083737   \n",
       "94746            0.546551            0.306439            0.080016   \n",
       "\n",
       "       ENSMUST00000091554  ENSMUST00000162772  ENSMUST00000021284  \\\n",
       "0                0.084546            0.082760            0.323102   \n",
       "1                0.087023            0.086119            0.317896   \n",
       "2                0.089679            0.089638            0.312165   \n",
       "3                0.092639            0.093405            0.305956   \n",
       "4                0.095860            0.097383            0.299470   \n",
       "...                   ...                 ...                 ...   \n",
       "94742            0.233756            0.189178            0.395402   \n",
       "94743            0.232701            0.176244            0.399768   \n",
       "94744            0.231340            0.165354            0.403440   \n",
       "94745            0.233229            0.159442            0.401881   \n",
       "94746            0.231559            0.152272            0.403618   \n",
       "\n",
       "       ENSMUST00000022195  ...  ENSMUST00000109964  ENSMUST00000114553  \\\n",
       "0                0.056296  ...            0.028234            0.008777   \n",
       "1                0.055298  ...            0.028362            0.008728   \n",
       "2                0.054260  ...            0.028319            0.008792   \n",
       "3                0.053218  ...            0.028152            0.008972   \n",
       "4                0.052184  ...            0.027889            0.009264   \n",
       "...                   ...  ...                 ...                 ...   \n",
       "94742            0.047084  ...            0.015559            0.061364   \n",
       "94743            0.044931  ...            0.014296            0.059693   \n",
       "94744            0.042890  ...            0.013114            0.058164   \n",
       "94745            0.041783  ...            0.012161            0.057034   \n",
       "94746            0.040139  ...            0.011186            0.055776   \n",
       "\n",
       "       ENSMUST00000152412  ENSMUST00000159365  ENSMUST00000175965  \\\n",
       "0                0.023342            0.219471            0.014906   \n",
       "1                0.025231            0.221363            0.015748   \n",
       "2                0.027318            0.223340            0.016716   \n",
       "3                0.029628            0.225443            0.017806   \n",
       "4                0.032130            0.227648            0.018977   \n",
       "...                   ...                 ...                 ...   \n",
       "94742            0.013411            0.123024            0.035987   \n",
       "94743            0.012953            0.125018            0.036701   \n",
       "94744            0.012628            0.127244            0.037463   \n",
       "94745            0.012614            0.129440            0.038170   \n",
       "94746            0.012629            0.131944            0.038872   \n",
       "\n",
       "       ENSMUST00000196378  ENSMUST00000228095  ENSMUST00000000219  \\\n",
       "0                0.242949            0.012779            0.019391   \n",
       "1                0.246850            0.012993            0.019144   \n",
       "2                0.249978            0.013238            0.019240   \n",
       "3                0.252181            0.013485            0.019686   \n",
       "4                0.253397            0.013693            0.020454   \n",
       "...                   ...                 ...                 ...   \n",
       "94742            0.144793            0.012435            0.033792   \n",
       "94743            0.143519            0.011728            0.031828   \n",
       "94744            0.141764            0.011042            0.029889   \n",
       "94745            0.138102            0.010601            0.028355   \n",
       "94746            0.135608            0.010006            0.026557   \n",
       "\n",
       "       ENSMUST00000035577  ENSMUST00000060943  \n",
       "0                0.035380            0.037051  \n",
       "1                0.036661            0.038754  \n",
       "2                0.037741            0.040391  \n",
       "3                0.038572            0.042067  \n",
       "4                0.039100            0.043767  \n",
       "...                   ...                 ...  \n",
       "94742            0.030281            0.008782  \n",
       "94743            0.029679            0.007909  \n",
       "94744            0.029413            0.007109  \n",
       "94745            0.029906            0.006518  \n",
       "94746            0.030300            0.005878  \n",
       "\n",
       "[94747 rows x 500 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregated_gene_data_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LPC O-16:2</th>\n",
       "      <th>LPC 16:0_dup</th>\n",
       "      <th>LPC O- 18:3</th>\n",
       "      <th>LPC O-18:2</th>\n",
       "      <th>LPC O-16:2_dup</th>\n",
       "      <th>LPC 15:1</th>\n",
       "      <th>LPC 18:1</th>\n",
       "      <th>LPC 18:0_dup</th>\n",
       "      <th>LPC 16:0</th>\n",
       "      <th>LPC O-18:3</th>\n",
       "      <th>...</th>\n",
       "      <th>SM(t42:1)</th>\n",
       "      <th>PC(40:7)</th>\n",
       "      <th>PC 40:6_dup</th>\n",
       "      <th>PG(42:6)</th>\n",
       "      <th>Hex2Cer 32:0</th>\n",
       "      <th>SHexCer 38:1;3</th>\n",
       "      <th>PE(44:11(OH))</th>\n",
       "      <th>PC(40:4)</th>\n",
       "      <th>PS(40:4)</th>\n",
       "      <th>PIP(O-36:5)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>section12_pixel23_121</th>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000197</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000241</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>section12_pixel23_122</th>\n",
       "      <td>0.000213</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.000204</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.000395</td>\n",
       "      <td>0.000208</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000268</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>section12_pixel23_123</th>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000232</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.000233</td>\n",
       "      <td>0.000203</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000232</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>section12_pixel23_124</th>\n",
       "      <td>0.000147</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000229</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.000285</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000255</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>section12_pixel23_125</th>\n",
       "      <td>0.000229</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000247</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000323</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>section12_pixel308_157</th>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>section12_pixel308_158</th>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>section12_pixel308_159</th>\n",
       "      <td>0.000155</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>section12_pixel308_160</th>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000237</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>section12_pixel308_161</th>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000335</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>94747 rows × 202 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        LPC O-16:2  LPC 16:0_dup  LPC O- 18:3  LPC O-18:2  \\\n",
       "section12_pixel23_121     0.000140      0.000112     0.000116    0.000125   \n",
       "section12_pixel23_122     0.000213      0.000112     0.000114    0.000125   \n",
       "section12_pixel23_123     0.000154      0.000100     0.000117    0.000134   \n",
       "section12_pixel23_124     0.000147      0.000113     0.000114    0.000136   \n",
       "section12_pixel23_125     0.000229      0.000112     0.000115    0.000206   \n",
       "...                            ...           ...          ...         ...   \n",
       "section12_pixel308_157    0.000139      0.000100     0.000100    0.000100   \n",
       "section12_pixel308_158    0.000139      0.000100     0.000100    0.000100   \n",
       "section12_pixel308_159    0.000155      0.000112     0.000117    0.000100   \n",
       "section12_pixel308_160    0.000141      0.000113     0.000100    0.000100   \n",
       "section12_pixel308_161    0.000163      0.000139     0.000100    0.000141   \n",
       "\n",
       "                        LPC O-16:2_dup  LPC 15:1  LPC 18:1  LPC 18:0_dup  \\\n",
       "section12_pixel23_121         0.000214  0.000100    0.0001      0.000197   \n",
       "section12_pixel23_122         0.000204  0.000162    0.0001      0.000100   \n",
       "section12_pixel23_123         0.000195  0.000151    0.0001      0.000232   \n",
       "section12_pixel23_124         0.000229  0.000154    0.0001      0.000100   \n",
       "section12_pixel23_125         0.000100  0.000100    0.0001      0.000100   \n",
       "...                                ...       ...       ...           ...   \n",
       "section12_pixel308_157        0.000100  0.000100    0.0001      0.000100   \n",
       "section12_pixel308_158        0.000100  0.000100    0.0001      0.000100   \n",
       "section12_pixel308_159        0.000100  0.000100    0.0001      0.000100   \n",
       "section12_pixel308_160        0.000100  0.000100    0.0001      0.000237   \n",
       "section12_pixel308_161        0.000100  0.000100    0.0001      0.000335   \n",
       "\n",
       "                        LPC 16:0  LPC O-18:3  ...  SM(t42:1)   PC(40:7)   \\\n",
       "section12_pixel23_121   0.000179      0.0001  ...      0.0001   0.000100   \n",
       "section12_pixel23_122   0.000181      0.0001  ...      0.0001   0.000114   \n",
       "section12_pixel23_123   0.000179      0.0001  ...      0.0001   0.000114   \n",
       "section12_pixel23_124   0.000120      0.0001  ...      0.0001   0.000114   \n",
       "section12_pixel23_125   0.000122      0.0001  ...      0.0001   0.000100   \n",
       "...                          ...         ...  ...         ...        ...   \n",
       "section12_pixel308_157  0.000100      0.0001  ...      0.0001   0.000100   \n",
       "section12_pixel308_158  0.000119      0.0001  ...      0.0001   0.000100   \n",
       "section12_pixel308_159  0.000100      0.0001  ...      0.0001   0.000100   \n",
       "section12_pixel308_160  0.000119      0.0001  ...      0.0001   0.000100   \n",
       "section12_pixel308_161  0.000119      0.0001  ...      0.0001   0.000100   \n",
       "\n",
       "                        PC 40:6_dup  PG(42:6)   Hex2Cer 32:0  SHexCer 38:1;3  \\\n",
       "section12_pixel23_121      0.000241   0.000179        0.0001        0.000100   \n",
       "section12_pixel23_122      0.000395   0.000208        0.0001        0.000316   \n",
       "section12_pixel23_123      0.000233   0.000203        0.0001        0.000100   \n",
       "section12_pixel23_124      0.000285   0.000187        0.0001        0.000255   \n",
       "section12_pixel23_125      0.000247   0.000179        0.0001        0.000323   \n",
       "...                             ...        ...           ...             ...   \n",
       "section12_pixel308_157     0.000100   0.000100        0.0001        0.000100   \n",
       "section12_pixel308_158     0.000100   0.000100        0.0001        0.000100   \n",
       "section12_pixel308_159     0.000100   0.000100        0.0001        0.000100   \n",
       "section12_pixel308_160     0.000100   0.000100        0.0001        0.000100   \n",
       "section12_pixel308_161     0.000100   0.000100        0.0001        0.000100   \n",
       "\n",
       "                        PE(44:11(OH))   PC(40:4)   PS(40:4)   PIP(O-36:5)   \n",
       "section12_pixel23_121           0.0001   0.000261     0.0001      0.000360  \n",
       "section12_pixel23_122           0.0001   0.000268     0.0001      0.000100  \n",
       "section12_pixel23_123           0.0001   0.000232     0.0001      0.000100  \n",
       "section12_pixel23_124           0.0001   0.000100     0.0001      0.000366  \n",
       "section12_pixel23_125           0.0001   0.000100     0.0001      0.000100  \n",
       "...                                ...        ...        ...           ...  \n",
       "section12_pixel308_157          0.0001   0.000100     0.0001      0.000100  \n",
       "section12_pixel308_158          0.0001   0.000100     0.0001      0.000100  \n",
       "section12_pixel308_159          0.0001   0.000100     0.0001      0.000100  \n",
       "section12_pixel308_160          0.0001   0.000100     0.0001      0.000100  \n",
       "section12_pixel308_161          0.0001   0.000100     0.0001      0.000100  \n",
       "\n",
       "[94747 rows x 202 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "section_12_lipids_only = lipids_section_12.iloc[:, 3:-3]\n",
    "section_12_lipids_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_gene_data_df = aggregated_gene_data_df.reset_index(drop=True)\n",
    "section_12_lipids_only = section_12_lipids_only.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simplify lipid names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_lipid(lipid_name):\n",
    "    # Example renaming scheme - customize as needed\n",
    "    new_name = lipid_name.replace(':', ';').replace('\\xa0', ' ')\n",
    "    return new_name\n",
    "\n",
    "section_12_lipids_only.columns = [rename_lipid(lipid) for lipid in section_12_lipids_only.columns] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Train/Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the features and target dataframes\n",
    "features_df = aggregated_gene_data_df.copy()\n",
    "target_df = section_12_lipids_only.copy()\n",
    "\n",
    "# Rename the columns of target_df to ensure uniqueness\n",
    "unique_column_names = [f'{name}_{i}' for i, name in enumerate(target_df.columns)]\n",
    "target_df.columns = unique_column_names\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_df, target_df, test_size=0.3, random_state=42)\n",
    "\n",
    "# Export the full training and test sets to .h5 files\n",
    "X_train.to_hdf('data/train_features.h5', key='X_train', mode='w')\n",
    "X_test.to_hdf('data/test_features.h5', key='X_test', mode='w')\n",
    "y_train.to_hdf('data/train_targets.h5', key='y_train', mode='w')\n",
    "y_test.to_hdf('data/test_targets.h5', key='y_test', mode='w')\n",
    "\n",
    "# Restoring the names\n",
    "new_column_names = [re.sub(r'_(\\d+)$', '', col) for col in target_df.columns]\n",
    "target_df.columns = new_column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting best model and training it and see the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(y_train.columns)):\n",
    "    # Extract the column name for the current index\n",
    "    lipid_name = y_train.columns[i]\n",
    "\n",
    "    # Concatenate the lipid column with the training and testing features\n",
    "    train_data = pd.concat([X_train, y_train.iloc[:, i]], axis=1)\n",
    "    test_data = pd.concat([X_test, y_test.iloc[:, i]], axis=1)\n",
    "    \n",
    "    # Setup PyCaret for each lipid\n",
    "    # Ensure that the test dataset is correctly specified\n",
    "    setup(data=train_data, test_data=test_data, \n",
    "          fold=5, session_id=42, use_gpu=True, verbose=False, preprocess=False)\n",
    "\n",
    "    # Create and plot the model\n",
    "    model = create_model('catboost')\n",
    "    plot_model(model, plot='feature')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating One Model using K-Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Fatal] CUDA Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_CUDA=1\n",
      "[LightGBM] [Fatal] CUDA Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_CUDA=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Fatal] CUDA Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_CUDA=1\n",
      "[LightGBM] [Fatal] CUDA Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_CUDA=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Fatal] CUDA Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_CUDA=1\n",
      "[LightGBM] [Fatal] CUDA Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_CUDA=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Fatal] CUDA Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_CUDA=1\n",
      "[LightGBM] [Fatal] CUDA Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_CUDA=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Fatal] CUDA Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_CUDA=1\n",
      "[LightGBM] [Fatal] CUDA Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_CUDA=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Fatal] CUDA Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_CUDA=1\n",
      "[LightGBM] [Fatal] CUDA Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_CUDA=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Fatal] CUDA Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_CUDA=1\n",
      "[LightGBM] [Fatal] CUDA Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_CUDA=1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Initiated</th>\n",
       "      <td>. . . . . . . . . . . . . . . . . .</td>\n",
       "      <td>21:55:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Status</th>\n",
       "      <td>. . . . . . . . . . . . . . . . . .</td>\n",
       "      <td>Fitting 5 Folds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Estimator</th>\n",
       "      <td>. . . . . . . . . . . . . . . . . .</td>\n",
       "      <td>CatBoost Regressor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                  \n",
       "                                                                  \n",
       "Initiated  . . . . . . . . . . . . . . . . . .            21:55:40\n",
       "Status     . . . . . . . . . . . . . . . . . .     Fitting 5 Folds\n",
       "Estimator  . . . . . . . . . . . . . . . . . .  CatBoost Regressor"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "668951067ab3420f842b21e33eddaabc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/jules/ml-project-2-genelipids/models.ipynb Cell 34\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/jules/ml-project-2-genelipids/models.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m setup(data\u001b[39m=\u001b[39mtrain_data, test_data\u001b[39m=\u001b[39mtest_data, \n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/jules/ml-project-2-genelipids/models.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m       fold\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, session_id\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m, use_gpu\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, verbose\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, preprocess\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/jules/ml-project-2-genelipids/models.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# Create and plot the model\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/jules/ml-project-2-genelipids/models.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m model \u001b[39m=\u001b[39m create_model(\u001b[39m'\u001b[39;49m\u001b[39mcatboost\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/jules/ml-project-2-genelipids/models.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# Retrieving cross-validation results\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/jules/ml-project-2-genelipids/models.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m metrics \u001b[39m=\u001b[39m pull()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/pycaret/utils/generic.py:965\u001b[0m, in \u001b[0;36mcheck_if_global_is_not_none.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[39mif\u001b[39;00m globals_d[name] \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    964\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(message)\n\u001b[0;32m--> 965\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/pycaret/regression/functional.py:990\u001b[0m, in \u001b[0;36mcreate_model\u001b[0;34m(estimator, fold, round, cross_validation, fit_kwargs, groups, experiment_custom_tags, engine, verbose, return_train_score, **kwargs)\u001b[0m\n\u001b[1;32m    862\u001b[0m \u001b[39m@check_if_global_is_not_none\u001b[39m(\u001b[39mglobals\u001b[39m(), _CURRENT_EXPERIMENT_DECORATOR_DICT)\n\u001b[1;32m    863\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate_model\u001b[39m(\n\u001b[1;32m    864\u001b[0m     estimator: Union[\u001b[39mstr\u001b[39m, Any],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    874\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    875\u001b[0m ):\n\u001b[1;32m    876\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[39m    This function trains and evaluates the performance of a given estimator\u001b[39;00m\n\u001b[1;32m    878\u001b[0m \u001b[39m    using cross validation. The output of this function is a score grid with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    987\u001b[0m \n\u001b[1;32m    988\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 990\u001b[0m     \u001b[39mreturn\u001b[39;00m _CURRENT_EXPERIMENT\u001b[39m.\u001b[39;49mcreate_model(\n\u001b[1;32m    991\u001b[0m         estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[1;32m    992\u001b[0m         fold\u001b[39m=\u001b[39;49mfold,\n\u001b[1;32m    993\u001b[0m         \u001b[39mround\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mround\u001b[39;49m,\n\u001b[1;32m    994\u001b[0m         cross_validation\u001b[39m=\u001b[39;49mcross_validation,\n\u001b[1;32m    995\u001b[0m         fit_kwargs\u001b[39m=\u001b[39;49mfit_kwargs,\n\u001b[1;32m    996\u001b[0m         groups\u001b[39m=\u001b[39;49mgroups,\n\u001b[1;32m    997\u001b[0m         experiment_custom_tags\u001b[39m=\u001b[39;49mexperiment_custom_tags,\n\u001b[1;32m    998\u001b[0m         engine\u001b[39m=\u001b[39;49mengine,\n\u001b[1;32m    999\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   1000\u001b[0m         return_train_score\u001b[39m=\u001b[39;49mreturn_train_score,\n\u001b[1;32m   1001\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   1002\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/pycaret/regression/oop.py:1285\u001b[0m, in \u001b[0;36mRegressionExperiment.create_model\u001b[0;34m(self, estimator, fold, round, cross_validation, fit_kwargs, groups, experiment_custom_tags, engine, verbose, return_train_score, **kwargs)\u001b[0m\n\u001b[1;32m   1282\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_engine(estimator\u001b[39m=\u001b[39mestimator, engine\u001b[39m=\u001b[39mengine, severity\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1284\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1285\u001b[0m     return_values \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate_model(\n\u001b[1;32m   1286\u001b[0m         estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[1;32m   1287\u001b[0m         fold\u001b[39m=\u001b[39;49mfold,\n\u001b[1;32m   1288\u001b[0m         \u001b[39mround\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mround\u001b[39;49m,\n\u001b[1;32m   1289\u001b[0m         cross_validation\u001b[39m=\u001b[39;49mcross_validation,\n\u001b[1;32m   1290\u001b[0m         fit_kwargs\u001b[39m=\u001b[39;49mfit_kwargs,\n\u001b[1;32m   1291\u001b[0m         groups\u001b[39m=\u001b[39;49mgroups,\n\u001b[1;32m   1292\u001b[0m         experiment_custom_tags\u001b[39m=\u001b[39;49mexperiment_custom_tags,\n\u001b[1;32m   1293\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   1294\u001b[0m         return_train_score\u001b[39m=\u001b[39;49mreturn_train_score,\n\u001b[1;32m   1295\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   1296\u001b[0m     )\n\u001b[1;32m   1297\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1298\u001b[0m     \u001b[39mif\u001b[39;00m engine \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1299\u001b[0m         \u001b[39m# Reset the models back to the default engines\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/pycaret/internal/pycaret_experiment/supervised_experiment.py:1766\u001b[0m, in \u001b[0;36m_SupervisedExperiment.create_model\u001b[0;34m(self, estimator, fold, round, cross_validation, predict, fit_kwargs, groups, refit, probability_threshold, experiment_custom_tags, verbose, return_train_score, **kwargs)\u001b[0m\n\u001b[1;32m   1754\u001b[0m \u001b[39m# TODO improve error message\u001b[39;00m\n\u001b[1;32m   1755\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39many\u001b[39m(\n\u001b[1;32m   1756\u001b[0m     x\n\u001b[1;32m   1757\u001b[0m     \u001b[39min\u001b[39;00m (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1764\u001b[0m     \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m kwargs\n\u001b[1;32m   1765\u001b[0m )\n\u001b[0;32m-> 1766\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_model(\n\u001b[1;32m   1767\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[1;32m   1768\u001b[0m     fold\u001b[39m=\u001b[39;49mfold,\n\u001b[1;32m   1769\u001b[0m     \u001b[39mround\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mround\u001b[39;49m,\n\u001b[1;32m   1770\u001b[0m     cross_validation\u001b[39m=\u001b[39;49mcross_validation,\n\u001b[1;32m   1771\u001b[0m     predict\u001b[39m=\u001b[39;49mpredict,\n\u001b[1;32m   1772\u001b[0m     fit_kwargs\u001b[39m=\u001b[39;49mfit_kwargs,\n\u001b[1;32m   1773\u001b[0m     groups\u001b[39m=\u001b[39;49mgroups,\n\u001b[1;32m   1774\u001b[0m     refit\u001b[39m=\u001b[39;49mrefit,\n\u001b[1;32m   1775\u001b[0m     probability_threshold\u001b[39m=\u001b[39;49mprobability_threshold,\n\u001b[1;32m   1776\u001b[0m     experiment_custom_tags\u001b[39m=\u001b[39;49mexperiment_custom_tags,\n\u001b[1;32m   1777\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   1778\u001b[0m     return_train_score\u001b[39m=\u001b[39;49mreturn_train_score,\n\u001b[1;32m   1779\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   1780\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/pycaret/internal/pycaret_experiment/supervised_experiment.py:1533\u001b[0m, in \u001b[0;36m_SupervisedExperiment._create_model\u001b[0;34m(self, estimator, fold, round, cross_validation, predict, fit_kwargs, groups, refit, probability_threshold, experiment_custom_tags, verbose, system, add_to_model_list, X_train_data, y_train_data, metrics, display, model_only, return_train_score, error_score, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m         \u001b[39mreturn\u001b[39;00m model, model_fit_time\n\u001b[1;32m   1531\u001b[0m     \u001b[39mreturn\u001b[39;00m model\n\u001b[0;32m-> 1533\u001b[0m model, model_fit_time, model_results, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_model_with_cv(\n\u001b[1;32m   1534\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m   1535\u001b[0m     data_X\u001b[39m=\u001b[39;49mdata_X,\n\u001b[1;32m   1536\u001b[0m     data_y\u001b[39m=\u001b[39;49mdata_y,\n\u001b[1;32m   1537\u001b[0m     fit_kwargs\u001b[39m=\u001b[39;49mfit_kwargs,\n\u001b[1;32m   1538\u001b[0m     \u001b[39mround\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mround\u001b[39;49m,\n\u001b[1;32m   1539\u001b[0m     cv\u001b[39m=\u001b[39;49mcv,\n\u001b[1;32m   1540\u001b[0m     groups\u001b[39m=\u001b[39;49mgroups,\n\u001b[1;32m   1541\u001b[0m     metrics\u001b[39m=\u001b[39;49mmetrics,\n\u001b[1;32m   1542\u001b[0m     refit\u001b[39m=\u001b[39;49mrefit,\n\u001b[1;32m   1543\u001b[0m     system\u001b[39m=\u001b[39;49msystem,\n\u001b[1;32m   1544\u001b[0m     display\u001b[39m=\u001b[39;49mdisplay,\n\u001b[1;32m   1545\u001b[0m     error_score\u001b[39m=\u001b[39;49merror_score,\n\u001b[1;32m   1546\u001b[0m     return_train_score\u001b[39m=\u001b[39;49mreturn_train_score,\n\u001b[1;32m   1547\u001b[0m )\n\u001b[1;32m   1549\u001b[0m \u001b[39m# end runtime\u001b[39;00m\n\u001b[1;32m   1550\u001b[0m runtime_end \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/pycaret/internal/pycaret_experiment/supervised_experiment.py:1126\u001b[0m, in \u001b[0;36m_SupervisedExperiment._create_model_with_cv\u001b[0;34m(self, model, data_X, data_y, fit_kwargs, round, cv, groups, metrics, refit, system, display, error_score, return_train_score)\u001b[0m\n\u001b[1;32m   1124\u001b[0m     model_fit_start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m   1125\u001b[0m     \u001b[39mwith\u001b[39;00m redirect_output(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogger):\n\u001b[0;32m-> 1126\u001b[0m         scores \u001b[39m=\u001b[39m cross_validate(\n\u001b[1;32m   1127\u001b[0m             pipeline_with_model,\n\u001b[1;32m   1128\u001b[0m             data_X,\n\u001b[1;32m   1129\u001b[0m             data_y,\n\u001b[1;32m   1130\u001b[0m             cv\u001b[39m=\u001b[39;49mcv,\n\u001b[1;32m   1131\u001b[0m             groups\u001b[39m=\u001b[39;49mgroups,\n\u001b[1;32m   1132\u001b[0m             scoring\u001b[39m=\u001b[39;49mmetrics_dict,\n\u001b[1;32m   1133\u001b[0m             fit_params\u001b[39m=\u001b[39;49mfit_kwargs,\n\u001b[1;32m   1134\u001b[0m             n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m   1135\u001b[0m             return_train_score\u001b[39m=\u001b[39;49mreturn_train_score,\n\u001b[1;32m   1136\u001b[0m             error_score\u001b[39m=\u001b[39;49merror_score,\n\u001b[1;32m   1137\u001b[0m         )\n\u001b[1;32m   1139\u001b[0m model_fit_end \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m   1140\u001b[0m model_fit_time \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(model_fit_end \u001b[39m-\u001b[39m model_fit_start)\u001b[39m.\u001b[39mround(\u001b[39m2\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:266\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[39m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[1;32m    265\u001b[0m parallel \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39mn_jobs, verbose\u001b[39m=\u001b[39mverbose, pre_dispatch\u001b[39m=\u001b[39mpre_dispatch)\n\u001b[0;32m--> 266\u001b[0m results \u001b[39m=\u001b[39m parallel(\n\u001b[1;32m    267\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    268\u001b[0m         clone(estimator),\n\u001b[1;32m    269\u001b[0m         X,\n\u001b[1;32m    270\u001b[0m         y,\n\u001b[1;32m    271\u001b[0m         scorers,\n\u001b[1;32m    272\u001b[0m         train,\n\u001b[1;32m    273\u001b[0m         test,\n\u001b[1;32m    274\u001b[0m         verbose,\n\u001b[1;32m    275\u001b[0m         \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    276\u001b[0m         fit_params,\n\u001b[1;32m    277\u001b[0m         return_train_score\u001b[39m=\u001b[39;49mreturn_train_score,\n\u001b[1;32m    278\u001b[0m         return_times\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    279\u001b[0m         return_estimator\u001b[39m=\u001b[39;49mreturn_estimator,\n\u001b[1;32m    280\u001b[0m         error_score\u001b[39m=\u001b[39;49merror_score,\n\u001b[1;32m    281\u001b[0m     )\n\u001b[1;32m    282\u001b[0m     \u001b[39mfor\u001b[39;49;00m train, test \u001b[39min\u001b[39;49;00m cv\u001b[39m.\u001b[39;49msplit(X, y, groups)\n\u001b[1;32m    283\u001b[0m )\n\u001b[1;32m    285\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[1;32m    287\u001b[0m \u001b[39m# For callabe scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[39m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[39m# the correct key.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/joblib/parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1861\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1862\u001b[0m     \u001b[39mnext\u001b[39m(output)\n\u001b[0;32m-> 1863\u001b[0m     \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39;49m(output)\n\u001b[1;32m   1865\u001b[0m \u001b[39m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1866\u001b[0m \u001b[39m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1867\u001b[0m \u001b[39m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m \u001b[39m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1869\u001b[0m \u001b[39m# callback.\u001b[39;00m\n\u001b[1;32m   1870\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/joblib/parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1790\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_batches \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1791\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m-> 1792\u001b[0m res \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1793\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_completed_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1794\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/sklearn/utils/parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[1;32m    122\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[0;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/pycaret/internal/patches/sklearn.py:124\u001b[0m, in \u001b[0;36mfit_and_score\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[39mwith\u001b[39;00m patch(\u001b[39m\"\u001b[39m\u001b[39msklearn.model_selection._validation._score\u001b[39m\u001b[39m\"\u001b[39m, score(_score)):\n\u001b[1;32m    122\u001b[0m         \u001b[39mreturn\u001b[39;00m _fit_and_score(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 124\u001b[0m \u001b[39mreturn\u001b[39;00m wrapper(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/pycaret/internal/patches/sklearn.py:122\u001b[0m, in \u001b[0;36mfit_and_score.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mdict\u001b[39m:\n\u001b[1;32m    121\u001b[0m     \u001b[39mwith\u001b[39;00m patch(\u001b[39m\"\u001b[39m\u001b[39msklearn.model_selection._validation._score\u001b[39m\u001b[39m\"\u001b[39m, score(_score)):\n\u001b[0;32m--> 122\u001b[0m         \u001b[39mreturn\u001b[39;00m _fit_and_score(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:686\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    684\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[1;32m    685\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 686\u001b[0m         estimator\u001b[39m.\u001b[39;49mfit(X_train, y_train, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[1;32m    688\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m    689\u001b[0m     \u001b[39m# Note fit time as time until error\u001b[39;00m\n\u001b[1;32m    690\u001b[0m     fit_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/pycaret/internal/pipeline.py:275\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpassthrough\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    274\u001b[0m     fit_params_last_step \u001b[39m=\u001b[39m fit_params_steps[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m]]\n\u001b[0;32m--> 275\u001b[0m     fitted_estimator \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_memory_fit(\n\u001b[1;32m    276\u001b[0m         clone(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msteps[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m][\u001b[39m1\u001b[39;49m]), X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params_last_step\n\u001b[1;32m    277\u001b[0m     )\n\u001b[1;32m    278\u001b[0m     \u001b[39m# Hacky way to make sure that the state of the estimator\u001b[39;00m\n\u001b[1;32m    279\u001b[0m     \u001b[39m# loaded from cache is carried over to the estimator\u001b[39;00m\n\u001b[1;32m    280\u001b[0m     \u001b[39m# in steps\u001b[39;00m\n\u001b[1;32m    281\u001b[0m     _copy_estimator_state(fitted_estimator, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m1\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/joblib/memory.py:353\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 353\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/pycaret/internal/pipeline.py:68\u001b[0m, in \u001b[0;36m_fit_one\u001b[0;34m(transformer, X, y, message, **fit_params)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m signature(transformer\u001b[39m.\u001b[39mfit)\u001b[39m.\u001b[39mparameters:\n\u001b[1;32m     67\u001b[0m             args\u001b[39m.\u001b[39mappend(y)\n\u001b[0;32m---> 68\u001b[0m         transformer\u001b[39m.\u001b[39;49mfit(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[1;32m     69\u001b[0m \u001b[39mreturn\u001b[39;00m transformer\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/catboost/core.py:5703\u001b[0m, in \u001b[0;36mCatBoostRegressor.fit\u001b[0;34m(self, X, y, cat_features, text_features, embedding_features, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[1;32m   5700\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mloss_function\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m params:\n\u001b[1;32m   5701\u001b[0m     CatBoostRegressor\u001b[39m.\u001b[39m_check_is_compatible_loss(params[\u001b[39m'\u001b[39m\u001b[39mloss_function\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m-> 5703\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X, y, cat_features, text_features, embedding_features, \u001b[39mNone\u001b[39;49;00m, sample_weight, \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m, baseline,\n\u001b[1;32m   5704\u001b[0m                  use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description,\n\u001b[1;32m   5705\u001b[0m                  verbose_eval, metric_period, silent, early_stopping_rounds,\n\u001b[1;32m   5706\u001b[0m                  save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/catboost/core.py:2319\u001b[0m, in \u001b[0;36mCatBoost._fit\u001b[0;34m(self, X, y, cat_features, text_features, embedding_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[1;32m   2315\u001b[0m allow_clear_pool \u001b[39m=\u001b[39m train_params[\u001b[39m\"\u001b[39m\u001b[39mallow_clear_pool\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   2317\u001b[0m \u001b[39mwith\u001b[39;00m log_fixup(log_cout, log_cerr), \\\n\u001b[1;32m   2318\u001b[0m     plot_wrapper(plot, plot_file, \u001b[39m'\u001b[39m\u001b[39mTraining plots\u001b[39m\u001b[39m'\u001b[39m, [_get_train_dir(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_params())]):\n\u001b[0;32m-> 2319\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train(\n\u001b[1;32m   2320\u001b[0m         train_pool,\n\u001b[1;32m   2321\u001b[0m         train_params[\u001b[39m\"\u001b[39;49m\u001b[39meval_sets\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m   2322\u001b[0m         params,\n\u001b[1;32m   2323\u001b[0m         allow_clear_pool,\n\u001b[1;32m   2324\u001b[0m         train_params[\u001b[39m\"\u001b[39;49m\u001b[39minit_model\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n\u001b[1;32m   2325\u001b[0m     )\n\u001b[1;32m   2327\u001b[0m \u001b[39m# Have property feature_importance possibly set\u001b[39;00m\n\u001b[1;32m   2328\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_object\u001b[39m.\u001b[39m_get_loss_function_name()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/catboost/core.py:1723\u001b[0m, in \u001b[0;36m_CatBoostBase._train\u001b[0;34m(self, train_pool, test_pool, params, allow_clear_pool, init_model)\u001b[0m\n\u001b[1;32m   1722\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_train\u001b[39m(\u001b[39mself\u001b[39m, train_pool, test_pool, params, allow_clear_pool, init_model):\n\u001b[0;32m-> 1723\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_object\u001b[39m.\u001b[39;49m_train(train_pool, test_pool, params, allow_clear_pool, init_model\u001b[39m.\u001b[39;49m_object \u001b[39mif\u001b[39;49;00m init_model \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m   1724\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_trained_model_attributes()\n",
      "File \u001b[0;32m_catboost.pyx:4645\u001b[0m, in \u001b[0;36m_catboost._CatBoost._train\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_catboost.pyx:4694\u001b[0m, in \u001b[0;36m_catboost._CatBoost._train\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame(columns=['Lipid', 'R2_mean', 'MAPE_mean'])\n",
    "\n",
    "for i, lipid_name in enumerate(y_train.columns):\n",
    "    # Extract the column name for the current index\n",
    "    lipid_name = y_train.columns[i]\n",
    "\n",
    "    # Concatenate the lipid column with the training and testing features\n",
    "    train_data = pd.concat([X_train, y_train.iloc[:, i]], axis=1)\n",
    "    test_data = pd.concat([X_test, y_test.iloc[:, i]], axis=1)\n",
    "    \n",
    "    # Setup PyCaret for each lipid\n",
    "    # Ensure that the test dataset is correctly specified\n",
    "    setup(data=train_data, test_data=test_data, \n",
    "          fold=5, session_id=42, use_gpu=True, verbose=False, preprocess=False)\n",
    "    \n",
    "    # Create and plot the model\n",
    "    model = create_model('catboost')\n",
    "    \n",
    "    # Retrieving cross-validation results\n",
    "    metrics = pull()\n",
    "    \n",
    "    r2_mean = metrics.loc['Mean','R2']\n",
    "    mape_mean = metrics.loc['Mean', 'MAPE']\n",
    "    \n",
    "    print(\"R2 mean: \", r2_mean)\n",
    "    print(\"MAPE mean: \", mape_mean)\n",
    "    \n",
    "    # Append the results to the DataFrame\n",
    "    results_df = results_df.append({'Lipid': lipid_name, 'R2_mean': r2_mean, 'MAPE_mean': mape_mean}, ignore_index=True)\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('results.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-project2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
