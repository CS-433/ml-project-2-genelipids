{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Neural Network for all lipids"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-16 18:42:19.763303: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Users/violarenne/opt/anaconda3/envs/ml-project-2-genelipids/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "/Users/violarenne/opt/anaconda3/envs/ml-project-2-genelipids/lib/python3.10/site-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.13.0 and strictly below 2.16.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.10.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from keras import metrics\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization, Dropout, Activation\n",
    "from keras.initializers import he_normal\n",
    "from keras.regularizers import l2\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.callbacks import EarlyStopping"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Reproducibility"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T19:34:08.341412800Z",
     "start_time": "2023-12-12T19:34:08.163373600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Random seed for reproducibility\n",
    "seed = 42\n",
    "\n",
    "random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "tf.compat.v1.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data loading"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T19:34:10.588414Z",
     "start_time": "2023-12-12T19:34:10.580663600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dataset path \n",
    "dataset_dir = 'data/processed_data'\n",
    "# Training loading\n",
    "training_input_path = os.path.join(dataset_dir, 'train_features.parquet')\n",
    "training_output_path = os.path.join(dataset_dir, 'train_targets.parquet')\n",
    "# Testing loading\n",
    "testing_input_path = os.path.join(dataset_dir, 'test_features.parquet')\n",
    "testing_output_path = os.path.join(dataset_dir, 'test_targets.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T19:34:12.188127200Z",
     "start_time": "2023-12-12T19:34:11.369415Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load dataset into Pandas dataframes\n",
    "training_input = pd.read_parquet(training_input_path)\n",
    "training_output = pd.read_parquet(training_output_path)\n",
    "\n",
    "testing_input = pd.read_parquet(testing_input_path)\n",
    "testing_output = pd.read_parquet(testing_output_path)\n",
    "\n",
    "training_output = training_output * 1000\n",
    "\n",
    "# Number of output nodes (lipids) for the model\n",
    "OUTPUT_NODES = training_output.shape[1]\n",
    "# Number of input nodes (genes) for the model\n",
    "input_dim = training_input.shape[1]\n",
    "# Batch size\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T19:34:18.847395600Z",
     "start_time": "2023-12-12T19:34:18.054833400Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-16 18:42:31.628164: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 512)               256512    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 512)               262656    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 16)                528       \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 16)                272       \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 8)                 72        \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 4)                 36        \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 4)                 20        \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 156)               780       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 783,092\n",
      "Trainable params: 783,092\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.engine.sequential.Sequential at 0x7f9322d30400>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lr_schedule(epoch):\n",
    "    \"\"\"\n",
    "    Set the learning rate considering the epoch's number\n",
    "    :param epoch: epoch's number\n",
    "    :return: learning rate\n",
    "    \"\"\"\n",
    "    initial_learning_rate = 0.1  # Set initial learning rate\n",
    "    decay_factor = 0.9  # Set decay factor\n",
    "    lr = initial_learning_rate * decay_factor ** epoch  # Compute learnign rate\n",
    "    return lr\n",
    "\n",
    "# Create a Sequential model\n",
    "def build_model(summary=False):\n",
    "    \"\"\"\n",
    "    Build the neural network\n",
    "    :param summary: if True, print the summary of the model, if False, do not print\n",
    "    :return: the model\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add the input layer with 500 nodes\n",
    "    model.add(Dense(512, input_dim=input_dim, activation='gelu', kernel_initializer=he_normal(seed=seed)))\n",
    "    model.add(Dense(512, activation='gelu',  kernel_initializer=he_normal(seed=seed)))\n",
    "\n",
    "    model.add(Dense(256, activation='gelu',  kernel_initializer=he_normal(seed=seed)))\n",
    "    model.add(Dense(256, activation='gelu',  kernel_initializer=he_normal(seed=seed)))\n",
    "\n",
    "    model.add(Dense(128, activation='gelu',  kernel_initializer=he_normal(seed=seed)))\n",
    "    model.add(Dense(128, activation='gelu',  kernel_initializer=he_normal(seed=seed)))\n",
    "\n",
    "    model.add(Dense(64, activation='gelu',  kernel_initializer=he_normal(seed=seed)))\n",
    "    model.add(Dense(64, activation='gelu',  kernel_initializer=he_normal(seed=seed)))\n",
    "\n",
    "    model.add(Dense(32, activation='gelu',  kernel_initializer=he_normal(seed=seed)))\n",
    "    model.add(Dense(32, activation='gelu',  kernel_initializer=he_normal(seed=seed)))\n",
    "\n",
    "    model.add(Dense(16, activation='gelu',  kernel_initializer=he_normal(seed=seed)))\n",
    "    model.add(Dense(16, activation='gelu',  kernel_initializer=he_normal(seed=seed)))\n",
    "\n",
    "    model.add(Dense(8, activation='gelu',  kernel_initializer=he_normal(seed=seed)))\n",
    "    model.add(Dense(8, activation='gelu',  kernel_initializer=he_normal(seed=seed)))\n",
    "\n",
    "    model.add(Dense(4, activation='gelu',  kernel_initializer=he_normal(seed=seed)))\n",
    "    model.add(Dense(4, activation='gelu',  kernel_initializer=he_normal(seed=seed)))\n",
    "\n",
    "    # Add the output layer with OUTPUT_NODES nodes (for multiple regression)\n",
    "    model.add(Dense(OUTPUT_NODES, activation='relu'))\n",
    "\n",
    "    if summary:\n",
    "        # Display the model summary\n",
    "        model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "# Print the model summary\n",
    "build_model(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Use early stopping on the validation\n",
    "early_stopping = EarlyStopping(monitor='val_loss',  # Metric chose is validation loss (MSE)\n",
    "                               patience=10,         # Number of epochs with no improvement after which training stops\n",
    "                               restore_best_weights=True)  # Restores model weights from the epoch with the best value of the monitored metric"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def data_generator(X, y, batch_size):\n",
    "    \"\"\"\n",
    "    Data augmentation\n",
    "    :param X: input DataFrame (genes)\n",
    "    :param y: output DataFrame (single lipid)\n",
    "    :param batch_size: size of the mini batch\n",
    "    :return batch_X: dataset containing random modification on the input DataFrame with size corresponding to batch size\n",
    "    :return batch_y: dataset containing output with size corresponding to batch size\n",
    "    \"\"\"\n",
    "    # Set noise std and scale factor for random modifications\n",
    "    noise_std = 0.1\n",
    "    scale_factor_range = 0.3\n",
    "    while True:\n",
    "        indices = np.random.choice(X.shape[0], batch_size, replace=False)\n",
    "        batch_df = X.iloc[indices]\n",
    "\n",
    "        # Create a copy of the batch for augmentation\n",
    "        augmented_batch_df = batch_df.copy()\n",
    "\n",
    "        # Random scaling\n",
    "        scale_factor = np.random.uniform(1 - scale_factor_range, 1 + scale_factor_range)\n",
    "        augmented_batch_df *= scale_factor\n",
    "\n",
    "        # Add Gaussian noise to all features\n",
    "        augmented_batch_df += np.random.normal(loc=0, scale=noise_std, size=augmented_batch_df.shape)\n",
    "\n",
    "        batch_X = augmented_batch_df.values\n",
    "        batch_y = y.iloc[indices].values\n",
    "\n",
    "        yield batch_X, batch_y"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T19:44:35.203613400Z",
     "start_time": "2023-12-12T19:43:17.930797700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/Users/violarenne/opt/anaconda3/envs/ml-project-2-genelipids/lib/python3.10/site-packages/keras/engine/training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/violarenne/opt/anaconda3/envs/ml-project-2-genelipids/lib/python3.10/site-packages/tensorflow_addons/metrics/r_square.py\", line 114, in update_state  *\n        self.squared_sum = self.add_weight(\n    File \"/Users/violarenne/opt/anaconda3/envs/ml-project-2-genelipids/lib/python3.10/site-packages/keras/metrics/base_metric.py\", line 375, in add_weight  **\n        return super().add_weight(\n    File \"/Users/violarenne/opt/anaconda3/envs/ml-project-2-genelipids/lib/python3.10/site-packages/keras/engine/base_layer.py\", line 705, in add_weight\n        variable = self._add_variable_with_custom_getter(\n    File \"/Users/violarenne/opt/anaconda3/envs/ml-project-2-genelipids/lib/python3.10/site-packages/keras/engine/base_layer_utils.py\", line 134, in make_variable\n        return tf1.Variable(\n    File \"/Users/violarenne/opt/anaconda3/envs/ml-project-2-genelipids/lib/python3.10/site-packages/keras/initializers/initializers_v2.py\", line 171, in __call__\n        return tf.zeros(shape, dtype)\n\n    ValueError: Cannot convert a partially known TensorShape (None,) to a Tensor.\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[13], line 31\u001B[0m\n\u001B[1;32m     29\u001B[0m train_generator \u001B[38;5;241m=\u001B[39m data_generator(training_input\u001B[38;5;241m.\u001B[39miloc[input_indices], training_output\u001B[38;5;241m.\u001B[39miloc[input_indices], batch_size)\n\u001B[1;32m     30\u001B[0m \u001B[38;5;66;03m# Train the model\u001B[39;00m\n\u001B[0;32m---> 31\u001B[0m history \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     32\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_generator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     33\u001B[0m \u001B[43m    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m50\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Adjust the number of epochs as needed\u001B[39;49;00m\n\u001B[1;32m     34\u001B[0m \u001B[43m    \u001B[49m\u001B[43msteps_per_epoch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43minput_indices\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     35\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtraining_input\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43miloc\u001B[49m\u001B[43m[\u001B[49m\u001B[43moutput_indices\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtraining_output\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43miloc\u001B[49m\u001B[43m[\u001B[49m\u001B[43moutput_indices\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     36\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mearly_stopping\u001B[49m\u001B[43m]\u001B[49m\n\u001B[1;32m     37\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mhistory\n\u001B[1;32m     39\u001B[0m \u001B[38;5;66;03m# Generate generalization metrics\u001B[39;00m\n\u001B[1;32m     40\u001B[0m scores \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mevaluate(training_input\u001B[38;5;241m.\u001B[39miloc[output_indices], training_output\u001B[38;5;241m.\u001B[39miloc[output_indices], verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/ml-project-2-genelipids/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[1;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[1;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[0;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[0;32m/var/folders/5n/syddp42x51g1n2_q59jchk580000gn/T/__autograph_generated_file1439i10v.py:15\u001B[0m, in \u001B[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001B[0;34m(iterator)\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     14\u001B[0m     do_return \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m---> 15\u001B[0m     retval_ \u001B[38;5;241m=\u001B[39m ag__\u001B[38;5;241m.\u001B[39mconverted_call(ag__\u001B[38;5;241m.\u001B[39mld(step_function), (ag__\u001B[38;5;241m.\u001B[39mld(\u001B[38;5;28mself\u001B[39m), ag__\u001B[38;5;241m.\u001B[39mld(iterator)), \u001B[38;5;28;01mNone\u001B[39;00m, fscope)\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m:\n\u001B[1;32m     17\u001B[0m     do_return \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m/var/folders/5n/syddp42x51g1n2_q59jchk580000gn/T/__autograph_generated_fileychrzd7k.py:20\u001B[0m, in \u001B[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__update_state\u001B[0;34m(self, y_true, y_pred, sample_weight)\u001B[0m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21melse_body\u001B[39m():\n\u001B[1;32m     19\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[0;32m---> 20\u001B[0m \u001B[43mag__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mif_stmt\u001B[49m\u001B[43m(\u001B[49m\u001B[43mag__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnot_\u001B[49m\u001B[43m(\u001B[49m\u001B[43mag__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconverted_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43mag__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mld\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mhasattr\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mag__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mld\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msquared_sum\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfscope\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mif_body\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43melse_body\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mget_state\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mset_state\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mself.squared_sum\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_state_1\u001B[39m():\n\u001B[1;32m     23\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (ag__\u001B[38;5;241m.\u001B[39mldu(\u001B[38;5;28;01mlambda\u001B[39;00m : \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msum, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mself.sum\u001B[39m\u001B[38;5;124m'\u001B[39m),)\n",
      "File \u001B[0;32m/var/folders/5n/syddp42x51g1n2_q59jchk580000gn/T/__autograph_generated_fileychrzd7k.py:16\u001B[0m, in \u001B[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__update_state.<locals>.if_body\u001B[0;34m()\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mif_body\u001B[39m():\n\u001B[0;32m---> 16\u001B[0m     ag__\u001B[38;5;241m.\u001B[39mld(\u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39msquared_sum \u001B[38;5;241m=\u001B[39m \u001B[43mag__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconverted_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43mag__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mld\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd_weight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mdict\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msquared_sum\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshape\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mag__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mld\u001B[49m\u001B[43m(\u001B[49m\u001B[43my_true\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minitializer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mzeros\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mag__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mld\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dtype\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfscope\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mValueError\u001B[0m: in user code:\n\n    File \"/Users/violarenne/opt/anaconda3/envs/ml-project-2-genelipids/lib/python3.10/site-packages/keras/engine/training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/violarenne/opt/anaconda3/envs/ml-project-2-genelipids/lib/python3.10/site-packages/tensorflow_addons/metrics/r_square.py\", line 114, in update_state  *\n        self.squared_sum = self.add_weight(\n    File \"/Users/violarenne/opt/anaconda3/envs/ml-project-2-genelipids/lib/python3.10/site-packages/keras/metrics/base_metric.py\", line 375, in add_weight  **\n        return super().add_weight(\n    File \"/Users/violarenne/opt/anaconda3/envs/ml-project-2-genelipids/lib/python3.10/site-packages/keras/engine/base_layer.py\", line 705, in add_weight\n        variable = self._add_variable_with_custom_getter(\n    File \"/Users/violarenne/opt/anaconda3/envs/ml-project-2-genelipids/lib/python3.10/site-packages/keras/engine/base_layer_utils.py\", line 134, in make_variable\n        return tf1.Variable(\n    File \"/Users/violarenne/opt/anaconda3/envs/ml-project-2-genelipids/lib/python3.10/site-packages/keras/initializers/initializers_v2.py\", line 171, in __call__\n        return tf.zeros(shape, dtype)\n\n    ValueError: Cannot convert a partially known TensorShape (None,) to a Tensor.\n"
     ]
    }
   ],
   "source": [
    "lipid_names = list(map(lambda s: s.strip(), training_output.columns.values))\n",
    "lipids_metrics_avg = pd.DataFrame(columns=['Loss', 'R2'], index=lipid_names)\n",
    "\n",
    "print(f'Start training')\n",
    "num_folds = 5\n",
    "# Define the K-fold Cross Validator\n",
    "k_fold = KFold(n_splits=num_folds, shuffle=True, random_state=seed)\n",
    "\n",
    "# Metrics for each fold\n",
    "loss_per_fold = np.zeros((num_folds,))\n",
    "r2_per_fold = np.zeros((num_folds,))\n",
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "fold_no = 1\n",
    "models = []\n",
    "split_indices = []\n",
    "for input_indices, output_indices in k_fold.split(training_input, training_output):\n",
    "    split_indices.append(output_indices)\n",
    "    # Build the model\n",
    "    model = build_model()\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error',\n",
    "                  metrics=[metrics.mean_squared_error, metrics.mean_absolute_error, tfa.metrics.RSquare(), metrics.mean_absolute_percentage_error])\n",
    "\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "    train_generator = data_generator(training_input.iloc[input_indices], training_output.iloc[input_indices], batch_size)\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        epochs=50,  # Adjust the number of epochs as needed\n",
    "        steps_per_epoch=len(input_indices) // batch_size,\n",
    "        validation_data=(training_input.iloc[output_indices], training_output.iloc[output_indices]),\n",
    "        callbacks=[early_stopping]\n",
    "    ).history\n",
    "\n",
    "    # Generate generalization metrics\n",
    "    scores = model.evaluate(training_input.iloc[output_indices], training_output.iloc[output_indices], verbose=0)\n",
    "    print(scores)\n",
    "    loss_per_fold[fold_no-1] = scores[0]\n",
    "    r2_per_fold[fold_no-1] = scores[3]\n",
    "\n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1\n",
    "\n",
    "mean_loss = loss_per_fold.mean()\n",
    "mean_r2 = r2_per_fold.mean()\n",
    "lipids_metrics_avg.loc[lipid_names[0]] = [mean_loss, mean_r2]\n",
    "print('#'*72)\n",
    "print(f'Finish training for lipid {lipid_names[0]}')\n",
    "print(f'Mean loss: {mean_loss}')\n",
    "print(f'Mean r2: {mean_r2}')\n",
    "\n",
    "lipids_metrics_avg.to_csv('lipids_metrics_avg_neural_network.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T19:40:13.547234Z",
     "start_time": "2023-12-12T19:40:13.512769300Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Column 'Loss' has dtype object, cannot use method 'nsmallest' with this dtype",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[54], line 10\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mformat_values\u001B[39m(df: pd\u001B[38;5;241m.\u001B[39mDataFrame, col: \u001B[38;5;28mstr\u001B[39m, just\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m15\u001B[39m):\n\u001B[1;32m      8\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin([\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mval\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.5e\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mrjust(just) \u001B[38;5;28;01mfor\u001B[39;00m val \u001B[38;5;129;01min\u001B[39;00m df[col]\u001B[38;5;241m.\u001B[39mvalues])\n\u001B[0;32m---> 10\u001B[0m best_losses \u001B[38;5;241m=\u001B[39m \u001B[43mlipids_metrics_avg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnsmallest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mk\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mLoss\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     11\u001B[0m worst_losses \u001B[38;5;241m=\u001B[39m lipids_metrics_avg\u001B[38;5;241m.\u001B[39mnlargest(k, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mLoss\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLoss:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/ml-project-2-genelipids/lib/python3.10/site-packages/pandas/core/frame.py:7490\u001B[0m, in \u001B[0;36mDataFrame.nsmallest\u001B[0;34m(self, n, columns, keep)\u001B[0m\n\u001B[1;32m   7392\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mnsmallest\u001B[39m(\n\u001B[1;32m   7393\u001B[0m     \u001B[38;5;28mself\u001B[39m, n: \u001B[38;5;28mint\u001B[39m, columns: IndexLabel, keep: NsmallestNlargestKeep \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfirst\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   7394\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataFrame:\n\u001B[1;32m   7395\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   7396\u001B[0m \u001B[38;5;124;03m    Return the first `n` rows ordered by `columns` in ascending order.\u001B[39;00m\n\u001B[1;32m   7397\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   7488\u001B[0m \u001B[38;5;124;03m    Nauru         337000  182      NR\u001B[39;00m\n\u001B[1;32m   7489\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 7490\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mselectn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mSelectNFrame\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkeep\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkeep\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnsmallest\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/ml-project-2-genelipids/lib/python3.10/site-packages/pandas/core/methods/selectn.py:61\u001B[0m, in \u001B[0;36mSelectN.nsmallest\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     59\u001B[0m \u001B[38;5;129m@final\u001B[39m\n\u001B[1;32m     60\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mnsmallest\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m---> 61\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mnsmallest\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/ml-project-2-genelipids/lib/python3.10/site-packages/pandas/core/methods/selectn.py:197\u001B[0m, in \u001B[0;36mSelectNFrame.compute\u001B[0;34m(self, method)\u001B[0m\n\u001B[1;32m    195\u001B[0m     dtype \u001B[38;5;241m=\u001B[39m frame[column]\u001B[38;5;241m.\u001B[39mdtype\n\u001B[1;32m    196\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mis_valid_dtype_n_method(dtype):\n\u001B[0;32m--> 197\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[1;32m    198\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mColumn \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mrepr\u001B[39m(column)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m has dtype \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdtype\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    199\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcannot use method \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mrepr\u001B[39m(method)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m with this dtype\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    200\u001B[0m         )\n\u001B[1;32m    202\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_indexer\u001B[39m(current_indexer, other_indexer):\n\u001B[1;32m    203\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    204\u001B[0m \u001B[38;5;124;03m    Helper function to concat `current_indexer` and `other_indexer`\u001B[39;00m\n\u001B[1;32m    205\u001B[0m \u001B[38;5;124;03m    depending on `method`\u001B[39;00m\n\u001B[1;32m    206\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
      "\u001B[0;31mTypeError\u001B[0m: Column 'Loss' has dtype object, cannot use method 'nsmallest' with this dtype"
     ]
    }
   ],
   "source": [
    "# Print k best and worst lipids for average between folds of metrics loss and r2\n",
    "k = 5\n",
    "\n",
    "def format_names(df: pd.DataFrame, just=15):\n",
    "    return ', '.join([name.rjust(just) for name in df.index.values])\n",
    "\n",
    "def format_values(df: pd.DataFrame, col: str, just=15):\n",
    "    return ', '.join([f'{val:.5e}'.rjust(just) for val in df[col].values])\n",
    "\n",
    "best_losses = lipids_metrics_avg.nsmallest(k, 'Loss')\n",
    "worst_losses = lipids_metrics_avg.nlargest(k, 'Loss')\n",
    "print(\"Loss:\")\n",
    "print(\"  Best:\")\n",
    "print(f\"  {format_names(best_losses)}\")\n",
    "print(f\"  {format_values(best_losses, 'Loss')}\")\n",
    "print(\"  Worst:\")\n",
    "print(f\"  {format_names(worst_losses)}\")\n",
    "print(f\"  {format_values(worst_losses, 'Loss')}\")\n",
    "\n",
    "best_r2s = lipids_metrics_avg.nlargest(k, 'R2')\n",
    "worst_r2s = lipids_metrics_avg.nsmallest(k, 'R2')\n",
    "print(\"R2:\")\n",
    "print(\"  Best:\")\n",
    "print(f\"  {format_names(best_r2s)}\")\n",
    "print(f\"  {format_values(best_r2s, 'R2')}\")\n",
    "print(\"  Worst:\")\n",
    "print(f\"  {format_names(worst_r2s)}\")\n",
    "print(f\"  {format_values(worst_r2s, 'R2')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4115045,
     "sourceId": 7132195,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30587,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
