{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from scipy.spatial import cKDTree\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "from pycaret.regression import *\n",
    "from multiprocessing import Pool, cpu_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lipid_path = 'data/section12/lipids_section_12.h5'\n",
    "gene_path = 'data/section12/genes_section_12.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_weight(distance, std_dev):\n",
    "    return norm.pdf(distance, 0, std_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_decay(distance, threshold, decay_rate):\n",
    "    adjusted_distance = distance - threshold\n",
    "    return np.exp(-decay_rate * adjusted_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def filter_dataframe_from_file(df, filepath):\n",
    "    # Read the text file and store the entries in a list\n",
    "    with open(filepath, 'r') as file:\n",
    "        entries = file.read().splitlines()\n",
    "\n",
    "    # Filter the DataFrame based on the index matching the entries\n",
    "    filtered_df = df.loc[df.index.isin(entries)]\n",
    "\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "lipids_section_12 = pd.read_hdf(lipid_path)\n",
    "genes_section_12 = pd.read_hdf(gene_path)\n",
    "\n",
    "# Need to remove the trailing naming scheme added before\n",
    "new_column_names = [re.sub(r'_(\\d+)$', '', col) for col in lipids_section_12.columns]\n",
    "lipids_section_12.columns = new_column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nearest Neighbor selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a KDTree object for the genes\n",
    "genes_coords = genes_section_12[['y_ccf', 'z_ccf']].values\n",
    "genes_kdtree = cKDTree(genes_coords)\n",
    "\n",
    "# Extract coordinates from section_12 for lipids\n",
    "lipids_coords = lipids_section_12[['y_ccf', 'z_ccf']].values\n",
    "\n",
    "# Find the indices of the closest gene for each lipid point\n",
    "_, indices = genes_kdtree.query(lipids_coords, k=1)\n",
    "\n",
    "# Initialize an empty array for aggregated gene data\n",
    "aggregated_gene_data = np.zeros((len(lipids_coords), genes_section_12.iloc[:, 46:-50].shape[1]))\n",
    "\n",
    "# Aggregate gene data based on the closest neighbor\n",
    "for i, gene_index in enumerate(indices):\n",
    "    aggregated_gene_data[i] = genes_section_12.iloc[gene_index, 46:-50]\n",
    "\n",
    "# Convert the aggregated data into a DataFrame\n",
    "aggregated_gene_data_df = pd.DataFrame(aggregated_gene_data, columns=genes_section_12.columns[46:-50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exponential Decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a KDTree object for the genes\n",
    "genes_coords = genes_section_12[['y_ccf', 'z_ccf']].values\n",
    "genes_kdtree = cKDTree(genes_coords)\n",
    "\n",
    "# Extract coordinates from section_12 for lipids\n",
    "lipids_coords = lipids_section_12[['y_ccf', 'z_ccf']].values\n",
    "\n",
    "# Find the distances and indices of the 6 closest genes for each lipid point\n",
    "distances, indices = genes_kdtree.query(lipids_coords, k=6)\n",
    "\n",
    "# Calculate the average distance\n",
    "average_closest_distance = np.mean(distances)\n",
    "\n",
    "# Query to get the 100 closest genes and their distances\n",
    "distances, indices = genes_kdtree.query(lipids_coords, k=1000)\n",
    "\n",
    "# Initialize an empty array for aggregated gene data\n",
    "aggregated_gene_data = np.zeros((len(lipids_coords), genes_section_12.iloc[:, 46:-50].shape[1]))\n",
    "\n",
    "# Perform weighted aggregation\n",
    "for i, (gene_indices, dists) in enumerate(zip(indices, distances)):\n",
    "    # Weights based on distance, with a penalty for distances greater than the average closest distance\n",
    "    weights = np.where(dists <= average_closest_distance, 1, exponential_decay(dists, average_closest_distance, 3))  # Apply penalty for dist > average_closest_distance\n",
    "    weighted_data = genes_section_12.iloc[gene_indices, 46:-50] * weights[:, np.newaxis]\n",
    "    aggregated_gene_data[i] = weighted_data.sum(axis=0) / weights.sum() if weights.sum() > 0 else np.zeros(genes_section_12.iloc[:, 46:-50].shape[1])\n",
    "\n",
    "# Convert the aggregated data into a DataFrame\n",
    "aggregated_gene_data_df = pd.DataFrame(aggregated_gene_data, columns=genes_section_12.columns[46:-50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# K neighbors gaussian mean of genes for a given lipids datapoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a KDTree object for the genes\n",
    "genes_coords = genes_section_12[['y_ccf', 'z_ccf']].values\n",
    "genes_kdtree = cKDTree(genes_coords)\n",
    "\n",
    "# Extract coordinates from section_12 for lipids\n",
    "lipids_coords = lipids_section_12[['y_ccf', 'z_ccf']].values\n",
    "\n",
    "# Find the distances and indices of the closest 6 genes for each lipid point\n",
    "distances, indices = genes_kdtree.query(lipids_coords, k=6)\n",
    "\n",
    "# Calculate the std of the distances\n",
    "std_closest_distance = np.std(distances)\n",
    "\n",
    "# Query to get the 100 closest genes and their distances\n",
    "distances, indices = genes_kdtree.query(lipids_coords, k=1000)\n",
    "\n",
    "# Initialize an empty array for aggregated gene data\n",
    "aggregated_gene_data = np.zeros((len(lipids_coords), genes_section_12.iloc[:, 46:-50].shape[1]))\n",
    "\n",
    "# Perform weighted aggregation\n",
    "for i, (gene_indices, dists) in enumerate(zip(indices, distances)):\n",
    "    # Weights based on distance, with a penalty for distances greater than the average closest distance\n",
    "    weights = gaussian_weight(dists, std_closest_distance)\n",
    "    weighted_data = genes_section_12.iloc[gene_indices, 46:-50] * weights[:, np.newaxis]\n",
    "    aggregated_gene_data[i] = weighted_data.sum(axis=0) / weights.sum() if weights.sum() > 0 else np.zeros(genes_section_12.iloc[:, 46:-50].shape[1])\n",
    "\n",
    "# Convert the aggregated data into a DataFrame\n",
    "aggregated_gene_data_df = pd.DataFrame(aggregated_gene_data, columns=genes_section_12.columns[46:-50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take all the points into consideration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_lipid_point(lipid_coord, genes_coords, gene_data, std_closest_distance):\n",
    "    \"\"\"Process a single lipid point and return the aggregated data.\"\"\"\n",
    "    # Calculate distances to all gene points\n",
    "    dists = np.linalg.norm(genes_coords - lipid_coord, axis=1)\n",
    "\n",
    "    # Weights based on distance, with a penalty for distances greater than the average closest distance\n",
    "    weights = gaussian_weight(dists, std_closest_distance)\n",
    "\n",
    "    # Perform weighted aggregation\n",
    "    weighted_data = gene_data * weights[:, np.newaxis]\n",
    "    return weighted_data.sum(axis=0) / weights.sum() if weights.sum() > 0 else np.zeros(gene_data.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lipids_coords = lipids_section_12[['y_ccf', 'z_ccf']].values\n",
    "genes_coords = genes_section_12[['y_ccf', 'z_ccf']].values\n",
    "\n",
    "gene_data_columns = genes_section_12.columns[46:-50]\n",
    "gene_data_shape = genes_section_12[gene_data_columns].shape[1]\n",
    "\n",
    "# Initialize an empty array for aggregated gene data\n",
    "aggregated_gene_data = np.zeros((len(lipids_coords), gene_data_shape))\n",
    "\n",
    "# Calculate the std of the distances of the closest neighbors using KDTree\n",
    "genes_kdtree = cKDTree(genes_coords)\n",
    "distances, _ = genes_kdtree.query(lipids_coords, k=6)\n",
    "std_closest_distance = np.std(distances)\n",
    "\n",
    "# Extract gene data only once\n",
    "gene_data = genes_section_12[gene_data_columns].values\n",
    "\n",
    "# Parallel processing\n",
    "with Pool(cpu_count()) as pool:\n",
    "    results = pool.starmap(process_lipid_point, [(lipid_coord, genes_coords, gene_data, std_closest_distance) for lipid_coord in lipids_coords])\n",
    "\n",
    "# Combine results into aggregated_gene_data\n",
    "for i, result in enumerate(results):\n",
    "    aggregated_gene_data[i] = result\n",
    "\n",
    "# Convert the aggregated data into a DataFrame\n",
    "aggregated_gene_data_df = pd.DataFrame(aggregated_gene_data, columns=gene_data_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ENSMUST00000028118</th>\n",
       "      <th>ENSMUST00000028280</th>\n",
       "      <th>ENSMUST00000030676</th>\n",
       "      <th>ENSMUST00000047328</th>\n",
       "      <th>ENSMUST00000057021</th>\n",
       "      <th>ENSMUST00000090697</th>\n",
       "      <th>ENSMUST00000091554</th>\n",
       "      <th>ENSMUST00000162772</th>\n",
       "      <th>ENSMUST00000021284</th>\n",
       "      <th>ENSMUST00000022195</th>\n",
       "      <th>...</th>\n",
       "      <th>ENSMUST00000109964</th>\n",
       "      <th>ENSMUST00000114553</th>\n",
       "      <th>ENSMUST00000152412</th>\n",
       "      <th>ENSMUST00000159365</th>\n",
       "      <th>ENSMUST00000175965</th>\n",
       "      <th>ENSMUST00000196378</th>\n",
       "      <th>ENSMUST00000228095</th>\n",
       "      <th>ENSMUST00000000219</th>\n",
       "      <th>ENSMUST00000035577</th>\n",
       "      <th>ENSMUST00000060943</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.147987</td>\n",
       "      <td>0.462327</td>\n",
       "      <td>0.072980</td>\n",
       "      <td>0.737074</td>\n",
       "      <td>0.393011</td>\n",
       "      <td>0.036840</td>\n",
       "      <td>0.010277</td>\n",
       "      <td>0.046437</td>\n",
       "      <td>0.388947</td>\n",
       "      <td>0.061269</td>\n",
       "      <td>...</td>\n",
       "      <td>2.253574e-02</td>\n",
       "      <td>0.004449</td>\n",
       "      <td>3.522729e-03</td>\n",
       "      <td>0.181875</td>\n",
       "      <td>0.000367</td>\n",
       "      <td>0.229580</td>\n",
       "      <td>1.073747e-03</td>\n",
       "      <td>2.887407e-03</td>\n",
       "      <td>0.010568</td>\n",
       "      <td>5.719403e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.145376</td>\n",
       "      <td>0.438662</td>\n",
       "      <td>0.072397</td>\n",
       "      <td>0.723101</td>\n",
       "      <td>0.391337</td>\n",
       "      <td>0.046304</td>\n",
       "      <td>0.012539</td>\n",
       "      <td>0.045965</td>\n",
       "      <td>0.375394</td>\n",
       "      <td>0.054864</td>\n",
       "      <td>...</td>\n",
       "      <td>2.113728e-02</td>\n",
       "      <td>0.005848</td>\n",
       "      <td>5.497692e-03</td>\n",
       "      <td>0.187041</td>\n",
       "      <td>0.000451</td>\n",
       "      <td>0.239797</td>\n",
       "      <td>1.107482e-03</td>\n",
       "      <td>2.221527e-03</td>\n",
       "      <td>0.013870</td>\n",
       "      <td>6.551450e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.136072</td>\n",
       "      <td>0.409503</td>\n",
       "      <td>0.071754</td>\n",
       "      <td>0.713176</td>\n",
       "      <td>0.383924</td>\n",
       "      <td>0.056158</td>\n",
       "      <td>0.016338</td>\n",
       "      <td>0.047515</td>\n",
       "      <td>0.362651</td>\n",
       "      <td>0.046586</td>\n",
       "      <td>...</td>\n",
       "      <td>1.805532e-02</td>\n",
       "      <td>0.008643</td>\n",
       "      <td>8.560258e-03</td>\n",
       "      <td>0.194036</td>\n",
       "      <td>0.000720</td>\n",
       "      <td>0.247177</td>\n",
       "      <td>1.334278e-03</td>\n",
       "      <td>1.823251e-03</td>\n",
       "      <td>0.018507</td>\n",
       "      <td>7.652190e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.122332</td>\n",
       "      <td>0.378809</td>\n",
       "      <td>0.071948</td>\n",
       "      <td>0.708389</td>\n",
       "      <td>0.372212</td>\n",
       "      <td>0.065685</td>\n",
       "      <td>0.021395</td>\n",
       "      <td>0.051439</td>\n",
       "      <td>0.352460</td>\n",
       "      <td>0.037783</td>\n",
       "      <td>...</td>\n",
       "      <td>1.408733e-02</td>\n",
       "      <td>0.012332</td>\n",
       "      <td>1.268588e-02</td>\n",
       "      <td>0.201241</td>\n",
       "      <td>0.001189</td>\n",
       "      <td>0.251840</td>\n",
       "      <td>1.785261e-03</td>\n",
       "      <td>1.665490e-03</td>\n",
       "      <td>0.023573</td>\n",
       "      <td>9.222543e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.107172</td>\n",
       "      <td>0.350215</td>\n",
       "      <td>0.073201</td>\n",
       "      <td>0.707679</td>\n",
       "      <td>0.359079</td>\n",
       "      <td>0.073976</td>\n",
       "      <td>0.027104</td>\n",
       "      <td>0.057717</td>\n",
       "      <td>0.344678</td>\n",
       "      <td>0.030314</td>\n",
       "      <td>...</td>\n",
       "      <td>1.012393e-02</td>\n",
       "      <td>0.016146</td>\n",
       "      <td>1.745457e-02</td>\n",
       "      <td>0.206715</td>\n",
       "      <td>0.001865</td>\n",
       "      <td>0.253557</td>\n",
       "      <td>2.430526e-03</td>\n",
       "      <td>1.812998e-03</td>\n",
       "      <td>0.028051</td>\n",
       "      <td>1.139714e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94742</th>\n",
       "      <td>0.179308</td>\n",
       "      <td>0.783596</td>\n",
       "      <td>0.071025</td>\n",
       "      <td>1.242076</td>\n",
       "      <td>0.912762</td>\n",
       "      <td>0.000876</td>\n",
       "      <td>0.025595</td>\n",
       "      <td>0.003868</td>\n",
       "      <td>0.902205</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>...</td>\n",
       "      <td>5.239804e-06</td>\n",
       "      <td>0.040863</td>\n",
       "      <td>9.169214e-06</td>\n",
       "      <td>0.162888</td>\n",
       "      <td>0.082121</td>\n",
       "      <td>0.254811</td>\n",
       "      <td>1.044477e-06</td>\n",
       "      <td>1.516171e-05</td>\n",
       "      <td>0.001038</td>\n",
       "      <td>5.425096e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94743</th>\n",
       "      <td>0.178182</td>\n",
       "      <td>0.746224</td>\n",
       "      <td>0.079776</td>\n",
       "      <td>1.251607</td>\n",
       "      <td>0.935562</td>\n",
       "      <td>0.000321</td>\n",
       "      <td>0.038039</td>\n",
       "      <td>0.007169</td>\n",
       "      <td>0.889543</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>...</td>\n",
       "      <td>3.037505e-06</td>\n",
       "      <td>0.052966</td>\n",
       "      <td>1.998711e-06</td>\n",
       "      <td>0.201139</td>\n",
       "      <td>0.076683</td>\n",
       "      <td>0.211145</td>\n",
       "      <td>5.171308e-07</td>\n",
       "      <td>5.838183e-06</td>\n",
       "      <td>0.001985</td>\n",
       "      <td>3.417587e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94744</th>\n",
       "      <td>0.174173</td>\n",
       "      <td>0.705389</td>\n",
       "      <td>0.088836</td>\n",
       "      <td>1.255733</td>\n",
       "      <td>0.959206</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>0.054116</td>\n",
       "      <td>0.012900</td>\n",
       "      <td>0.875982</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>...</td>\n",
       "      <td>1.690604e-06</td>\n",
       "      <td>0.067753</td>\n",
       "      <td>4.494659e-07</td>\n",
       "      <td>0.237964</td>\n",
       "      <td>0.068000</td>\n",
       "      <td>0.173529</td>\n",
       "      <td>2.457966e-07</td>\n",
       "      <td>2.196025e-06</td>\n",
       "      <td>0.003629</td>\n",
       "      <td>2.211258e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94745</th>\n",
       "      <td>0.167855</td>\n",
       "      <td>0.661175</td>\n",
       "      <td>0.095272</td>\n",
       "      <td>1.252810</td>\n",
       "      <td>0.988879</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.077583</td>\n",
       "      <td>0.024482</td>\n",
       "      <td>0.854269</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>...</td>\n",
       "      <td>1.291216e-06</td>\n",
       "      <td>0.085074</td>\n",
       "      <td>2.676939e-07</td>\n",
       "      <td>0.270275</td>\n",
       "      <td>0.056731</td>\n",
       "      <td>0.137119</td>\n",
       "      <td>1.620983e-07</td>\n",
       "      <td>1.102981e-06</td>\n",
       "      <td>0.007266</td>\n",
       "      <td>2.438017e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94746</th>\n",
       "      <td>0.157422</td>\n",
       "      <td>0.634168</td>\n",
       "      <td>0.096129</td>\n",
       "      <td>1.249691</td>\n",
       "      <td>1.017020</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.098451</td>\n",
       "      <td>0.039224</td>\n",
       "      <td>0.835280</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>...</td>\n",
       "      <td>6.479714e-07</td>\n",
       "      <td>0.098910</td>\n",
       "      <td>5.575010e-07</td>\n",
       "      <td>0.286852</td>\n",
       "      <td>0.044418</td>\n",
       "      <td>0.114966</td>\n",
       "      <td>6.964010e-08</td>\n",
       "      <td>4.576574e-07</td>\n",
       "      <td>0.011768</td>\n",
       "      <td>1.456325e-09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>94747 rows × 500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ENSMUST00000028118  ENSMUST00000028280  ENSMUST00000030676  \\\n",
       "0                0.147987            0.462327            0.072980   \n",
       "1                0.145376            0.438662            0.072397   \n",
       "2                0.136072            0.409503            0.071754   \n",
       "3                0.122332            0.378809            0.071948   \n",
       "4                0.107172            0.350215            0.073201   \n",
       "...                   ...                 ...                 ...   \n",
       "94742            0.179308            0.783596            0.071025   \n",
       "94743            0.178182            0.746224            0.079776   \n",
       "94744            0.174173            0.705389            0.088836   \n",
       "94745            0.167855            0.661175            0.095272   \n",
       "94746            0.157422            0.634168            0.096129   \n",
       "\n",
       "       ENSMUST00000047328  ENSMUST00000057021  ENSMUST00000090697  \\\n",
       "0                0.737074            0.393011            0.036840   \n",
       "1                0.723101            0.391337            0.046304   \n",
       "2                0.713176            0.383924            0.056158   \n",
       "3                0.708389            0.372212            0.065685   \n",
       "4                0.707679            0.359079            0.073976   \n",
       "...                   ...                 ...                 ...   \n",
       "94742            1.242076            0.912762            0.000876   \n",
       "94743            1.251607            0.935562            0.000321   \n",
       "94744            1.255733            0.959206            0.000132   \n",
       "94745            1.252810            0.988879            0.000095   \n",
       "94746            1.249691            1.017020            0.000107   \n",
       "\n",
       "       ENSMUST00000091554  ENSMUST00000162772  ENSMUST00000021284  \\\n",
       "0                0.010277            0.046437            0.388947   \n",
       "1                0.012539            0.045965            0.375394   \n",
       "2                0.016338            0.047515            0.362651   \n",
       "3                0.021395            0.051439            0.352460   \n",
       "4                0.027104            0.057717            0.344678   \n",
       "...                   ...                 ...                 ...   \n",
       "94742            0.025595            0.003868            0.902205   \n",
       "94743            0.038039            0.007169            0.889543   \n",
       "94744            0.054116            0.012900            0.875982   \n",
       "94745            0.077583            0.024482            0.854269   \n",
       "94746            0.098451            0.039224            0.835280   \n",
       "\n",
       "       ENSMUST00000022195  ...  ENSMUST00000109964  ENSMUST00000114553  \\\n",
       "0                0.061269  ...        2.253574e-02            0.004449   \n",
       "1                0.054864  ...        2.113728e-02            0.005848   \n",
       "2                0.046586  ...        1.805532e-02            0.008643   \n",
       "3                0.037783  ...        1.408733e-02            0.012332   \n",
       "4                0.030314  ...        1.012393e-02            0.016146   \n",
       "...                   ...  ...                 ...                 ...   \n",
       "94742            0.000017  ...        5.239804e-06            0.040863   \n",
       "94743            0.000015  ...        3.037505e-06            0.052966   \n",
       "94744            0.000014  ...        1.690604e-06            0.067753   \n",
       "94745            0.000017  ...        1.291216e-06            0.085074   \n",
       "94746            0.000014  ...        6.479714e-07            0.098910   \n",
       "\n",
       "       ENSMUST00000152412  ENSMUST00000159365  ENSMUST00000175965  \\\n",
       "0            3.522729e-03            0.181875            0.000367   \n",
       "1            5.497692e-03            0.187041            0.000451   \n",
       "2            8.560258e-03            0.194036            0.000720   \n",
       "3            1.268588e-02            0.201241            0.001189   \n",
       "4            1.745457e-02            0.206715            0.001865   \n",
       "...                   ...                 ...                 ...   \n",
       "94742        9.169214e-06            0.162888            0.082121   \n",
       "94743        1.998711e-06            0.201139            0.076683   \n",
       "94744        4.494659e-07            0.237964            0.068000   \n",
       "94745        2.676939e-07            0.270275            0.056731   \n",
       "94746        5.575010e-07            0.286852            0.044418   \n",
       "\n",
       "       ENSMUST00000196378  ENSMUST00000228095  ENSMUST00000000219  \\\n",
       "0                0.229580        1.073747e-03        2.887407e-03   \n",
       "1                0.239797        1.107482e-03        2.221527e-03   \n",
       "2                0.247177        1.334278e-03        1.823251e-03   \n",
       "3                0.251840        1.785261e-03        1.665490e-03   \n",
       "4                0.253557        2.430526e-03        1.812998e-03   \n",
       "...                   ...                 ...                 ...   \n",
       "94742            0.254811        1.044477e-06        1.516171e-05   \n",
       "94743            0.211145        5.171308e-07        5.838183e-06   \n",
       "94744            0.173529        2.457966e-07        2.196025e-06   \n",
       "94745            0.137119        1.620983e-07        1.102981e-06   \n",
       "94746            0.114966        6.964010e-08        4.576574e-07   \n",
       "\n",
       "       ENSMUST00000035577  ENSMUST00000060943  \n",
       "0                0.010568        5.719403e-04  \n",
       "1                0.013870        6.551450e-04  \n",
       "2                0.018507        7.652190e-04  \n",
       "3                0.023573        9.222543e-04  \n",
       "4                0.028051        1.139714e-03  \n",
       "...                   ...                 ...  \n",
       "94742            0.001038        5.425096e-09  \n",
       "94743            0.001985        3.417587e-09  \n",
       "94744            0.003629        2.211258e-09  \n",
       "94745            0.007266        2.438017e-09  \n",
       "94746            0.011768        1.456325e-09  \n",
       "\n",
       "[94747 rows x 500 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregated_gene_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LPC O-16:2</th>\n",
       "      <th>LPC 16:0_dup</th>\n",
       "      <th>LPC O- 18:3</th>\n",
       "      <th>LPC O-18:2</th>\n",
       "      <th>LPC O-16:2_dup</th>\n",
       "      <th>LPC 15:1</th>\n",
       "      <th>LPC 18:1</th>\n",
       "      <th>LPC 18:0_dup</th>\n",
       "      <th>LPC 16:0</th>\n",
       "      <th>LPC O-18:3</th>\n",
       "      <th>...</th>\n",
       "      <th>SM(t42:1)</th>\n",
       "      <th>PC(40:7)</th>\n",
       "      <th>PC 40:6_dup</th>\n",
       "      <th>PG(42:6)</th>\n",
       "      <th>Hex2Cer 32:0</th>\n",
       "      <th>SHexCer 38:1;3</th>\n",
       "      <th>PE(44:11(OH))</th>\n",
       "      <th>PC(40:4)</th>\n",
       "      <th>PS(40:4)</th>\n",
       "      <th>PIP(O-36:5)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>section12_pixel23_121</th>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000197</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000241</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>section12_pixel23_122</th>\n",
       "      <td>0.000213</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.000204</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.000395</td>\n",
       "      <td>0.000208</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000268</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>section12_pixel23_123</th>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000232</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.000233</td>\n",
       "      <td>0.000203</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000232</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>section12_pixel23_124</th>\n",
       "      <td>0.000147</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000229</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.000285</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000255</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>section12_pixel23_125</th>\n",
       "      <td>0.000229</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000247</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000323</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>section12_pixel308_157</th>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>section12_pixel308_158</th>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>section12_pixel308_159</th>\n",
       "      <td>0.000155</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>section12_pixel308_160</th>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000237</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>section12_pixel308_161</th>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000335</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>94747 rows × 202 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        LPC O-16:2  LPC 16:0_dup  LPC O- 18:3  LPC O-18:2  \\\n",
       "section12_pixel23_121     0.000140      0.000112     0.000116    0.000125   \n",
       "section12_pixel23_122     0.000213      0.000112     0.000114    0.000125   \n",
       "section12_pixel23_123     0.000154      0.000100     0.000117    0.000134   \n",
       "section12_pixel23_124     0.000147      0.000113     0.000114    0.000136   \n",
       "section12_pixel23_125     0.000229      0.000112     0.000115    0.000206   \n",
       "...                            ...           ...          ...         ...   \n",
       "section12_pixel308_157    0.000139      0.000100     0.000100    0.000100   \n",
       "section12_pixel308_158    0.000139      0.000100     0.000100    0.000100   \n",
       "section12_pixel308_159    0.000155      0.000112     0.000117    0.000100   \n",
       "section12_pixel308_160    0.000141      0.000113     0.000100    0.000100   \n",
       "section12_pixel308_161    0.000163      0.000139     0.000100    0.000141   \n",
       "\n",
       "                        LPC O-16:2_dup  LPC 15:1  LPC 18:1  LPC 18:0_dup  \\\n",
       "section12_pixel23_121         0.000214  0.000100    0.0001      0.000197   \n",
       "section12_pixel23_122         0.000204  0.000162    0.0001      0.000100   \n",
       "section12_pixel23_123         0.000195  0.000151    0.0001      0.000232   \n",
       "section12_pixel23_124         0.000229  0.000154    0.0001      0.000100   \n",
       "section12_pixel23_125         0.000100  0.000100    0.0001      0.000100   \n",
       "...                                ...       ...       ...           ...   \n",
       "section12_pixel308_157        0.000100  0.000100    0.0001      0.000100   \n",
       "section12_pixel308_158        0.000100  0.000100    0.0001      0.000100   \n",
       "section12_pixel308_159        0.000100  0.000100    0.0001      0.000100   \n",
       "section12_pixel308_160        0.000100  0.000100    0.0001      0.000237   \n",
       "section12_pixel308_161        0.000100  0.000100    0.0001      0.000335   \n",
       "\n",
       "                        LPC 16:0  LPC O-18:3  ...  SM(t42:1)   PC(40:7)   \\\n",
       "section12_pixel23_121   0.000179      0.0001  ...      0.0001   0.000100   \n",
       "section12_pixel23_122   0.000181      0.0001  ...      0.0001   0.000114   \n",
       "section12_pixel23_123   0.000179      0.0001  ...      0.0001   0.000114   \n",
       "section12_pixel23_124   0.000120      0.0001  ...      0.0001   0.000114   \n",
       "section12_pixel23_125   0.000122      0.0001  ...      0.0001   0.000100   \n",
       "...                          ...         ...  ...         ...        ...   \n",
       "section12_pixel308_157  0.000100      0.0001  ...      0.0001   0.000100   \n",
       "section12_pixel308_158  0.000119      0.0001  ...      0.0001   0.000100   \n",
       "section12_pixel308_159  0.000100      0.0001  ...      0.0001   0.000100   \n",
       "section12_pixel308_160  0.000119      0.0001  ...      0.0001   0.000100   \n",
       "section12_pixel308_161  0.000119      0.0001  ...      0.0001   0.000100   \n",
       "\n",
       "                        PC 40:6_dup  PG(42:6)   Hex2Cer 32:0  SHexCer 38:1;3  \\\n",
       "section12_pixel23_121      0.000241   0.000179        0.0001        0.000100   \n",
       "section12_pixel23_122      0.000395   0.000208        0.0001        0.000316   \n",
       "section12_pixel23_123      0.000233   0.000203        0.0001        0.000100   \n",
       "section12_pixel23_124      0.000285   0.000187        0.0001        0.000255   \n",
       "section12_pixel23_125      0.000247   0.000179        0.0001        0.000323   \n",
       "...                             ...        ...           ...             ...   \n",
       "section12_pixel308_157     0.000100   0.000100        0.0001        0.000100   \n",
       "section12_pixel308_158     0.000100   0.000100        0.0001        0.000100   \n",
       "section12_pixel308_159     0.000100   0.000100        0.0001        0.000100   \n",
       "section12_pixel308_160     0.000100   0.000100        0.0001        0.000100   \n",
       "section12_pixel308_161     0.000100   0.000100        0.0001        0.000100   \n",
       "\n",
       "                        PE(44:11(OH))   PC(40:4)   PS(40:4)   PIP(O-36:5)   \n",
       "section12_pixel23_121           0.0001   0.000261     0.0001      0.000360  \n",
       "section12_pixel23_122           0.0001   0.000268     0.0001      0.000100  \n",
       "section12_pixel23_123           0.0001   0.000232     0.0001      0.000100  \n",
       "section12_pixel23_124           0.0001   0.000100     0.0001      0.000366  \n",
       "section12_pixel23_125           0.0001   0.000100     0.0001      0.000100  \n",
       "...                                ...        ...        ...           ...  \n",
       "section12_pixel308_157          0.0001   0.000100     0.0001      0.000100  \n",
       "section12_pixel308_158          0.0001   0.000100     0.0001      0.000100  \n",
       "section12_pixel308_159          0.0001   0.000100     0.0001      0.000100  \n",
       "section12_pixel308_160          0.0001   0.000100     0.0001      0.000100  \n",
       "section12_pixel308_161          0.0001   0.000100     0.0001      0.000100  \n",
       "\n",
       "[94747 rows x 202 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "section_12_lipids_only = lipids_section_12.iloc[:, 3:-3]\n",
    "section_12_lipids_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_gene_data_df = aggregated_gene_data_df.reset_index(drop=True)\n",
    "section_12_lipids_only = section_12_lipids_only.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simplify lipid names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_lipid(lipid_name):\n",
    "    # Example renaming scheme - customize as needed\n",
    "    new_name = lipid_name.replace(':', ';').replace('\\xa0', ' ')\n",
    "    return new_name\n",
    "\n",
    "section_12_lipids_only.columns = [rename_lipid(lipid) for lipid in section_12_lipids_only.columns] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting best model and training it and see the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Fatal] CUDA Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_CUDAP=1\n",
      "[LightGBM] [Fatal] CUDA Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_CUDAP=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Fatal] CUDA Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_CUDAP=1\n",
      "[LightGBM] [Fatal] CUDA Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_CUDAP=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Fatal] CUDA Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_CUDAP=1\n",
      "[LightGBM] [Fatal] CUDA Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_CUDAP=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Fatal] CUDA Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_CUDAP=1\n",
      "[LightGBM] [Fatal] CUDA Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_CUDAP=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Fatal] CUDA Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_CUDAP=1\n",
      "[LightGBM] [Fatal] CUDA Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_CUDAP=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Fatal] CUDA Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_CUDAP=1\n",
      "[LightGBM] [Fatal] CUDA Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_CUDAP=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Fatal] CUDA Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_CUDAP=1\n",
      "[LightGBM] [Fatal] CUDA Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_CUDAP=1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Initiated</th>\n",
       "      <td>. . . . . . . . . . . . . . . . . .</td>\n",
       "      <td>18:45:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Status</th>\n",
       "      <td>. . . . . . . . . . . . . . . . . .</td>\n",
       "      <td>Fitting 5 Folds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Estimator</th>\n",
       "      <td>. . . . . . . . . . . . . . . . . .</td>\n",
       "      <td>Bayesian Ridge</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                               \n",
       "                                                               \n",
       "Initiated  . . . . . . . . . . . . . . . . . .         18:45:22\n",
       "Status     . . . . . . . . . . . . . . . . . .  Fitting 5 Folds\n",
       "Estimator  . . . . . . . . . . . . . . . . . .   Bayesian Ridge"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_f45e3 th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_f45e3_row0_col0, #T_f45e3_row0_col1, #T_f45e3_row0_col2, #T_f45e3_row0_col3, #T_f45e3_row0_col4, #T_f45e3_row0_col5, #T_f45e3_row0_col6, #T_f45e3_row0_col7, #T_f45e3_row1_col0, #T_f45e3_row1_col1, #T_f45e3_row1_col2, #T_f45e3_row1_col3, #T_f45e3_row1_col4, #T_f45e3_row1_col5, #T_f45e3_row1_col6, #T_f45e3_row1_col7, #T_f45e3_row2_col0, #T_f45e3_row2_col1, #T_f45e3_row2_col2, #T_f45e3_row2_col3, #T_f45e3_row2_col4, #T_f45e3_row2_col5, #T_f45e3_row2_col6, #T_f45e3_row2_col7, #T_f45e3_row3_col0, #T_f45e3_row3_col1, #T_f45e3_row3_col2, #T_f45e3_row3_col3, #T_f45e3_row3_col4, #T_f45e3_row3_col5, #T_f45e3_row3_col6, #T_f45e3_row3_col7, #T_f45e3_row4_col0, #T_f45e3_row4_col1, #T_f45e3_row4_col2, #T_f45e3_row4_col3, #T_f45e3_row4_col4, #T_f45e3_row4_col5, #T_f45e3_row4_col6, #T_f45e3_row4_col7, #T_f45e3_row5_col0, #T_f45e3_row5_col1, #T_f45e3_row5_col2, #T_f45e3_row5_col3, #T_f45e3_row5_col4, #T_f45e3_row5_col5, #T_f45e3_row5_col6, #T_f45e3_row5_col7, #T_f45e3_row6_col0, #T_f45e3_row6_col1, #T_f45e3_row6_col2, #T_f45e3_row6_col3, #T_f45e3_row6_col4, #T_f45e3_row6_col5, #T_f45e3_row6_col6, #T_f45e3_row6_col7 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_f45e3\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_f45e3_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_f45e3_level0_col1\" class=\"col_heading level0 col1\" >MAE</th>\n",
       "      <th id=\"T_f45e3_level0_col2\" class=\"col_heading level0 col2\" >MSE</th>\n",
       "      <th id=\"T_f45e3_level0_col3\" class=\"col_heading level0 col3\" >RMSE</th>\n",
       "      <th id=\"T_f45e3_level0_col4\" class=\"col_heading level0 col4\" >R2</th>\n",
       "      <th id=\"T_f45e3_level0_col5\" class=\"col_heading level0 col5\" >RMSLE</th>\n",
       "      <th id=\"T_f45e3_level0_col6\" class=\"col_heading level0 col6\" >MAPE</th>\n",
       "      <th id=\"T_f45e3_level0_col7\" class=\"col_heading level0 col7\" >TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_f45e3_level0_row0\" class=\"row_heading level0 row0\" >lr</th>\n",
       "      <td id=\"T_f45e3_row0_col0\" class=\"data row0 col0\" >Linear Regression</td>\n",
       "      <td id=\"T_f45e3_row0_col1\" class=\"data row0 col1\" >0.0003</td>\n",
       "      <td id=\"T_f45e3_row0_col2\" class=\"data row0 col2\" >0.0000</td>\n",
       "      <td id=\"T_f45e3_row0_col3\" class=\"data row0 col3\" >0.0004</td>\n",
       "      <td id=\"T_f45e3_row0_col4\" class=\"data row0 col4\" >0.4759</td>\n",
       "      <td id=\"T_f45e3_row0_col5\" class=\"data row0 col5\" >0.0004</td>\n",
       "      <td id=\"T_f45e3_row0_col6\" class=\"data row0 col6\" >0.3643</td>\n",
       "      <td id=\"T_f45e3_row0_col7\" class=\"data row0 col7\" >0.4800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f45e3_level0_row1\" class=\"row_heading level0 row1\" >ridge</th>\n",
       "      <td id=\"T_f45e3_row1_col0\" class=\"data row1 col0\" >Ridge Regression</td>\n",
       "      <td id=\"T_f45e3_row1_col1\" class=\"data row1 col1\" >0.0003</td>\n",
       "      <td id=\"T_f45e3_row1_col2\" class=\"data row1 col2\" >0.0000</td>\n",
       "      <td id=\"T_f45e3_row1_col3\" class=\"data row1 col3\" >0.0004</td>\n",
       "      <td id=\"T_f45e3_row1_col4\" class=\"data row1 col4\" >0.4761</td>\n",
       "      <td id=\"T_f45e3_row1_col5\" class=\"data row1 col5\" >0.0004</td>\n",
       "      <td id=\"T_f45e3_row1_col6\" class=\"data row1 col6\" >0.3649</td>\n",
       "      <td id=\"T_f45e3_row1_col7\" class=\"data row1 col7\" >0.1680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f45e3_level0_row2\" class=\"row_heading level0 row2\" >omp</th>\n",
       "      <td id=\"T_f45e3_row2_col0\" class=\"data row2 col0\" >Orthogonal Matching Pursuit</td>\n",
       "      <td id=\"T_f45e3_row2_col1\" class=\"data row2 col1\" >0.0003</td>\n",
       "      <td id=\"T_f45e3_row2_col2\" class=\"data row2 col2\" >0.0000</td>\n",
       "      <td id=\"T_f45e3_row2_col3\" class=\"data row2 col3\" >0.0004</td>\n",
       "      <td id=\"T_f45e3_row2_col4\" class=\"data row2 col4\" >0.3870</td>\n",
       "      <td id=\"T_f45e3_row2_col5\" class=\"data row2 col5\" >0.0004</td>\n",
       "      <td id=\"T_f45e3_row2_col6\" class=\"data row2 col6\" >0.4289</td>\n",
       "      <td id=\"T_f45e3_row2_col7\" class=\"data row2 col7\" >0.1400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f45e3_level0_row3\" class=\"row_heading level0 row3\" >lasso</th>\n",
       "      <td id=\"T_f45e3_row3_col0\" class=\"data row3 col0\" >Lasso Regression</td>\n",
       "      <td id=\"T_f45e3_row3_col1\" class=\"data row3 col1\" >0.0004</td>\n",
       "      <td id=\"T_f45e3_row3_col2\" class=\"data row3 col2\" >0.0000</td>\n",
       "      <td id=\"T_f45e3_row3_col3\" class=\"data row3 col3\" >0.0005</td>\n",
       "      <td id=\"T_f45e3_row3_col4\" class=\"data row3 col4\" >-0.0000</td>\n",
       "      <td id=\"T_f45e3_row3_col5\" class=\"data row3 col5\" >0.0005</td>\n",
       "      <td id=\"T_f45e3_row3_col6\" class=\"data row3 col6\" >0.5921</td>\n",
       "      <td id=\"T_f45e3_row3_col7\" class=\"data row3 col7\" >0.1620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f45e3_level0_row4\" class=\"row_heading level0 row4\" >en</th>\n",
       "      <td id=\"T_f45e3_row4_col0\" class=\"data row4 col0\" >Elastic Net</td>\n",
       "      <td id=\"T_f45e3_row4_col1\" class=\"data row4 col1\" >0.0004</td>\n",
       "      <td id=\"T_f45e3_row4_col2\" class=\"data row4 col2\" >0.0000</td>\n",
       "      <td id=\"T_f45e3_row4_col3\" class=\"data row4 col3\" >0.0005</td>\n",
       "      <td id=\"T_f45e3_row4_col4\" class=\"data row4 col4\" >-0.0000</td>\n",
       "      <td id=\"T_f45e3_row4_col5\" class=\"data row4 col5\" >0.0005</td>\n",
       "      <td id=\"T_f45e3_row4_col6\" class=\"data row4 col6\" >0.5921</td>\n",
       "      <td id=\"T_f45e3_row4_col7\" class=\"data row4 col7\" >0.1420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f45e3_level0_row5\" class=\"row_heading level0 row5\" >llar</th>\n",
       "      <td id=\"T_f45e3_row5_col0\" class=\"data row5 col0\" >Lasso Least Angle Regression</td>\n",
       "      <td id=\"T_f45e3_row5_col1\" class=\"data row5 col1\" >0.0004</td>\n",
       "      <td id=\"T_f45e3_row5_col2\" class=\"data row5 col2\" >0.0000</td>\n",
       "      <td id=\"T_f45e3_row5_col3\" class=\"data row5 col3\" >0.0005</td>\n",
       "      <td id=\"T_f45e3_row5_col4\" class=\"data row5 col4\" >-0.0000</td>\n",
       "      <td id=\"T_f45e3_row5_col5\" class=\"data row5 col5\" >0.0005</td>\n",
       "      <td id=\"T_f45e3_row5_col6\" class=\"data row5 col6\" >0.5921</td>\n",
       "      <td id=\"T_f45e3_row5_col7\" class=\"data row5 col7\" >0.1460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f45e3_level0_row6\" class=\"row_heading level0 row6\" >lar</th>\n",
       "      <td id=\"T_f45e3_row6_col0\" class=\"data row6 col0\" >Least Angle Regression</td>\n",
       "      <td id=\"T_f45e3_row6_col1\" class=\"data row6 col1\" >0.0119</td>\n",
       "      <td id=\"T_f45e3_row6_col2\" class=\"data row6 col2\" >0.0006</td>\n",
       "      <td id=\"T_f45e3_row6_col3\" class=\"data row6 col3\" >0.0152</td>\n",
       "      <td id=\"T_f45e3_row6_col4\" class=\"data row6 col4\" >-2186.7267</td>\n",
       "      <td id=\"T_f45e3_row6_col5\" class=\"data row6 col5\" >0.0141</td>\n",
       "      <td id=\"T_f45e3_row6_col6\" class=\"data row6 col6\" >14.1471</td>\n",
       "      <td id=\"T_f45e3_row6_col7\" class=\"data row6 col7\" >0.2120</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fee8a9dafd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba59d8ded40844d087aee70ddc74b3ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/85 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/jules/ml-project-2-genelipids/models.ipynb Cell 25\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/jules/ml-project-2-genelipids/models.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m setup(data\u001b[39m=\u001b[39mdf, fold\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, session_id\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m, use_gpu\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, verbose\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, preprocess\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/jules/ml-project-2-genelipids/models.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# Step 2: Select best model\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/jules/ml-project-2-genelipids/models.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m best_model \u001b[39m=\u001b[39m compare_models(fold\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, sort\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mMAPE\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/jules/ml-project-2-genelipids/models.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# Step 2: Create the model\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/jules/ml-project-2-genelipids/models.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m model \u001b[39m=\u001b[39m create_model(\u001b[39m'\u001b[39m\u001b[39mcatboost\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/pycaret/utils/generic.py:965\u001b[0m, in \u001b[0;36mcheck_if_global_is_not_none.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[39mif\u001b[39;00m globals_d[name] \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    964\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(message)\n\u001b[0;32m--> 965\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/pycaret/regression/functional.py:805\u001b[0m, in \u001b[0;36mcompare_models\u001b[0;34m(include, exclude, fold, round, cross_validation, sort, n_select, budget_time, turbo, errors, fit_kwargs, groups, experiment_custom_tags, engine, verbose, parallel)\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[39m@check_if_global_is_not_none\u001b[39m(\u001b[39mglobals\u001b[39m(), _CURRENT_EXPERIMENT_DECORATOR_DICT)\n\u001b[1;32m    671\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompare_models\u001b[39m(\n\u001b[1;32m    672\u001b[0m     include: Optional[List[Union[\u001b[39mstr\u001b[39m, Any]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    687\u001b[0m     parallel: Optional[ParallelBackend] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    688\u001b[0m ):\n\u001b[1;32m    689\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    690\u001b[0m \u001b[39m    This function trains and evaluates performance of all estimators available in the\u001b[39;00m\n\u001b[1;32m    691\u001b[0m \u001b[39m    model library using cross validation. The output of this function is a score grid\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    803\u001b[0m \n\u001b[1;32m    804\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 805\u001b[0m     \u001b[39mreturn\u001b[39;00m _CURRENT_EXPERIMENT\u001b[39m.\u001b[39;49mcompare_models(\n\u001b[1;32m    806\u001b[0m         include\u001b[39m=\u001b[39;49minclude,\n\u001b[1;32m    807\u001b[0m         exclude\u001b[39m=\u001b[39;49mexclude,\n\u001b[1;32m    808\u001b[0m         fold\u001b[39m=\u001b[39;49mfold,\n\u001b[1;32m    809\u001b[0m         \u001b[39mround\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mround\u001b[39;49m,\n\u001b[1;32m    810\u001b[0m         cross_validation\u001b[39m=\u001b[39;49mcross_validation,\n\u001b[1;32m    811\u001b[0m         sort\u001b[39m=\u001b[39;49msort,\n\u001b[1;32m    812\u001b[0m         n_select\u001b[39m=\u001b[39;49mn_select,\n\u001b[1;32m    813\u001b[0m         budget_time\u001b[39m=\u001b[39;49mbudget_time,\n\u001b[1;32m    814\u001b[0m         turbo\u001b[39m=\u001b[39;49mturbo,\n\u001b[1;32m    815\u001b[0m         errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    816\u001b[0m         fit_kwargs\u001b[39m=\u001b[39;49mfit_kwargs,\n\u001b[1;32m    817\u001b[0m         groups\u001b[39m=\u001b[39;49mgroups,\n\u001b[1;32m    818\u001b[0m         experiment_custom_tags\u001b[39m=\u001b[39;49mexperiment_custom_tags,\n\u001b[1;32m    819\u001b[0m         engine\u001b[39m=\u001b[39;49mengine,\n\u001b[1;32m    820\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    821\u001b[0m         parallel\u001b[39m=\u001b[39;49mparallel,\n\u001b[1;32m    822\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/pycaret/regression/oop.py:1122\u001b[0m, in \u001b[0;36mRegressionExperiment.compare_models\u001b[0;34m(self, include, exclude, fold, round, cross_validation, sort, n_select, budget_time, turbo, errors, fit_kwargs, groups, experiment_custom_tags, engine, verbose, parallel)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_engine(estimator\u001b[39m=\u001b[39mestimator, engine\u001b[39m=\u001b[39meng, severity\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1121\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1122\u001b[0m     return_values \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcompare_models(\n\u001b[1;32m   1123\u001b[0m         include\u001b[39m=\u001b[39;49minclude,\n\u001b[1;32m   1124\u001b[0m         exclude\u001b[39m=\u001b[39;49mexclude,\n\u001b[1;32m   1125\u001b[0m         fold\u001b[39m=\u001b[39;49mfold,\n\u001b[1;32m   1126\u001b[0m         \u001b[39mround\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mround\u001b[39;49m,\n\u001b[1;32m   1127\u001b[0m         cross_validation\u001b[39m=\u001b[39;49mcross_validation,\n\u001b[1;32m   1128\u001b[0m         sort\u001b[39m=\u001b[39;49msort,\n\u001b[1;32m   1129\u001b[0m         n_select\u001b[39m=\u001b[39;49mn_select,\n\u001b[1;32m   1130\u001b[0m         budget_time\u001b[39m=\u001b[39;49mbudget_time,\n\u001b[1;32m   1131\u001b[0m         turbo\u001b[39m=\u001b[39;49mturbo,\n\u001b[1;32m   1132\u001b[0m         errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m   1133\u001b[0m         fit_kwargs\u001b[39m=\u001b[39;49mfit_kwargs,\n\u001b[1;32m   1134\u001b[0m         groups\u001b[39m=\u001b[39;49mgroups,\n\u001b[1;32m   1135\u001b[0m         experiment_custom_tags\u001b[39m=\u001b[39;49mexperiment_custom_tags,\n\u001b[1;32m   1136\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   1137\u001b[0m         parallel\u001b[39m=\u001b[39;49mparallel,\n\u001b[1;32m   1138\u001b[0m         caller_params\u001b[39m=\u001b[39;49mcaller_params,\n\u001b[1;32m   1139\u001b[0m     )\n\u001b[1;32m   1141\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1142\u001b[0m     \u001b[39mif\u001b[39;00m engine \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1143\u001b[0m         \u001b[39m# Reset the models back to the default engines\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/pycaret/internal/pycaret_experiment/supervised_experiment.py:794\u001b[0m, in \u001b[0;36m_SupervisedExperiment.compare_models\u001b[0;34m(self, include, exclude, fold, round, cross_validation, sort, n_select, budget_time, turbo, errors, fit_kwargs, groups, experiment_custom_tags, probability_threshold, verbose, parallel, caller_params)\u001b[0m\n\u001b[1;32m    791\u001b[0m results_columns_to_ignore \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mObject\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mruntime\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcutoff\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    793\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 794\u001b[0m     model, model_fit_time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcreate_model_args)\n\u001b[1;32m    795\u001b[0m     model_results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpull(pop\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    796\u001b[0m     \u001b[39massert\u001b[39;00m (\n\u001b[1;32m    797\u001b[0m         np\u001b[39m.\u001b[39msum(\n\u001b[1;32m    798\u001b[0m             model_results\u001b[39m.\u001b[39mdrop(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[39m!=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[1;32m    803\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/pycaret/internal/pycaret_experiment/supervised_experiment.py:1533\u001b[0m, in \u001b[0;36m_SupervisedExperiment._create_model\u001b[0;34m(self, estimator, fold, round, cross_validation, predict, fit_kwargs, groups, refit, probability_threshold, experiment_custom_tags, verbose, system, add_to_model_list, X_train_data, y_train_data, metrics, display, model_only, return_train_score, error_score, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m         \u001b[39mreturn\u001b[39;00m model, model_fit_time\n\u001b[1;32m   1531\u001b[0m     \u001b[39mreturn\u001b[39;00m model\n\u001b[0;32m-> 1533\u001b[0m model, model_fit_time, model_results, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_model_with_cv(\n\u001b[1;32m   1534\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m   1535\u001b[0m     data_X\u001b[39m=\u001b[39;49mdata_X,\n\u001b[1;32m   1536\u001b[0m     data_y\u001b[39m=\u001b[39;49mdata_y,\n\u001b[1;32m   1537\u001b[0m     fit_kwargs\u001b[39m=\u001b[39;49mfit_kwargs,\n\u001b[1;32m   1538\u001b[0m     \u001b[39mround\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mround\u001b[39;49m,\n\u001b[1;32m   1539\u001b[0m     cv\u001b[39m=\u001b[39;49mcv,\n\u001b[1;32m   1540\u001b[0m     groups\u001b[39m=\u001b[39;49mgroups,\n\u001b[1;32m   1541\u001b[0m     metrics\u001b[39m=\u001b[39;49mmetrics,\n\u001b[1;32m   1542\u001b[0m     refit\u001b[39m=\u001b[39;49mrefit,\n\u001b[1;32m   1543\u001b[0m     system\u001b[39m=\u001b[39;49msystem,\n\u001b[1;32m   1544\u001b[0m     display\u001b[39m=\u001b[39;49mdisplay,\n\u001b[1;32m   1545\u001b[0m     error_score\u001b[39m=\u001b[39;49merror_score,\n\u001b[1;32m   1546\u001b[0m     return_train_score\u001b[39m=\u001b[39;49mreturn_train_score,\n\u001b[1;32m   1547\u001b[0m )\n\u001b[1;32m   1549\u001b[0m \u001b[39m# end runtime\u001b[39;00m\n\u001b[1;32m   1550\u001b[0m runtime_end \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/pycaret/internal/pycaret_experiment/supervised_experiment.py:1126\u001b[0m, in \u001b[0;36m_SupervisedExperiment._create_model_with_cv\u001b[0;34m(self, model, data_X, data_y, fit_kwargs, round, cv, groups, metrics, refit, system, display, error_score, return_train_score)\u001b[0m\n\u001b[1;32m   1124\u001b[0m     model_fit_start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m   1125\u001b[0m     \u001b[39mwith\u001b[39;00m redirect_output(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogger):\n\u001b[0;32m-> 1126\u001b[0m         scores \u001b[39m=\u001b[39m cross_validate(\n\u001b[1;32m   1127\u001b[0m             pipeline_with_model,\n\u001b[1;32m   1128\u001b[0m             data_X,\n\u001b[1;32m   1129\u001b[0m             data_y,\n\u001b[1;32m   1130\u001b[0m             cv\u001b[39m=\u001b[39;49mcv,\n\u001b[1;32m   1131\u001b[0m             groups\u001b[39m=\u001b[39;49mgroups,\n\u001b[1;32m   1132\u001b[0m             scoring\u001b[39m=\u001b[39;49mmetrics_dict,\n\u001b[1;32m   1133\u001b[0m             fit_params\u001b[39m=\u001b[39;49mfit_kwargs,\n\u001b[1;32m   1134\u001b[0m             n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m   1135\u001b[0m             return_train_score\u001b[39m=\u001b[39;49mreturn_train_score,\n\u001b[1;32m   1136\u001b[0m             error_score\u001b[39m=\u001b[39;49merror_score,\n\u001b[1;32m   1137\u001b[0m         )\n\u001b[1;32m   1139\u001b[0m model_fit_end \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m   1140\u001b[0m model_fit_time \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(model_fit_end \u001b[39m-\u001b[39m model_fit_start)\u001b[39m.\u001b[39mround(\u001b[39m2\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:266\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[39m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[1;32m    265\u001b[0m parallel \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39mn_jobs, verbose\u001b[39m=\u001b[39mverbose, pre_dispatch\u001b[39m=\u001b[39mpre_dispatch)\n\u001b[0;32m--> 266\u001b[0m results \u001b[39m=\u001b[39m parallel(\n\u001b[1;32m    267\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    268\u001b[0m         clone(estimator),\n\u001b[1;32m    269\u001b[0m         X,\n\u001b[1;32m    270\u001b[0m         y,\n\u001b[1;32m    271\u001b[0m         scorers,\n\u001b[1;32m    272\u001b[0m         train,\n\u001b[1;32m    273\u001b[0m         test,\n\u001b[1;32m    274\u001b[0m         verbose,\n\u001b[1;32m    275\u001b[0m         \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    276\u001b[0m         fit_params,\n\u001b[1;32m    277\u001b[0m         return_train_score\u001b[39m=\u001b[39;49mreturn_train_score,\n\u001b[1;32m    278\u001b[0m         return_times\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    279\u001b[0m         return_estimator\u001b[39m=\u001b[39;49mreturn_estimator,\n\u001b[1;32m    280\u001b[0m         error_score\u001b[39m=\u001b[39;49merror_score,\n\u001b[1;32m    281\u001b[0m     )\n\u001b[1;32m    282\u001b[0m     \u001b[39mfor\u001b[39;49;00m train, test \u001b[39min\u001b[39;49;00m cv\u001b[39m.\u001b[39;49msplit(X, y, groups)\n\u001b[1;32m    283\u001b[0m )\n\u001b[1;32m    285\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[1;32m    287\u001b[0m \u001b[39m# For callabe scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[39m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[39m# the correct key.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/joblib/parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1861\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1862\u001b[0m     \u001b[39mnext\u001b[39m(output)\n\u001b[0;32m-> 1863\u001b[0m     \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39m(output)\n\u001b[1;32m   1865\u001b[0m \u001b[39m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1866\u001b[0m \u001b[39m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1867\u001b[0m \u001b[39m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m \u001b[39m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1869\u001b[0m \u001b[39m# callback.\u001b[39;00m\n\u001b[1;32m   1870\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/joblib/parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1790\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_batches \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1791\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m-> 1792\u001b[0m res \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1793\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_completed_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1794\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/sklearn/utils/parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[1;32m    122\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[0;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/pycaret/internal/patches/sklearn.py:124\u001b[0m, in \u001b[0;36mfit_and_score\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[39mwith\u001b[39;00m patch(\u001b[39m\"\u001b[39m\u001b[39msklearn.model_selection._validation._score\u001b[39m\u001b[39m\"\u001b[39m, score(_score)):\n\u001b[1;32m    122\u001b[0m         \u001b[39mreturn\u001b[39;00m _fit_and_score(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 124\u001b[0m \u001b[39mreturn\u001b[39;00m wrapper(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/pycaret/internal/patches/sklearn.py:122\u001b[0m, in \u001b[0;36mfit_and_score.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mdict\u001b[39m:\n\u001b[1;32m    121\u001b[0m     \u001b[39mwith\u001b[39;00m patch(\u001b[39m\"\u001b[39m\u001b[39msklearn.model_selection._validation._score\u001b[39m\u001b[39m\"\u001b[39m, score(_score)):\n\u001b[0;32m--> 122\u001b[0m         \u001b[39mreturn\u001b[39;00m _fit_and_score(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:686\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    684\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[1;32m    685\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 686\u001b[0m         estimator\u001b[39m.\u001b[39;49mfit(X_train, y_train, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[1;32m    688\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m    689\u001b[0m     \u001b[39m# Note fit time as time until error\u001b[39;00m\n\u001b[1;32m    690\u001b[0m     fit_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/pycaret/internal/pipeline.py:275\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpassthrough\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    274\u001b[0m     fit_params_last_step \u001b[39m=\u001b[39m fit_params_steps[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m]]\n\u001b[0;32m--> 275\u001b[0m     fitted_estimator \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_memory_fit(\n\u001b[1;32m    276\u001b[0m         clone(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msteps[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m][\u001b[39m1\u001b[39;49m]), X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params_last_step\n\u001b[1;32m    277\u001b[0m     )\n\u001b[1;32m    278\u001b[0m     \u001b[39m# Hacky way to make sure that the state of the estimator\u001b[39;00m\n\u001b[1;32m    279\u001b[0m     \u001b[39m# loaded from cache is carried over to the estimator\u001b[39;00m\n\u001b[1;32m    280\u001b[0m     \u001b[39m# in steps\u001b[39;00m\n\u001b[1;32m    281\u001b[0m     _copy_estimator_state(fitted_estimator, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m1\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/joblib/memory.py:353\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 353\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/pycaret/internal/pipeline.py:68\u001b[0m, in \u001b[0;36m_fit_one\u001b[0;34m(transformer, X, y, message, **fit_params)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m signature(transformer\u001b[39m.\u001b[39mfit)\u001b[39m.\u001b[39mparameters:\n\u001b[1;32m     67\u001b[0m             args\u001b[39m.\u001b[39mappend(y)\n\u001b[0;32m---> 68\u001b[0m         transformer\u001b[39m.\u001b[39;49mfit(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[1;32m     69\u001b[0m \u001b[39mreturn\u001b[39;00m transformer\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/sklearn/linear_model/_bayes.py:273\u001b[0m, in \u001b[0;36mBayesianRidge.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    270\u001b[0m coef_old_ \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    272\u001b[0m XT_y \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(X\u001b[39m.\u001b[39mT, y)\n\u001b[0;32m--> 273\u001b[0m U, S, Vh \u001b[39m=\u001b[39m linalg\u001b[39m.\u001b[39;49msvd(X, full_matrices\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    274\u001b[0m eigen_vals_ \u001b[39m=\u001b[39m S\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m\n\u001b[1;32m    276\u001b[0m \u001b[39m# Convergence loop of the bayesian ridge regression\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/scipy/linalg/_decomp_svd.py:127\u001b[0m, in \u001b[0;36msvd\u001b[0;34m(a, full_matrices, compute_uv, overwrite_a, check_finite, lapack_driver)\u001b[0m\n\u001b[1;32m    123\u001b[0m lwork \u001b[39m=\u001b[39m _compute_lwork(gesXd_lwork, a1\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], a1\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m],\n\u001b[1;32m    124\u001b[0m                        compute_uv\u001b[39m=\u001b[39mcompute_uv, full_matrices\u001b[39m=\u001b[39mfull_matrices)\n\u001b[1;32m    126\u001b[0m \u001b[39m# perform decomposition\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m u, s, v, info \u001b[39m=\u001b[39m gesXd(a1, compute_uv\u001b[39m=\u001b[39;49mcompute_uv, lwork\u001b[39m=\u001b[39;49mlwork,\n\u001b[1;32m    128\u001b[0m                       full_matrices\u001b[39m=\u001b[39;49mfull_matrices, overwrite_a\u001b[39m=\u001b[39;49moverwrite_a)\n\u001b[1;32m    130\u001b[0m \u001b[39mif\u001b[39;00m info \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    131\u001b[0m     \u001b[39mraise\u001b[39;00m LinAlgError(\u001b[39m\"\u001b[39m\u001b[39mSVD did not converge\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Loop through each lipid type based on the lipid names in the column headers.\n",
    "for i, lipid_name in enumerate(section_12_lipids_only.columns):\n",
    "    \n",
    "    # Create a DataFrame to store genes and the corresponding lipid values\n",
    "    # Use the aggregated gene data (aggregated_gene_data_df) and the lipid data\n",
    "    df = aggregated_gene_data_df.copy()\n",
    "    \n",
    "    # Add a column for the lipid values\n",
    "    # The index should match the index of the aggregated gene data\n",
    "    df[lipid_name] = section_12_lipids_only.iloc[:, i]\n",
    "    \n",
    "    # Step 1: Setup the environment in PyCaret\n",
    "    setup(data=df, fold=5, session_id=42, use_gpu=True, verbose=False, preprocess=False)\n",
    "    \n",
    "    # Step 2: Select best model\n",
    "    best_model = compare_models(fold=5)\n",
    "\n",
    "    # Step 2: Create the model\n",
    "    model = create_model('catboost')\n",
    "\n",
    "    # Step 3: Finalize the model\n",
    "    #final_model = finalize_model(model)\n",
    "    \n",
    "    # Step 4: Save the model\n",
    "    #save_model(model, f'models/{lipid_name}_model_{i}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating One Model using K-Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Fatal] CUDA Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_CUDAP=1\n",
      "[LightGBM] [Fatal] CUDA Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_CUDAP=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Fatal] CUDA Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_CUDAP=1\n",
      "[LightGBM] [Fatal] CUDA Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_CUDAP=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Fatal] CUDA Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_CUDAP=1\n",
      "[LightGBM] [Fatal] CUDA Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_CUDAP=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Fatal] CUDA Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_CUDAP=1\n",
      "[LightGBM] [Fatal] CUDA Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_CUDAP=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Fatal] CUDA Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_CUDAP=1\n",
      "[LightGBM] [Fatal] CUDA Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_CUDAP=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Fatal] CUDA Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_CUDAP=1\n",
      "[LightGBM] [Fatal] CUDA Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_CUDAP=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Fatal] CUDA Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_CUDAP=1\n",
      "[LightGBM] [Fatal] CUDA Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_CUDAP=1\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_edb14_row5_col0, #T_edb14_row5_col1, #T_edb14_row5_col2, #T_edb14_row5_col3, #T_edb14_row5_col4, #T_edb14_row5_col5 {\n",
       "  background: yellow;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_edb14\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_edb14_level0_col0\" class=\"col_heading level0 col0\" >MAE</th>\n",
       "      <th id=\"T_edb14_level0_col1\" class=\"col_heading level0 col1\" >MSE</th>\n",
       "      <th id=\"T_edb14_level0_col2\" class=\"col_heading level0 col2\" >RMSE</th>\n",
       "      <th id=\"T_edb14_level0_col3\" class=\"col_heading level0 col3\" >R2</th>\n",
       "      <th id=\"T_edb14_level0_col4\" class=\"col_heading level0 col4\" >RMSLE</th>\n",
       "      <th id=\"T_edb14_level0_col5\" class=\"col_heading level0 col5\" >MAPE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Fold</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "      <th class=\"blank col5\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_edb14_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_edb14_row0_col0\" class=\"data row0 col0\" >0.0002</td>\n",
       "      <td id=\"T_edb14_row0_col1\" class=\"data row0 col1\" >0.0000</td>\n",
       "      <td id=\"T_edb14_row0_col2\" class=\"data row0 col2\" >0.0003</td>\n",
       "      <td id=\"T_edb14_row0_col3\" class=\"data row0 col3\" >0.6247</td>\n",
       "      <td id=\"T_edb14_row0_col4\" class=\"data row0 col4\" >0.0003</td>\n",
       "      <td id=\"T_edb14_row0_col5\" class=\"data row0 col5\" >0.2721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_edb14_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_edb14_row1_col0\" class=\"data row1 col0\" >0.0002</td>\n",
       "      <td id=\"T_edb14_row1_col1\" class=\"data row1 col1\" >0.0000</td>\n",
       "      <td id=\"T_edb14_row1_col2\" class=\"data row1 col2\" >0.0003</td>\n",
       "      <td id=\"T_edb14_row1_col3\" class=\"data row1 col3\" >0.6181</td>\n",
       "      <td id=\"T_edb14_row1_col4\" class=\"data row1 col4\" >0.0003</td>\n",
       "      <td id=\"T_edb14_row1_col5\" class=\"data row1 col5\" >0.2677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_edb14_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_edb14_row2_col0\" class=\"data row2 col0\" >0.0002</td>\n",
       "      <td id=\"T_edb14_row2_col1\" class=\"data row2 col1\" >0.0000</td>\n",
       "      <td id=\"T_edb14_row2_col2\" class=\"data row2 col2\" >0.0003</td>\n",
       "      <td id=\"T_edb14_row2_col3\" class=\"data row2 col3\" >0.6199</td>\n",
       "      <td id=\"T_edb14_row2_col4\" class=\"data row2 col4\" >0.0003</td>\n",
       "      <td id=\"T_edb14_row2_col5\" class=\"data row2 col5\" >0.2647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_edb14_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_edb14_row3_col0\" class=\"data row3 col0\" >0.0002</td>\n",
       "      <td id=\"T_edb14_row3_col1\" class=\"data row3 col1\" >0.0000</td>\n",
       "      <td id=\"T_edb14_row3_col2\" class=\"data row3 col2\" >0.0003</td>\n",
       "      <td id=\"T_edb14_row3_col3\" class=\"data row3 col3\" >0.6199</td>\n",
       "      <td id=\"T_edb14_row3_col4\" class=\"data row3 col4\" >0.0003</td>\n",
       "      <td id=\"T_edb14_row3_col5\" class=\"data row3 col5\" >0.2646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_edb14_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_edb14_row4_col0\" class=\"data row4 col0\" >0.0002</td>\n",
       "      <td id=\"T_edb14_row4_col1\" class=\"data row4 col1\" >0.0000</td>\n",
       "      <td id=\"T_edb14_row4_col2\" class=\"data row4 col2\" >0.0003</td>\n",
       "      <td id=\"T_edb14_row4_col3\" class=\"data row4 col3\" >0.6197</td>\n",
       "      <td id=\"T_edb14_row4_col4\" class=\"data row4 col4\" >0.0003</td>\n",
       "      <td id=\"T_edb14_row4_col5\" class=\"data row4 col5\" >0.2675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_edb14_level0_row5\" class=\"row_heading level0 row5\" >Mean</th>\n",
       "      <td id=\"T_edb14_row5_col0\" class=\"data row5 col0\" >0.0002</td>\n",
       "      <td id=\"T_edb14_row5_col1\" class=\"data row5 col1\" >0.0000</td>\n",
       "      <td id=\"T_edb14_row5_col2\" class=\"data row5 col2\" >0.0003</td>\n",
       "      <td id=\"T_edb14_row5_col3\" class=\"data row5 col3\" >0.6204</td>\n",
       "      <td id=\"T_edb14_row5_col4\" class=\"data row5 col4\" >0.0003</td>\n",
       "      <td id=\"T_edb14_row5_col5\" class=\"data row5 col5\" >0.2673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_edb14_level0_row6\" class=\"row_heading level0 row6\" >Std</th>\n",
       "      <td id=\"T_edb14_row6_col0\" class=\"data row6 col0\" >0.0000</td>\n",
       "      <td id=\"T_edb14_row6_col1\" class=\"data row6 col1\" >0.0000</td>\n",
       "      <td id=\"T_edb14_row6_col2\" class=\"data row6 col2\" >0.0000</td>\n",
       "      <td id=\"T_edb14_row6_col3\" class=\"data row6 col3\" >0.0022</td>\n",
       "      <td id=\"T_edb14_row6_col4\" class=\"data row6 col4\" >0.0000</td>\n",
       "      <td id=\"T_edb14_row6_col5\" class=\"data row6 col5\" >0.0027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fee728b8210>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-29 18:46:29,289] Searching the best hyperparameters using 66322 samples...\n",
      "[I 2023-11-29 18:57:48,643] Finished hyperparameter search!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 mean:  0.6204\n",
      "MAPE mean:  0.2673\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Fatal] CUDA Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_CUDAP=1\n",
      "[LightGBM] [Fatal] CUDA Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_CUDAP=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Fatal] CUDA Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_CUDAP=1\n",
      "[LightGBM] [Fatal] CUDA Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_CUDAP=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Fatal] CUDA Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_CUDAP=1\n",
      "[LightGBM] [Fatal] CUDA Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_CUDAP=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Fatal] CUDA Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_CUDAP=1\n",
      "[LightGBM] [Fatal] CUDA Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_CUDAP=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Fatal] CUDA Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_CUDAP=1\n",
      "[LightGBM] [Fatal] CUDA Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_CUDAP=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Fatal] CUDA Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_CUDAP=1\n",
      "[LightGBM] [Fatal] CUDA Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_CUDAP=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Fatal] CUDA Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_CUDAP=1\n",
      "[LightGBM] [Fatal] CUDA Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_CUDAP=1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Initiated</th>\n",
       "      <td>. . . . . . . . . . . . . . . . . .</td>\n",
       "      <td>18:58:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Status</th>\n",
       "      <td>. . . . . . . . . . . . . . . . . .</td>\n",
       "      <td>Searching Hyperparameters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Estimator</th>\n",
       "      <td>. . . . . . . . . . . . . . . . . .</td>\n",
       "      <td>CatBoost Regressor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                         \n",
       "                                                                         \n",
       "Initiated  . . . . . . . . . . . . . . . . . .                   18:58:46\n",
       "Status     . . . . . . . . . . . . . . . . . .  Searching Hyperparameters\n",
       "Estimator  . . . . . . . . . . . . . . . . . .         CatBoost Regressor"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afcb262d8442447586391c2b6abbbdb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-29 18:58:46,403] Searching the best hyperparameters using 66322 samples...\n",
      "[W 2023-11-29 19:00:43,730] Trial 21 failed with parameters: {'actual_estimator__eta': 0.3472233733243945, 'actual_estimator__depth': 8, 'actual_estimator__n_estimators': 270, 'actual_estimator__random_strength': 0.5997890431883712, 'actual_estimator__l2_leaf_reg': 1} because of the following error: KeyboardInterrupt('').\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jules/miniconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/home/jules/miniconda3/lib/python3.11/site-packages/optuna/integration/sklearn.py\", line 219, in __call__\n",
      "    scores = cross_validate(\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/jules/miniconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 266, in cross_validate\n",
      "    results = parallel(\n",
      "              ^^^^^^^^^\n",
      "  File \"/home/jules/miniconda3/lib/python3.11/site-packages/sklearn/utils/parallel.py\", line 63, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jules/miniconda3/lib/python3.11/site-packages/joblib/parallel.py\", line 1863, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "                                                ^^^^^^^^^^^^\n",
      "  File \"/home/jules/miniconda3/lib/python3.11/site-packages/joblib/parallel.py\", line 1792, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jules/miniconda3/lib/python3.11/site-packages/sklearn/utils/parallel.py\", line 123, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jules/miniconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/jules/miniconda3/lib/python3.11/site-packages/pycaret/internal/pipeline.py\", line 275, in fit\n",
      "    fitted_estimator = self._memory_fit(\n",
      "                       ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jules/miniconda3/lib/python3.11/site-packages/joblib/memory.py\", line 353, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jules/miniconda3/lib/python3.11/site-packages/pycaret/internal/pipeline.py\", line 68, in _fit_one\n",
      "    transformer.fit(*args, **fit_params)\n",
      "  File \"/home/jules/miniconda3/lib/python3.11/site-packages/catboost/core.py\", line 5703, in fit\n",
      "    return self._fit(X, y, cat_features, text_features, embedding_features, None, sample_weight, None, None, None, None, baseline,\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jules/miniconda3/lib/python3.11/site-packages/catboost/core.py\", line 2319, in _fit\n",
      "    self._train(\n",
      "  File \"/home/jules/miniconda3/lib/python3.11/site-packages/catboost/core.py\", line 1723, in _train\n",
      "    self._object._train(train_pool, test_pool, params, allow_clear_pool, init_model._object if init_model else None)\n",
      "  File \"_catboost.pyx\", line 4645, in _catboost._CatBoost._train\n",
      "  File \"_catboost.pyx\", line 4694, in _catboost._CatBoost._train\n",
      "KeyboardInterrupt\n",
      "[W 2023-11-29 19:00:43,732] Trial 21 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/jules/ml-project-2-genelipids/models.ipynb Cell 27\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/jules/ml-project-2-genelipids/models.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m s \u001b[39m=\u001b[39m setup(data\u001b[39m=\u001b[39mdf, target\u001b[39m=\u001b[39mlipid_name, fold\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, session_id\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, use_gpu\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, preprocess\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/jules/ml-project-2-genelipids/models.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m model \u001b[39m=\u001b[39m create_model(\u001b[39m'\u001b[39m\u001b[39mcatboost\u001b[39m\u001b[39m'\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/jules/ml-project-2-genelipids/models.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m fine_tuned \u001b[39m=\u001b[39m tune_model(model, search_library\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39moptuna\u001b[39;49m\u001b[39m'\u001b[39;49m,optimize\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mMAPE\u001b[39;49m\u001b[39m\"\u001b[39;49m, n_iter\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, early_stopping\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/jules/ml-project-2-genelipids/models.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# Retrieving cross-validation results\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/jules/ml-project-2-genelipids/models.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m metrics \u001b[39m=\u001b[39m pull()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/pycaret/utils/generic.py:965\u001b[0m, in \u001b[0;36mcheck_if_global_is_not_none.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[39mif\u001b[39;00m globals_d[name] \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    964\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(message)\n\u001b[0;32m--> 965\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/pycaret/regression/functional.py:1197\u001b[0m, in \u001b[0;36mtune_model\u001b[0;34m(estimator, fold, round, n_iter, custom_grid, optimize, custom_scorer, search_library, search_algorithm, early_stopping, early_stopping_max_iters, choose_better, fit_kwargs, groups, return_tuner, verbose, tuner_verbose, return_train_score, **kwargs)\u001b[0m\n\u001b[1;32m   1005\u001b[0m \u001b[39m@check_if_global_is_not_none\u001b[39m(\u001b[39mglobals\u001b[39m(), _CURRENT_EXPERIMENT_DECORATOR_DICT)\n\u001b[1;32m   1006\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtune_model\u001b[39m(\n\u001b[1;32m   1007\u001b[0m     estimator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1025\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   1026\u001b[0m ):\n\u001b[1;32m   1027\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1028\u001b[0m \u001b[39m    This function tunes the hyperparameters of a given estimator. The output of\u001b[39;00m\n\u001b[1;32m   1029\u001b[0m \u001b[39m    this function is a score grid with CV scores by fold of the best selected\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1194\u001b[0m \n\u001b[1;32m   1195\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1197\u001b[0m     \u001b[39mreturn\u001b[39;00m _CURRENT_EXPERIMENT\u001b[39m.\u001b[39;49mtune_model(\n\u001b[1;32m   1198\u001b[0m         estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[1;32m   1199\u001b[0m         fold\u001b[39m=\u001b[39;49mfold,\n\u001b[1;32m   1200\u001b[0m         \u001b[39mround\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mround\u001b[39;49m,\n\u001b[1;32m   1201\u001b[0m         n_iter\u001b[39m=\u001b[39;49mn_iter,\n\u001b[1;32m   1202\u001b[0m         custom_grid\u001b[39m=\u001b[39;49mcustom_grid,\n\u001b[1;32m   1203\u001b[0m         optimize\u001b[39m=\u001b[39;49moptimize,\n\u001b[1;32m   1204\u001b[0m         custom_scorer\u001b[39m=\u001b[39;49mcustom_scorer,\n\u001b[1;32m   1205\u001b[0m         search_library\u001b[39m=\u001b[39;49msearch_library,\n\u001b[1;32m   1206\u001b[0m         search_algorithm\u001b[39m=\u001b[39;49msearch_algorithm,\n\u001b[1;32m   1207\u001b[0m         early_stopping\u001b[39m=\u001b[39;49mearly_stopping,\n\u001b[1;32m   1208\u001b[0m         early_stopping_max_iters\u001b[39m=\u001b[39;49mearly_stopping_max_iters,\n\u001b[1;32m   1209\u001b[0m         choose_better\u001b[39m=\u001b[39;49mchoose_better,\n\u001b[1;32m   1210\u001b[0m         fit_kwargs\u001b[39m=\u001b[39;49mfit_kwargs,\n\u001b[1;32m   1211\u001b[0m         groups\u001b[39m=\u001b[39;49mgroups,\n\u001b[1;32m   1212\u001b[0m         return_tuner\u001b[39m=\u001b[39;49mreturn_tuner,\n\u001b[1;32m   1213\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   1214\u001b[0m         tuner_verbose\u001b[39m=\u001b[39;49mtuner_verbose,\n\u001b[1;32m   1215\u001b[0m         return_train_score\u001b[39m=\u001b[39;49mreturn_train_score,\n\u001b[1;32m   1216\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   1217\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/pycaret/regression/oop.py:1499\u001b[0m, in \u001b[0;36mRegressionExperiment.tune_model\u001b[0;34m(self, estimator, fold, round, n_iter, custom_grid, optimize, custom_scorer, search_library, search_algorithm, early_stopping, early_stopping_max_iters, choose_better, fit_kwargs, groups, return_tuner, verbose, tuner_verbose, return_train_score, **kwargs)\u001b[0m\n\u001b[1;32m   1307\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtune_model\u001b[39m(\n\u001b[1;32m   1308\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1309\u001b[0m     estimator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1327\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   1328\u001b[0m ):\n\u001b[1;32m   1329\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1330\u001b[0m \u001b[39m    This function tunes the hyperparameters of a given estimator. The output of\u001b[39;00m\n\u001b[1;32m   1331\u001b[0m \u001b[39m    this function is a score grid with CV scores by fold of the best selected\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \n\u001b[1;32m   1497\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1499\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mtune_model(\n\u001b[1;32m   1500\u001b[0m         estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[1;32m   1501\u001b[0m         fold\u001b[39m=\u001b[39;49mfold,\n\u001b[1;32m   1502\u001b[0m         \u001b[39mround\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mround\u001b[39;49m,\n\u001b[1;32m   1503\u001b[0m         n_iter\u001b[39m=\u001b[39;49mn_iter,\n\u001b[1;32m   1504\u001b[0m         custom_grid\u001b[39m=\u001b[39;49mcustom_grid,\n\u001b[1;32m   1505\u001b[0m         optimize\u001b[39m=\u001b[39;49moptimize,\n\u001b[1;32m   1506\u001b[0m         custom_scorer\u001b[39m=\u001b[39;49mcustom_scorer,\n\u001b[1;32m   1507\u001b[0m         search_library\u001b[39m=\u001b[39;49msearch_library,\n\u001b[1;32m   1508\u001b[0m         search_algorithm\u001b[39m=\u001b[39;49msearch_algorithm,\n\u001b[1;32m   1509\u001b[0m         early_stopping\u001b[39m=\u001b[39;49mearly_stopping,\n\u001b[1;32m   1510\u001b[0m         early_stopping_max_iters\u001b[39m=\u001b[39;49mearly_stopping_max_iters,\n\u001b[1;32m   1511\u001b[0m         choose_better\u001b[39m=\u001b[39;49mchoose_better,\n\u001b[1;32m   1512\u001b[0m         fit_kwargs\u001b[39m=\u001b[39;49mfit_kwargs,\n\u001b[1;32m   1513\u001b[0m         groups\u001b[39m=\u001b[39;49mgroups,\n\u001b[1;32m   1514\u001b[0m         return_tuner\u001b[39m=\u001b[39;49mreturn_tuner,\n\u001b[1;32m   1515\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   1516\u001b[0m         tuner_verbose\u001b[39m=\u001b[39;49mtuner_verbose,\n\u001b[1;32m   1517\u001b[0m         return_train_score\u001b[39m=\u001b[39;49mreturn_train_score,\n\u001b[1;32m   1518\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   1519\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/pycaret/internal/pycaret_experiment/supervised_experiment.py:2674\u001b[0m, in \u001b[0;36m_SupervisedExperiment.tune_model\u001b[0;34m(self, estimator, fold, round, n_iter, custom_grid, optimize, custom_scorer, search_library, search_algorithm, early_stopping, early_stopping_max_iters, choose_better, fit_kwargs, groups, return_tuner, verbose, tuner_verbose, return_train_score, **kwargs)\u001b[0m\n\u001b[1;32m   2672\u001b[0m             model_grid\u001b[39m.\u001b[39mfit(data_X, data_y, groups\u001b[39m=\u001b[39mgroups, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_kwargs)\n\u001b[1;32m   2673\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2674\u001b[0m     model_grid\u001b[39m.\u001b[39;49mfit(data_X, data_y, groups\u001b[39m=\u001b[39;49mgroups, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_kwargs)\n\u001b[1;32m   2675\u001b[0m best_params \u001b[39m=\u001b[39m model_grid\u001b[39m.\u001b[39mbest_params_\n\u001b[1;32m   2676\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbest_params: \u001b[39m\u001b[39m{\u001b[39;00mbest_params\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/optuna/integration/sklearn.py:908\u001b[0m, in \u001b[0;36mOptunaSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    888\u001b[0m objective \u001b[39m=\u001b[39m _Objective(\n\u001b[1;32m    889\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimator,\n\u001b[1;32m    890\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparam_distributions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    900\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscorer_,\n\u001b[1;32m    901\u001b[0m )\n\u001b[1;32m    903\u001b[0m _logger\u001b[39m.\u001b[39minfo(\n\u001b[1;32m    904\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mSearching the best hyperparameters using \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    905\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39msamples...\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(_num_samples(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msample_indices_))\n\u001b[1;32m    906\u001b[0m )\n\u001b[0;32m--> 908\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstudy_\u001b[39m.\u001b[39;49moptimize(\n\u001b[1;32m    909\u001b[0m     objective,\n\u001b[1;32m    910\u001b[0m     n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs,\n\u001b[1;32m    911\u001b[0m     n_trials\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_trials,\n\u001b[1;32m    912\u001b[0m     timeout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout,\n\u001b[1;32m    913\u001b[0m     callbacks\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcallbacks,\n\u001b[1;32m    914\u001b[0m )\n\u001b[1;32m    916\u001b[0m _logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mFinished hyperparameter search!\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    918\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrefit:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/optuna/study/study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[1;32m    349\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    357\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    358\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \n\u001b[1;32m    361\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 451\u001b[0m     _optimize(\n\u001b[1;32m    452\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    453\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[1;32m    454\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[1;32m    455\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    456\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m    457\u001b[0m         catch\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(catch) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(catch, Iterable) \u001b[39melse\u001b[39;49;00m (catch,),\n\u001b[1;32m    458\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    459\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[1;32m    460\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[1;32m    461\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[1;32m     67\u001b[0m             study,\n\u001b[1;32m     68\u001b[0m             func,\n\u001b[1;32m     69\u001b[0m             n_trials,\n\u001b[1;32m     70\u001b[0m             timeout,\n\u001b[1;32m     71\u001b[0m             catch,\n\u001b[1;32m     72\u001b[0m             callbacks,\n\u001b[1;32m     73\u001b[0m             gc_after_trial,\n\u001b[1;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m     77\u001b[0m         )\n\u001b[1;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/optuna/study/_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[1;32m    164\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/optuna/study/_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    246\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    247\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[1;32m    248\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    250\u001b[0m ):\n\u001b[0;32m--> 251\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[1;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/optuna/study/_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[1;32m    199\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[1;32m    201\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    202\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/optuna/integration/sklearn.py:219\u001b[0m, in \u001b[0;36m_Objective.__call__\u001b[0;34m(self, trial)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    218\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 219\u001b[0m         scores \u001b[39m=\u001b[39m cross_validate(\n\u001b[1;32m    220\u001b[0m             estimator,\n\u001b[1;32m    221\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mX,\n\u001b[1;32m    222\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49my,\n\u001b[1;32m    223\u001b[0m             cv\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcv,\n\u001b[1;32m    224\u001b[0m             error_score\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merror_score,\n\u001b[1;32m    225\u001b[0m             fit_params\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_params,\n\u001b[1;32m    226\u001b[0m             groups\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups,\n\u001b[1;32m    227\u001b[0m             return_train_score\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreturn_train_score,\n\u001b[1;32m    228\u001b[0m             scoring\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscoring,\n\u001b[1;32m    229\u001b[0m         )\n\u001b[1;32m    230\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[1;32m    231\u001b[0m         n_splits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcv\u001b[39m.\u001b[39mget_n_splits(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mX, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39my, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:266\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[39m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[1;32m    265\u001b[0m parallel \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39mn_jobs, verbose\u001b[39m=\u001b[39mverbose, pre_dispatch\u001b[39m=\u001b[39mpre_dispatch)\n\u001b[0;32m--> 266\u001b[0m results \u001b[39m=\u001b[39m parallel(\n\u001b[1;32m    267\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    268\u001b[0m         clone(estimator),\n\u001b[1;32m    269\u001b[0m         X,\n\u001b[1;32m    270\u001b[0m         y,\n\u001b[1;32m    271\u001b[0m         scorers,\n\u001b[1;32m    272\u001b[0m         train,\n\u001b[1;32m    273\u001b[0m         test,\n\u001b[1;32m    274\u001b[0m         verbose,\n\u001b[1;32m    275\u001b[0m         \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    276\u001b[0m         fit_params,\n\u001b[1;32m    277\u001b[0m         return_train_score\u001b[39m=\u001b[39;49mreturn_train_score,\n\u001b[1;32m    278\u001b[0m         return_times\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    279\u001b[0m         return_estimator\u001b[39m=\u001b[39;49mreturn_estimator,\n\u001b[1;32m    280\u001b[0m         error_score\u001b[39m=\u001b[39;49merror_score,\n\u001b[1;32m    281\u001b[0m     )\n\u001b[1;32m    282\u001b[0m     \u001b[39mfor\u001b[39;49;00m train, test \u001b[39min\u001b[39;49;00m cv\u001b[39m.\u001b[39;49msplit(X, y, groups)\n\u001b[1;32m    283\u001b[0m )\n\u001b[1;32m    285\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[1;32m    287\u001b[0m \u001b[39m# For callabe scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[39m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[39m# the correct key.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/joblib/parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1861\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1862\u001b[0m     \u001b[39mnext\u001b[39m(output)\n\u001b[0;32m-> 1863\u001b[0m     \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39m(output)\n\u001b[1;32m   1865\u001b[0m \u001b[39m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1866\u001b[0m \u001b[39m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1867\u001b[0m \u001b[39m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m \u001b[39m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1869\u001b[0m \u001b[39m# callback.\u001b[39;00m\n\u001b[1;32m   1870\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/joblib/parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1790\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_batches \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1791\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m-> 1792\u001b[0m res \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1793\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_completed_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1794\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/sklearn/utils/parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[1;32m    122\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[0;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:686\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    684\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[1;32m    685\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 686\u001b[0m         estimator\u001b[39m.\u001b[39;49mfit(X_train, y_train, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[1;32m    688\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m    689\u001b[0m     \u001b[39m# Note fit time as time until error\u001b[39;00m\n\u001b[1;32m    690\u001b[0m     fit_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/pycaret/internal/pipeline.py:275\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpassthrough\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    274\u001b[0m     fit_params_last_step \u001b[39m=\u001b[39m fit_params_steps[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m]]\n\u001b[0;32m--> 275\u001b[0m     fitted_estimator \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_memory_fit(\n\u001b[1;32m    276\u001b[0m         clone(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msteps[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m][\u001b[39m1\u001b[39;49m]), X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params_last_step\n\u001b[1;32m    277\u001b[0m     )\n\u001b[1;32m    278\u001b[0m     \u001b[39m# Hacky way to make sure that the state of the estimator\u001b[39;00m\n\u001b[1;32m    279\u001b[0m     \u001b[39m# loaded from cache is carried over to the estimator\u001b[39;00m\n\u001b[1;32m    280\u001b[0m     \u001b[39m# in steps\u001b[39;00m\n\u001b[1;32m    281\u001b[0m     _copy_estimator_state(fitted_estimator, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m1\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/joblib/memory.py:353\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 353\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/pycaret/internal/pipeline.py:68\u001b[0m, in \u001b[0;36m_fit_one\u001b[0;34m(transformer, X, y, message, **fit_params)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m signature(transformer\u001b[39m.\u001b[39mfit)\u001b[39m.\u001b[39mparameters:\n\u001b[1;32m     67\u001b[0m             args\u001b[39m.\u001b[39mappend(y)\n\u001b[0;32m---> 68\u001b[0m         transformer\u001b[39m.\u001b[39;49mfit(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[1;32m     69\u001b[0m \u001b[39mreturn\u001b[39;00m transformer\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/catboost/core.py:5703\u001b[0m, in \u001b[0;36mCatBoostRegressor.fit\u001b[0;34m(self, X, y, cat_features, text_features, embedding_features, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[1;32m   5700\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mloss_function\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m params:\n\u001b[1;32m   5701\u001b[0m     CatBoostRegressor\u001b[39m.\u001b[39m_check_is_compatible_loss(params[\u001b[39m'\u001b[39m\u001b[39mloss_function\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m-> 5703\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X, y, cat_features, text_features, embedding_features, \u001b[39mNone\u001b[39;49;00m, sample_weight, \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m, baseline,\n\u001b[1;32m   5704\u001b[0m                  use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description,\n\u001b[1;32m   5705\u001b[0m                  verbose_eval, metric_period, silent, early_stopping_rounds,\n\u001b[1;32m   5706\u001b[0m                  save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/catboost/core.py:2319\u001b[0m, in \u001b[0;36mCatBoost._fit\u001b[0;34m(self, X, y, cat_features, text_features, embedding_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[1;32m   2315\u001b[0m allow_clear_pool \u001b[39m=\u001b[39m train_params[\u001b[39m\"\u001b[39m\u001b[39mallow_clear_pool\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   2317\u001b[0m \u001b[39mwith\u001b[39;00m log_fixup(log_cout, log_cerr), \\\n\u001b[1;32m   2318\u001b[0m     plot_wrapper(plot, plot_file, \u001b[39m'\u001b[39m\u001b[39mTraining plots\u001b[39m\u001b[39m'\u001b[39m, [_get_train_dir(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_params())]):\n\u001b[0;32m-> 2319\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train(\n\u001b[1;32m   2320\u001b[0m         train_pool,\n\u001b[1;32m   2321\u001b[0m         train_params[\u001b[39m\"\u001b[39;49m\u001b[39meval_sets\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m   2322\u001b[0m         params,\n\u001b[1;32m   2323\u001b[0m         allow_clear_pool,\n\u001b[1;32m   2324\u001b[0m         train_params[\u001b[39m\"\u001b[39;49m\u001b[39minit_model\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n\u001b[1;32m   2325\u001b[0m     )\n\u001b[1;32m   2327\u001b[0m \u001b[39m# Have property feature_importance possibly set\u001b[39;00m\n\u001b[1;32m   2328\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_object\u001b[39m.\u001b[39m_get_loss_function_name()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/catboost/core.py:1723\u001b[0m, in \u001b[0;36m_CatBoostBase._train\u001b[0;34m(self, train_pool, test_pool, params, allow_clear_pool, init_model)\u001b[0m\n\u001b[1;32m   1722\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_train\u001b[39m(\u001b[39mself\u001b[39m, train_pool, test_pool, params, allow_clear_pool, init_model):\n\u001b[0;32m-> 1723\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_object\u001b[39m.\u001b[39;49m_train(train_pool, test_pool, params, allow_clear_pool, init_model\u001b[39m.\u001b[39;49m_object \u001b[39mif\u001b[39;49;00m init_model \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m   1724\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_trained_model_attributes()\n",
      "File \u001b[0;32m_catboost.pyx:4645\u001b[0m, in \u001b[0;36m_catboost._CatBoost._train\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_catboost.pyx:4694\u001b[0m, in \u001b[0;36m_catboost._CatBoost._train\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame(columns=['Lipid', 'R2_mean', 'MAPE_mean'])\n",
    "\n",
    "for i, lipid_name in enumerate(section_12_lipids_only.columns):\n",
    "    df = aggregated_gene_data_df.copy()\n",
    "    df[lipid_name] = section_12_lipids_only.iloc[:, i]\n",
    "    \n",
    "    s = setup(data=df, target=lipid_name, fold=5, session_id=42, verbose=False, use_gpu=True, preprocess=False)\n",
    "    \n",
    "    model = create_model('catboost', verbose=False)\n",
    "    \n",
    "    #fine_tuned = tune_model(model, search_library='optuna',optimize=\"MAPE\", n_iter=100, early_stopping=True)\n",
    "    \n",
    "    # Retrieving cross-validation results\n",
    "    metrics = pull()\n",
    "    \n",
    "    r2_mean = metrics.loc['Mean','R2']\n",
    "    mape_mean = metrics.loc['Mean', 'MAPE']\n",
    "    \n",
    "    print(\"R2 mean: \", r2_mean)\n",
    "    print(\"MAPE mean: \", mape_mean)\n",
    "    \n",
    "    # Append the results to the DataFrame\n",
    "    results_df = results_df.append({'Lipid': lipid_name, 'R2_mean': r2_mean, 'MAPE_mean': mape_mean}, ignore_index=True)\n",
    "\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('results.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-project2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
