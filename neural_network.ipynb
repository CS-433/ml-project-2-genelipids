{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Neural Network for all lipids"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import metrics\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization, Dropout, Activation\n",
    "from keras.initializers import HeNormal\n",
    "from keras.regularizers import l2\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import KFold"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Reproducibility"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T19:34:08.341412800Z",
     "start_time": "2023-12-12T19:34:08.163373600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Random seed for reproducibility\n",
    "seed = 42\n",
    "\n",
    "random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "tf.compat.v1.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data loading"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T19:34:10.588414Z",
     "start_time": "2023-12-12T19:34:10.580663600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dataset path \n",
    "dataset_dir = 'data/processed_data'\n",
    "# Training loading\n",
    "training_input_path = os.path.join(dataset_dir, 'train_features.parquet')\n",
    "training_output_path = os.path.join(dataset_dir, 'train_targets.parquet')\n",
    "# Testing loading\n",
    "testing_input_path = os.path.join(dataset_dir, 'test_features.parquet')\n",
    "testing_output_path = os.path.join(dataset_dir, 'test_targets.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T19:34:12.188127200Z",
     "start_time": "2023-12-12T19:34:11.369415Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load dataset into Pandas dataframes\n",
    "# Load training\n",
    "training_input = pd.read_parquet(training_input_path)\n",
    "training_output = pd.read_parquet(training_output_path)\n",
    "\n",
    "# Load testing\n",
    "testing_input = pd.read_parquet(testing_input_path)\n",
    "testing_output = pd.read_parquet(testing_output_path)\n",
    "\n",
    "# Name of lipids to be removed\n",
    "lipids_to_drop = ['LPC O-18:3', 'Cer 36:1', 'PE(30:1) ', 'PE 34:2', 'PA(P-38:6)\\xa0',\n",
    " 'PI-Cer(t30:2)\\xa0', 'PE 36:3', 'PE O-39:6', 'PA 40:6', 'PA 42:4',\n",
    " 'PE(O-40:6)\\xa0', 'PE(42:6)\\xa0', 'PE(40:8)\\xa0', 'PGP(34:1) ', 'PE(44:11(OH))\\xa0',\n",
    " 'PS(40:4) ', 'PIP(O-36:5)\\xa0', 'PI-Cer(t30:0)\\xa0']\n",
    "\n",
    "training_output = training_output.drop(lipids_to_drop, axis=1)\n",
    "\n",
    "# Number of output nodes (lipids) for the model\n",
    "OUTPUT_NODES = training_output.shape[1]\n",
    "# Number of input nodes (genes) for the model\n",
    "input_dim = training_input.shape[1]\n",
    "# Batch size\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T19:34:18.847395600Z",
     "start_time": "2023-12-12T19:34:18.054833400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_40\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_177 (Dense)           (None, 128)               64128     \n",
      "                                                                 \n",
      " dense_178 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_179 (Dense)           (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_180 (Dense)           (None, 138)               4554      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 79,018\n",
      "Trainable params: 79,018\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.engine.sequential.Sequential at 0x7fc9a66195a0>"
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a Sequential model\n",
    "def build_model(summary=False):\n",
    "    \"\"\"\n",
    "    Build the MLP model\n",
    "    :param summary: if True, print the summary of the model\n",
    "    :return: MLP model\n",
    "    \"\"\"\n",
    "\n",
    "    model = Sequential()\n",
    "    # Add the input layer with 500 nodes\n",
    "    model.add(Dense(128, activation='sigmoid', input_dim=500))\n",
    "    model.add(Dense(64, activation='sigmoid'))\n",
    "    model.add(Dense(32, activation='sigmoid'))\n",
    "    # Add the output layer with OUTPUT_NODES nodes (for multiple regression)\n",
    "    model.add(Dense(OUTPUT_NODES, activation='sigmoid'))\n",
    "\n",
    "    if summary:\n",
    "        # Display the model summary\n",
    "        model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "# Print the model summary\n",
    "build_model(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "# Calculate loss for each output node (lipid)\n",
    "def calculate_losses(lipid_true: pd.DataFrame, lipid_pred: np.ndarray):\n",
    "    \"\"\"\n",
    "    Compute loss (MSE) for each lipid\n",
    "    :param lipid_true: true values of lipid abundance\n",
    "    :param lipid_pred: predicted values of lipid abundance\n",
    "    :return: loss (MSE) for each lipid\n",
    "    \"\"\"\n",
    "    # Initialize numpy array with losses\n",
    "    losses = np.zeros((OUTPUT_NODES,))\n",
    "    # Iterate over each lipid\n",
    "    for i in range(OUTPUT_NODES):\n",
    "        y_pred = lipid_pred[:, i]\n",
    "        y_true = lipid_true.iloc[:, i]\n",
    "        # Compute loss (MSE)\n",
    "        losses[i] = mean_squared_error(y_true, y_pred)\n",
    "    return losses\n",
    "\n",
    "# Calculate loss for each output node (lipid)\n",
    "def calculate_r2s(lipid_true: pd.DataFrame, lipid_pred: np.ndarray):\n",
    "    \"\"\"\n",
    "    Compute R2 score for each lipid\n",
    "    :param lipid_true: true values of lipid abundance\n",
    "    :param lipid_pred: predicted values of lipid abundance\n",
    "    :return: R2 score for each lipid\n",
    "    \"\"\"\n",
    "    # Initialize numpy array with R2 scores\n",
    "    r2s = np.zeros((OUTPUT_NODES,))\n",
    "    # Iterate over each lipid\n",
    "    for i in range(OUTPUT_NODES):\n",
    "        y_pred = lipid_pred[:, i]\n",
    "        y_true = lipid_true.iloc[:, i]\n",
    "        # Compute R2 score\n",
    "        r2s[i] = r2_score(y_true, y_pred)\n",
    "    return r2s"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "# Use early stopping on the validation\n",
    "early_stopping = EarlyStopping(monitor='val_loss',  # Metric chose is validation loss (MSE)\n",
    "                               patience=10,         # Number of epochs with no improvement after which training stops\n",
    "                               restore_best_weights=True)  # Restores model weights from the epoch with the best value of the monitored metric"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/violarenne/opt/anaconda3/envs/ml-project-2-genelipids/lib/python3.10/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/Users/violarenne/opt/anaconda3/envs/ml-project-2-genelipids/lib/python3.10/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/Users/violarenne/opt/anaconda3/envs/ml-project-2-genelipids/lib/python3.10/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/Users/violarenne/opt/anaconda3/envs/ml-project-2-genelipids/lib/python3.10/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/Users/violarenne/opt/anaconda3/envs/ml-project-2-genelipids/lib/python3.10/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/Users/violarenne/opt/anaconda3/envs/ml-project-2-genelipids/lib/python3.10/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/Users/violarenne/opt/anaconda3/envs/ml-project-2-genelipids/lib/python3.10/site-packages/sklearn/utils/validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/Users/violarenne/opt/anaconda3/envs/ml-project-2-genelipids/lib/python3.10/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/Users/violarenne/opt/anaconda3/envs/ml-project-2-genelipids/lib/python3.10/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Epoch 1/200\n",
      "1565/1565 [==============================] - 9s 5ms/step - loss: 0.0123 - mean_squared_error: 0.0123 - mean_absolute_error: 0.0789 - r_square: -0.5266 - val_loss: 0.0091 - val_mean_squared_error: 0.0091 - val_mean_absolute_error: 0.0691 - val_r_square: 0.0484\n",
      "Epoch 2/200\n",
      "1565/1565 [==============================] - 6s 4ms/step - loss: 0.0077 - mean_squared_error: 0.0077 - mean_absolute_error: 0.0626 - r_square: 0.1750 - val_loss: 0.0069 - val_mean_squared_error: 0.0069 - val_mean_absolute_error: 0.0583 - val_r_square: 0.2417\n",
      "Epoch 3/200\n",
      "1565/1565 [==============================] - 6s 4ms/step - loss: 0.0065 - mean_squared_error: 0.0065 - mean_absolute_error: 0.0563 - r_square: 0.2928 - val_loss: 0.0061 - val_mean_squared_error: 0.0061 - val_mean_absolute_error: 0.0548 - val_r_square: 0.3214\n",
      "Epoch 4/200\n",
      "1565/1565 [==============================] - 8s 5ms/step - loss: 0.0059 - mean_squared_error: 0.0059 - mean_absolute_error: 0.0536 - r_square: 0.3457 - val_loss: 0.0058 - val_mean_squared_error: 0.0058 - val_mean_absolute_error: 0.0529 - val_r_square: 0.3569\n",
      "Epoch 5/200\n",
      "1565/1565 [==============================] - 8s 5ms/step - loss: 0.0056 - mean_squared_error: 0.0056 - mean_absolute_error: 0.0519 - r_square: 0.3781 - val_loss: 0.0055 - val_mean_squared_error: 0.0055 - val_mean_absolute_error: 0.0513 - val_r_square: 0.3889\n",
      "Epoch 6/200\n",
      "1565/1565 [==============================] - 7s 4ms/step - loss: 0.0054 - mean_squared_error: 0.0054 - mean_absolute_error: 0.0510 - r_square: 0.3960 - val_loss: 0.0053 - val_mean_squared_error: 0.0053 - val_mean_absolute_error: 0.0502 - val_r_square: 0.4031\n",
      "Epoch 7/200\n",
      "1565/1565 [==============================] - 7s 5ms/step - loss: 0.0053 - mean_squared_error: 0.0053 - mean_absolute_error: 0.0503 - r_square: 0.4101 - val_loss: 0.0052 - val_mean_squared_error: 0.0052 - val_mean_absolute_error: 0.0498 - val_r_square: 0.4130\n",
      "Epoch 8/200\n",
      "1565/1565 [==============================] - 6s 4ms/step - loss: 0.0051 - mean_squared_error: 0.0051 - mean_absolute_error: 0.0496 - r_square: 0.4222 - val_loss: 0.0051 - val_mean_squared_error: 0.0051 - val_mean_absolute_error: 0.0497 - val_r_square: 0.4219\n",
      "Epoch 9/200\n",
      "1565/1565 [==============================] - 6s 4ms/step - loss: 0.0051 - mean_squared_error: 0.0051 - mean_absolute_error: 0.0492 - r_square: 0.4316 - val_loss: 0.0050 - val_mean_squared_error: 0.0050 - val_mean_absolute_error: 0.0490 - val_r_square: 0.4322\n",
      "Epoch 10/200\n",
      "1565/1565 [==============================] - 5s 4ms/step - loss: 0.0050 - mean_squared_error: 0.0050 - mean_absolute_error: 0.0487 - r_square: 0.4418 - val_loss: 0.0050 - val_mean_squared_error: 0.0050 - val_mean_absolute_error: 0.0486 - val_r_square: 0.4394\n",
      "Epoch 11/200\n",
      "1565/1565 [==============================] - 5s 3ms/step - loss: 0.0049 - mean_squared_error: 0.0049 - mean_absolute_error: 0.0483 - r_square: 0.4490 - val_loss: 0.0049 - val_mean_squared_error: 0.0049 - val_mean_absolute_error: 0.0483 - val_r_square: 0.4445\n",
      "Epoch 12/200\n",
      "1565/1565 [==============================] - 6s 4ms/step - loss: 0.0048 - mean_squared_error: 0.0048 - mean_absolute_error: 0.0480 - r_square: 0.4550 - val_loss: 0.0048 - val_mean_squared_error: 0.0048 - val_mean_absolute_error: 0.0483 - val_r_square: 0.4500\n",
      "Epoch 13/200\n",
      "1565/1565 [==============================] - 6s 4ms/step - loss: 0.0048 - mean_squared_error: 0.0048 - mean_absolute_error: 0.0477 - r_square: 0.4608 - val_loss: 0.0049 - val_mean_squared_error: 0.0049 - val_mean_absolute_error: 0.0483 - val_r_square: 0.4510\n",
      "Epoch 14/200\n",
      "1565/1565 [==============================] - 5s 3ms/step - loss: 0.0048 - mean_squared_error: 0.0048 - mean_absolute_error: 0.0476 - r_square: 0.4645 - val_loss: 0.0048 - val_mean_squared_error: 0.0048 - val_mean_absolute_error: 0.0476 - val_r_square: 0.4573\n",
      "Epoch 15/200\n",
      "1565/1565 [==============================] - 5s 3ms/step - loss: 0.0047 - mean_squared_error: 0.0047 - mean_absolute_error: 0.0473 - r_square: 0.4694 - val_loss: 0.0048 - val_mean_squared_error: 0.0048 - val_mean_absolute_error: 0.0473 - val_r_square: 0.4631\n",
      "Epoch 16/200\n",
      "1565/1565 [==============================] - 6s 4ms/step - loss: 0.0047 - mean_squared_error: 0.0047 - mean_absolute_error: 0.0472 - r_square: 0.4728 - val_loss: 0.0047 - val_mean_squared_error: 0.0047 - val_mean_absolute_error: 0.0471 - val_r_square: 0.4697\n",
      "Epoch 17/200\n",
      "1565/1565 [==============================] - 7s 5ms/step - loss: 0.0046 - mean_squared_error: 0.0046 - mean_absolute_error: 0.0470 - r_square: 0.4763 - val_loss: 0.0047 - val_mean_squared_error: 0.0047 - val_mean_absolute_error: 0.0470 - val_r_square: 0.4737\n",
      "Epoch 18/200\n",
      "1565/1565 [==============================] - 5s 3ms/step - loss: 0.0046 - mean_squared_error: 0.0046 - mean_absolute_error: 0.0468 - r_square: 0.4808 - val_loss: 0.0046 - val_mean_squared_error: 0.0046 - val_mean_absolute_error: 0.0466 - val_r_square: 0.4774\n",
      "Epoch 19/200\n",
      "1565/1565 [==============================] - 6s 4ms/step - loss: 0.0046 - mean_squared_error: 0.0046 - mean_absolute_error: 0.0466 - r_square: 0.4848 - val_loss: 0.0046 - val_mean_squared_error: 0.0046 - val_mean_absolute_error: 0.0466 - val_r_square: 0.4781\n",
      "Epoch 20/200\n",
      "1565/1565 [==============================] - 8s 5ms/step - loss: 0.0046 - mean_squared_error: 0.0046 - mean_absolute_error: 0.0465 - r_square: 0.4873 - val_loss: 0.0047 - val_mean_squared_error: 0.0047 - val_mean_absolute_error: 0.0469 - val_r_square: 0.4713\n",
      "Epoch 21/200\n",
      "1565/1565 [==============================] - 6s 4ms/step - loss: 0.0045 - mean_squared_error: 0.0045 - mean_absolute_error: 0.0463 - r_square: 0.4921 - val_loss: 0.0047 - val_mean_squared_error: 0.0047 - val_mean_absolute_error: 0.0472 - val_r_square: 0.4702\n",
      "Epoch 22/200\n",
      "1565/1565 [==============================] - 6s 4ms/step - loss: 0.0045 - mean_squared_error: 0.0045 - mean_absolute_error: 0.0463 - r_square: 0.4924 - val_loss: 0.0045 - val_mean_squared_error: 0.0045 - val_mean_absolute_error: 0.0462 - val_r_square: 0.4864\n",
      "Epoch 23/200\n",
      "1565/1565 [==============================] - 8s 5ms/step - loss: 0.0045 - mean_squared_error: 0.0045 - mean_absolute_error: 0.0461 - r_square: 0.4955 - val_loss: 0.0045 - val_mean_squared_error: 0.0045 - val_mean_absolute_error: 0.0465 - val_r_square: 0.4868\n",
      "Epoch 24/200\n",
      "1565/1565 [==============================] - 8s 5ms/step - loss: 0.0045 - mean_squared_error: 0.0045 - mean_absolute_error: 0.0460 - r_square: 0.4976 - val_loss: 0.0045 - val_mean_squared_error: 0.0045 - val_mean_absolute_error: 0.0466 - val_r_square: 0.4875\n",
      "Epoch 25/200\n",
      "1565/1565 [==============================] - 7s 4ms/step - loss: 0.0044 - mean_squared_error: 0.0044 - mean_absolute_error: 0.0459 - r_square: 0.5006 - val_loss: 0.0045 - val_mean_squared_error: 0.0045 - val_mean_absolute_error: 0.0460 - val_r_square: 0.4933\n",
      "Epoch 26/200\n",
      "1565/1565 [==============================] - 7s 4ms/step - loss: 0.0044 - mean_squared_error: 0.0044 - mean_absolute_error: 0.0458 - r_square: 0.5021 - val_loss: 0.0045 - val_mean_squared_error: 0.0045 - val_mean_absolute_error: 0.0462 - val_r_square: 0.4893\n",
      "Epoch 27/200\n",
      "1565/1565 [==============================] - 7s 4ms/step - loss: 0.0044 - mean_squared_error: 0.0044 - mean_absolute_error: 0.0456 - r_square: 0.5056 - val_loss: 0.0045 - val_mean_squared_error: 0.0045 - val_mean_absolute_error: 0.0464 - val_r_square: 0.4876\n",
      "Epoch 28/200\n",
      "1565/1565 [==============================] - 6s 4ms/step - loss: 0.0044 - mean_squared_error: 0.0044 - mean_absolute_error: 0.0456 - r_square: 0.5066 - val_loss: 0.0045 - val_mean_squared_error: 0.0045 - val_mean_absolute_error: 0.0464 - val_r_square: 0.4882\n",
      "Epoch 29/200\n",
      "1565/1565 [==============================] - 6s 4ms/step - loss: 0.0044 - mean_squared_error: 0.0044 - mean_absolute_error: 0.0455 - r_square: 0.5084 - val_loss: 0.0045 - val_mean_squared_error: 0.0045 - val_mean_absolute_error: 0.0457 - val_r_square: 0.4925\n",
      "Epoch 30/200\n",
      "1565/1565 [==============================] - 7s 4ms/step - loss: 0.0044 - mean_squared_error: 0.0044 - mean_absolute_error: 0.0454 - r_square: 0.5099 - val_loss: 0.0046 - val_mean_squared_error: 0.0046 - val_mean_absolute_error: 0.0471 - val_r_square: 0.4801\n",
      "Epoch 31/200\n",
      "1565/1565 [==============================] - 7s 5ms/step - loss: 0.0043 - mean_squared_error: 0.0043 - mean_absolute_error: 0.0454 - r_square: 0.5112 - val_loss: 0.0044 - val_mean_squared_error: 0.0044 - val_mean_absolute_error: 0.0457 - val_r_square: 0.5010\n",
      "Epoch 32/200\n",
      "1565/1565 [==============================] - 11s 7ms/step - loss: 0.0043 - mean_squared_error: 0.0043 - mean_absolute_error: 0.0453 - r_square: 0.5121 - val_loss: 0.0044 - val_mean_squared_error: 0.0044 - val_mean_absolute_error: 0.0455 - val_r_square: 0.5035\n",
      "Epoch 33/200\n",
      "1565/1565 [==============================] - 7s 4ms/step - loss: 0.0043 - mean_squared_error: 0.0043 - mean_absolute_error: 0.0452 - r_square: 0.5135 - val_loss: 0.0044 - val_mean_squared_error: 0.0044 - val_mean_absolute_error: 0.0453 - val_r_square: 0.5028\n",
      "Epoch 34/200\n",
      "1565/1565 [==============================] - 10s 6ms/step - loss: 0.0043 - mean_squared_error: 0.0043 - mean_absolute_error: 0.0451 - r_square: 0.5167 - val_loss: 0.0045 - val_mean_squared_error: 0.0045 - val_mean_absolute_error: 0.0456 - val_r_square: 0.4958\n",
      "Epoch 35/200\n",
      "1565/1565 [==============================] - 14s 9ms/step - loss: 0.0043 - mean_squared_error: 0.0043 - mean_absolute_error: 0.0450 - r_square: 0.5185 - val_loss: 0.0044 - val_mean_squared_error: 0.0044 - val_mean_absolute_error: 0.0456 - val_r_square: 0.5043\n",
      "Epoch 36/200\n",
      "1565/1565 [==============================] - 7s 5ms/step - loss: 0.0043 - mean_squared_error: 0.0043 - mean_absolute_error: 0.0450 - r_square: 0.5179 - val_loss: 0.0043 - val_mean_squared_error: 0.0043 - val_mean_absolute_error: 0.0451 - val_r_square: 0.5094\n",
      "Epoch 37/200\n",
      " 396/1565 [======>.......................] - ETA: 5s - loss: 0.0042 - mean_squared_error: 0.0042 - mean_absolute_error: 0.0448 - r_square: 0.5226"
     ]
    }
   ],
   "source": [
    "lipid_names = list(map(lambda s: s.strip(), training_output.columns.values))\n",
    "\n",
    "# DataFrame with the results\n",
    "lipids_metrics_avg = pd.DataFrame(columns=['Loss', 'R2', 'R2_no_scale'], index=lipid_names)\n",
    "\n",
    "print(f'Start training')\n",
    "\n",
    "# Define the K-fold Cross Validator\n",
    "num_folds = 5\n",
    "k_fold = KFold(n_splits=num_folds, shuffle=True, random_state=seed)\n",
    "\n",
    "# Metrics for each fold\n",
    "loss_per_fold = np.zeros((num_folds,len(lipid_names)))\n",
    "r2_per_fold = np.zeros((num_folds,len(lipid_names)))\n",
    "r2_per_fold_no_scale = np.zeros((num_folds,len(lipid_names)))\n",
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "fold_no = 1\n",
    "models = []\n",
    "split_indices = []\n",
    "final_losses = []\n",
    "final_r_score = []\n",
    "\n",
    "# Compute K-fold Cross Validation\n",
    "for input_indices, output_indices in k_fold.split(training_input, training_output):\n",
    "    split_indices.append(output_indices)\n",
    "    # Build the model\n",
    "    model = build_model()\n",
    "\n",
    "    # Fit and transform the data\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(training_output.iloc[input_indices])\n",
    "    training_output_scale = pd.DataFrame(scaler.transform(training_output.iloc[input_indices]), columns=training_output.columns, index=training_output.iloc[input_indices].index)\n",
    "    validation_output_scale = pd.DataFrame(scaler.transform(training_output.iloc[output_indices]), columns=training_output.columns, index=training_output.iloc[output_indices].index)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error',\n",
    "                  metrics=[metrics.mean_squared_error, metrics.mean_absolute_error, tfa.metrics.RSquare()])\n",
    "\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        training_input.iloc[input_indices], training_output_scale,\n",
    "        epochs=200,  # Adjust the number of epochs as needed\n",
    "        batch_size = 32,\n",
    "        validation_data=(training_input.iloc[output_indices], validation_output_scale),\n",
    "        callbacks=[early_stopping],\n",
    "        shuffle=True\n",
    "    ).history\n",
    "\n",
    "    # Save the results for each fold\n",
    "    final_losses.append(model.evaluate(training_input.iloc[output_indices], validation_output_scale)[0])\n",
    "    final_r_score.append(model.evaluate(training_input.iloc[output_indices], validation_output_scale)[3])\n",
    "\n",
    "    # Generate generalization metrics\n",
    "    prediction = model.predict(training_input.iloc[output_indices])\n",
    "    losses = calculate_losses(validation_output_scale, prediction)\n",
    "    r2s = calculate_r2s(validation_output_scale, prediction)\n",
    "    loss_per_fold[fold_no-1] = losses\n",
    "    r2_per_fold[fold_no-1] = r2s\n",
    "    reversed_data = scaler.inverse_transform(prediction)\n",
    "    r2_per_fold_no_scale[fold_no-1] = calculate_r2s(training_output.iloc[output_indices], reversed_data)\n",
    "\n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1\n",
    "\n",
    "# Compute mean loss and mean R2 score for each lipid\n",
    "mean_loss = loss_per_fold.mean(axis=0)\n",
    "mean_r2 = r2_per_fold.mean(axis=0)\n",
    "mean_r2_no_scale = r2_per_fold_no_scale.mean(axis=0)\n",
    "# Save the metrics\n",
    "lipids_metrics_avg['Loss'] = mean_loss\n",
    "lipids_metrics_avg['R2'] = mean_r2\n",
    "lipids_metrics_avg['R2_no_scale'] = mean_r2_no_scale\n",
    "print('#'*72)\n",
    "print(f'Finish training')\n",
    "\n",
    "# Compute overall loss and R2 score\n",
    "mean = np.mean(np.array(final_losses))\n",
    "std = np.std(np.array(final_losses))\n",
    "print(f'The final loss is: {mean} + {std}')\n",
    "mean = np.mean(np.array(final_r_score))\n",
    "std = np.std(np.array(final_r_score))\n",
    "print(f'The final R2 is: {mean} + {std}')\n",
    "\n",
    "# Save the metrics\n",
    "lipids_metrics_avg.to_csv('results/neural_network/lipids_metrics_avg_neural_network.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(np.median(np.array(final_r_score)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T19:40:13.547234Z",
     "start_time": "2023-12-12T19:40:13.512769300Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Print k best and worst lipids for average between folds of metrics loss and r2\n",
    "k = 5\n",
    "\n",
    "def format_names(df: pd.DataFrame, just=15):\n",
    "    return ', '.join([name.rjust(just) for name in df.index.values])\n",
    "\n",
    "def format_values(df: pd.DataFrame, col: str, just=15):\n",
    "    return ', '.join([f'{val:.5e}'.rjust(just) for val in df[col].values])\n",
    "\n",
    "best_losses = lipids_metrics_avg.nsmallest(k, 'Loss')\n",
    "worst_losses = lipids_metrics_avg.nlargest(k, 'Loss')\n",
    "print(\"Loss:\")\n",
    "print(\"  Best:\")\n",
    "print(f\"  {format_names(best_losses)}\")\n",
    "print(f\"  {format_values(best_losses, 'Loss')}\")\n",
    "print(\"  Worst:\")\n",
    "print(f\"  {format_names(worst_losses)}\")\n",
    "print(f\"  {format_values(worst_losses, 'Loss')}\")\n",
    "\n",
    "best_r2s = lipids_metrics_avg.nlargest(k, 'R2')\n",
    "worst_r2s = lipids_metrics_avg.nsmallest(k, 'R2')\n",
    "print(\"R2:\")\n",
    "print(\"  Best:\")\n",
    "print(f\"  {format_names(best_r2s)}\")\n",
    "print(f\"  {format_values(best_r2s, 'R2')}\")\n",
    "print(\"  Worst:\")\n",
    "print(f\"  {format_names(worst_r2s)}\")\n",
    "print(f\"  {format_values(worst_r2s, 'R2')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4115045,
     "sourceId": 7132195,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30587,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
